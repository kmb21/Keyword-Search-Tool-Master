
$$@@$$PAGE: 1
data structures and algorithm
analysis
edition 32 c version
clifford a shaffer
department of computer science
virginia tech
blacksburg va 24061
march 28 2013
update 32010
for a list of changes see
httppeoplecsvtedushafferbookerratahtml
copyright  20092012 by clifford a shaffer
this document is made freely available in pdf form for educational and
other noncommercial use you may make copies of this file and
redistribute in electronic form without charge you may extract portions of
this document provided that the front page including the title author and
this notice are included any commercial use of this document requires the
written consent of the author the author can be reached at
shaffercsvtedu
if you wish to have a printed version of this document print copies are
published by dover publications
see httpstoredoverpublicationscom048648582xhtml
further information about this text is available at
httppeoplecsvtedushafferbook


$$@@$$PAGE: 2
contents

preface

xiii

i

preliminaries

1

1

data structures and algorithms
11 a philosophy of data structures
111 the need for data structures
112 costs and benefits
12 abstract data types and data structures
13 design patterns
131 flyweight
132 visitor
133 composite
134 strategy
14 problems algorithms and programs
15 further reading
16 exercises

3
4
4
6
8
12
13
13
14
15
16
18
20

2

mathematical preliminaries
21 sets and relations
22 miscellaneous notation
23 logarithms
24 summations and recurrences
25 recursion
26 mathematical proof techniques

25
25
29
31
32
36
38
iii


$$@@$$PAGE: 3
iv

contents

27
28
29
3

ii
4

261 direct proof
262 proof by contradiction
263 proof by mathematical induction
estimation
further reading
exercises

algorithm analysis
31 introduction
32 best worst and average cases
33 a faster computer or a faster algorithm
34 asymptotic analysis
341 upper bounds
342 lower bounds
343  notation
344 simplifying rules
345 classifying functions
35 calculating the running time for a program
36 analyzing problems
37 common misunderstandings
38 multiple parameters
39 space bounds
310 speeding up your programs
311 empirical analysis
312 further reading
313 exercises
314 projects

fundamental data structures
lists stacks and queues
41 lists
411 arraybased list implementation
412 linked lists
413 comparison of list implementations

39
39
40
46
47
48
55
55
61
62
65
65
67
68
69
70
71
76
77
79
80
82
85
86
86
90

93
95
96
100
103
112


$$@@$$PAGE: 4
v

contents

42

43

44
45
46
47

414 element implementations
415 doubly linked lists
stacks
421 arraybased stacks
422 linked stacks
423 comparison of arraybased and linked stacks
424 implementing recursion
queues
431 arraybased queues
432 linked queues
433 comparison of arraybased and linked queues
dictionaries
further reading
exercises
projects

114
115
120
121
123
123
125
127
128
133
133
133
145
145
148

5

binary trees
51 definitions and properties
511 the full binary tree theorem
512 a binary tree node adt
52 binary tree traversals
53 binary tree node implementations
531 pointerbased node implementations
532 space requirements
533 array implementation for complete binary trees
54 binary search trees
55 heaps and priority queues
56 huffman coding trees
561 building huffman coding trees
562 assigning and using huffman codes
563 search in huffman trees
57 further reading
58 exercises
59 projects

151
151
153
155
155
160
160
166
168
168
178
185
186
192
195
196
196
200

6

nonbinary trees

203


$$@@$$PAGE: 5
vi

contents

61

iii
7

general tree definitions and terminology

203

611

an adt for general tree nodes

204

612

general tree traversals

205

62

the parent pointer implementation

207

63

general tree implementations

213

631

list of children

214

632

the leftchildrightsibling implementation

215

633

dynamic node implementations

215

634

dynamic leftchildrightsibling implementation

218

64

kary trees

218

65

sequential tree implementations

219

66

further reading

223

67

exercises

223

68

projects

226

sorting and searching

229

internal sorting

231

71

232

72

sorting terminology and notation
three

n2 

sorting algorithms

233

721

insertion sort

233

722

bubble sort

235

723

selection sort

237

724

the cost of exchange sorting

238

73

shellsort

239

74

mergesort

241

75

quicksort

244

76

heapsort

251

77

binsort and radix sort

252

78

an empirical comparison of sorting algorithms

259

79

lower bounds for sorting

261

710 further reading

265

711 exercises

265

712 projects

269


$$@@$$PAGE: 6
contents

vii

8

file processing and external sorting
81 primary versus secondary storage
82 disk drives
821 disk drive architecture
822 disk access costs
83 buffers and buffer pools
84 the programmers view of files
85 external sorting
851 simple approaches to external sorting
852 replacement selection
853 multiway merging
86 further reading
87 exercises
88 projects

273
273
276
276
280
282
290
291
294
296
300
303
304
307

9

searching
91 searching unsorted and sorted arrays
92 selforganizing lists
93 bit vectors for representing sets
94 hashing
941 hash functions
942 open hashing
943 closed hashing
944 analysis of closed hashing
945 deletion
95 further reading
96 exercises
97 projects

311
312
317
323
324
325
330
331
339
344
345
345
348

10 indexing
101 linear indexing
102 isam
103 treebased indexing
104 23 trees
105 btrees
1051 b trees

351
353
356
358
360
364
368


$$@@$$PAGE: 7
viii

contents

1052 btree analysis
106 further reading
107 exercises
108 projects

iv

advanced data structures

374
375
375
377

379

11 graphs
111 terminology and representations
112 graph implementations
113 graph traversals
1131 depthfirst search
1132 breadthfirst search
1133 topological sort
114 shortestpaths problems
1141 singlesource shortest paths
115 minimumcost spanning trees
1151 prims algorithm
1152 kruskals algorithm
116 further reading
117 exercises
118 projects

381
382
386
390
393
394
394
399
400
402
404
407
408
408
412

12 lists and arrays revisited
121 multilists
122 matrix representations
123 memory management
1231 dynamic storage allocation
1232 failure policies and garbage collection
124 further reading
125 exercises
126 projects

415
415
418
422
424
431
435
436
437

13 advanced tree structures
131 tries

439
439


$$@@$$PAGE: 8
contents

132 balanced trees
1321 the avl tree
1322 the splay tree
133 spatial data structures
1331 the kd tree
1332 the pr quadtree
1333 other point data structures
1334 other spatial data structures
134 further reading
135 exercises
136 projects

v

theory of algorithms

ix
444
445
447
450
452
457
461
463
463
464
465

469

14 analysis techniques
141 summation techniques
142 recurrence relations
1421 estimating upper and lower bounds
1422 expanding recurrences
1423 divide and conquer recurrences
1424 averagecase analysis of quicksort
143 amortized analysis
144 further reading
145 exercises
146 projects

471
472
477
477
480
482
484
486
489
489
493

15 lower bounds
151 introduction to lower bounds proofs
152 lower bounds on searching lists
1521 searching in unsorted lists
1522 searching in sorted lists
153 finding the maximum value
154 adversarial lower bounds proofs
155 state space lower bounds proofs
156 finding the ith best element

495
496
498
498
500
501
503
506
509


$$@@$$PAGE: 9
x

contents

157 optimal sorting

511

158 further reading

514

159 exercises

514

1510projects

517

16 patterns of algorithms
161 dynamic programming

519
519

1611 the knapsack problem

521

1612 allpairs shortest paths

523

162 randomized algorithms

525

1621 randomized algorithms for finding large values

525

1622 skip lists

526

163 numerical algorithms

532

1631 exponentiation

533

1632 largest common factor

533

1633 matrix multiplication

534

1634 random numbers

536

1635 the fast fourier transform

537

164 further reading

542

165 exercises

542

166 projects

543

17 limits to computation

545

171 reductions

546

172 hard problems

551

1721 the theory of n pcompleteness

553

1722 n pcompleteness proofs

557

1723 coping with n pcomplete problems

562

173 impossible problems

565

1731 uncountability

566

1732 the halting problem is unsolvable

569

174 further reading

571

175 exercises

572

176 projects

574


$$@@$$PAGE: 10
contents

vi

appendix

xi

577

a utility functions

579

bibliography

581

index

587


$$@@$$PAGE: 11

$$@@$$PAGE: 12
preface

we study data structures so that we can learn to write more efficient programs
but why must programs be efficient when new computers are faster every year
the reason is that our ambitions grow with our capabilities instead of rendering
efficiency needs obsolete the modern revolution in computing power and storage
capability merely raises the efficiency stakes as we attempt more complex tasks
the quest for program efficiency need not and should not conflict with sound
design and clear coding creating efficient programs has little to do with programming tricks but rather is based on good organization of information and good algorithms a programmer who has not mastered the basic principles of clear design
is not likely to write efficient programs conversely concerns related to development costs and maintainability should not be used as an excuse to justify inefficient
performance generality in design can and should be achieved without sacrificing
performance but this can only be done if the designer understands how to measure
performance and does so as an integral part of the design and implementation process most computer science curricula recognize that good programming skills begin with a strong emphasis on fundamental software engineering principles then
once a programmer has learned the principles of clear program design and implementation the next step is to study the effects of data organization and algorithms
on program efficiency
approach this book describes many techniques for representing data these
techniques are presented within the context of the following principles
1 each data structure and each algorithm has costs and benefits practitioners
need a thorough understanding of how to assess costs and benefits to be able
to adapt to new design challenges this requires an understanding of the
principles of algorithm analysis and also an appreciation for the significant
effects of the physical medium employed eg data stored on disk versus
main memory
2 related to costs and benefits is the notion of tradeoffs for example it is quite
common to reduce time requirements at the expense of an increase in space
requirements or vice versa programmers face tradeoff issues regularly in all
xiii


$$@@$$PAGE: 13
xiv

preface

phases of software design and implementation so the concept must become
deeply ingrained
3 programmers should know enough about common practice to avoid reinventing the wheel thus programmers need to learn the commonly used
data structures their related algorithms and the most frequently encountered
design patterns found in programming
4 data structures follow needs programmers must learn to assess application
needs first then find a data structure with matching capabilities to do this
requires competence in principles 1 2 and 3
as i have taught data structures through the years i have found that design
issues have played an ever greater role in my courses this can be traced through
the various editions of this textbook by the increasing coverage for design patterns
and generic interfaces the first edition had no mention of design patterns the
second edition had limited coverage of a few example patterns and introduced the
dictionary adt and comparator classes with the third edition there is explicit
coverage of some design patterns that are encountered when programming the basic
data structures and algorithms covered in the book
using the book in class data structures and algorithms textbooks tend to fall
into one of two categories teaching texts or encyclopedias books that attempt to
do both usually fail at both this book is intended as a teaching text i believe it is
more important for a practitioner to understand the principles required to select or
design the data structure that will best solve some problem than it is to memorize a
lot of textbook implementations hence i have designed this as a teaching text that
covers most standard data structures but not all a few data structures that are not
widely adopted are included to illustrate important principles some relatively new
data structures that should become widely used in the future are included
within an undergraduate program this textbook is designed for use in either an
advanced lower division sophomore or junior level data structures course or for
a senior level algorithms course new material has been added in the third edition
to support its use in an algorithms course normally this text would be used in a
course beyond the standard freshman level cs2 course that often serves as the
initial introduction to data structures readers of this book should typically have
two semesters of the equivalent of programming experience including at least some
exposure to c  readers who are already familiar with recursion will have an
advantage students of data structures will also benefit from having first completed
a good course in discrete mathematics nonetheless chapter 2 attempts to give
a reasonably complete survey of the prerequisite mathematical topics at the level
necessary to understand their use in this book readers may wish to refer back
to the appropriate sections as needed when encountering unfamiliar mathematical
material


$$@@$$PAGE: 14
preface

xv

a sophomorelevel class where students have only a little background in basic
data structures or analysis that is background equivalent to what would be had
from a traditional cs2 course might cover chapters 111 in detail as well as selected topics from chapter 13 that is how i use the book for my own sophomorelevel class students with greater background might cover chapter 1 skip most
of chapter 2 except for reference briefly cover chapters 3 and 4 and then cover
chapters 512 in detail again only certain topics from chapter 13 might be covered depending on the programming assignments selected by the instructor a
seniorlevel algorithms course would focus on chapters 11 and 1417
chapter 13 is intended in part as a source for larger programming exercises
i recommend that all students taking a data structures course be required to implement some advanced tree structure or another dynamic structure of comparable
difficulty such as the skip list or sparse matrix representations of chapter 12 none
of these data structures are significantly more difficult to implement than the binary
search tree and any of them should be within a students ability after completing
chapter 5
while i have attempted to arrange the presentation in an order that makes sense
instructors should feel free to rearrange the topics as they see fit the book has been
written so that once the reader has mastered chapters 16 the remaining material
has relatively few dependencies clearly external sorting depends on understanding internal sorting and disk files section 62 on the unionfind algorithm is
used in kruskals minimumcost spanning tree algorithm section 92 on selforganizing lists mentions the buffer replacement schemes covered in section 83
chapter 14 draws on examples from throughout the book section 172 relies on
knowledge of graphs otherwise most topics depend only on material presented
earlier within the same chapter
most chapters end with a section entitled further reading these sections
are not comprehensive lists of references on the topics presented rather i include
books and articles that in my opinion may prove exceptionally informative or
entertaining to the reader in some cases i include references to works that should
become familiar to any wellrounded computer scientist
use of c  the programming examples are written in c  but i do not wish to
discourage those unfamiliar with c from reading this book i have attempted to
make the examples as clear as possible while maintaining the advantages of c 
c is used here strictly as a tool to illustrate data structures concepts in particular i make use of c s support for hiding implementation details including features such as classes private class members constructors and destructors these
features of the language support the crucial concept of separating logical design as
embodied in the abstract data type from physical implementation as embodied in
the data structure


$$@@$$PAGE: 15
xvi

preface

to keep the presentation as clear as possible some important features of c
are avoided here i deliberately minimize use of certain features commonly used
by experienced c programmers such as class hierarchy inheritance and virtual
functions operator and function overloading is used sparingly clike initialization
syntax is preferred to some of the alternatives offered by c 
while the c features mentioned above have valid design rationale in real
programs they tend to obscure rather than enlighten the principles espoused in
this book for example inheritance is an important tool that helps programmers
avoid duplication and thus minimize bugs from a pedagogical standpoint however inheritance often makes code examples harder to understand since it tends to
spread the description for one logical unit among several classes thus my class
definitions only use inheritance where inheritance is explicitly relevant to the point
illustrated eg section 531 this does not mean that a programmer should do
likewise avoiding code duplication and minimizing errors are important goals
treat the programming examples as illustrations of data structure principles but do
not copy them directly into your own programs
one painful decision i had to make was whether to use templates in the code
examples in the first edition of this book the decision was to leave templates out
as it was felt that their syntax obscures the meaning of the code for those not familiar with c  in the years following the use of c in computer science curricula
has greatly expanded i now assume that readers of the text will be familiar with
template syntax thus templates are now used extensively in the code examples
my implementations are meant to provide concrete illustrations of data structure principles as an aid to the textual exposition code examples should not be
read or used in isolation from the associated text because the bulk of each examples documentation is contained in the text not the code the code complements
the text not the other way around they are not meant to be a series of commercialquality class implementations if you are looking for a complete implementation
of a standard data structure for use in your own code you would do well to do an
internet search
for instance the code examples provide less parameter checking than is sound
programming practice since including such checking would obscure rather than illuminate the text some parameter checking and testing for other constraints eg
whether a value is being removed from an empty container is included in the form
of a call to assert the inputs to assert are a boolean expression and a character string if this expression evaluates to false then a message is printed and the
program terminates immediately terminating a program when a function receives
a bad parameter is generally considered undesirable in real programs but is quite
adequate for understanding how a data structure is meant to operate in real programming applications c s exception handling features should be used to deal
with input data errors however assertions provide a simpler mechanism for indi


$$@@$$PAGE: 16
xvii

preface

cating required conditions in a way that is both adequate for clarifying how a data
structure is meant to operate and is easily modified into true exception handling
see the appendix for the implementation of assert
i make a distinction in the text between c implementations and pseudocode code labeled as a c implementation has actually been compiled and
tested on one or more c compilers pseudocode examples often conform closely
to c syntax but typically contain one or more lines of higherlevel description
pseudocode is used where i perceived a greater pedagogical advantage to a simpler
but less precise description
exercises and projects proper implementation and analysis of data structures
cannot be learned simply by reading a book you must practice by implementing
real programs constantly comparing different techniques to see what really works
best in a given situation
one of the most important aspects of a course in data structures is that it is
where students really learn to program using pointers and dynamic memory allocation by implementing data structures such as linked lists and trees it is often
where students truly learn recursion in our curriculum this is the first course where
students do significant design because it often requires real data structures to motivate significant design exercises finally the fundamental differences between
memorybased and diskbased data access cannot be appreciated without practical
programming experience for all of these reasons a data structures course cannot
succeed without a significant programming component in our department the data
structures course is one of the most difficult programming course in the curriculum
students should also work problems to develop their analytical abilities i provide over 450 exercises and suggestions for programming projects i urge readers
to take advantage of them
contacting the author and supplementary materials a book such as this
is sure to contain errors and have room for improvement i welcome bug reports
and constructive criticism i can be reached by electronic mail via the internet at
shaffervtedu alternatively comments can be mailed to
cliff shaffer
department of computer science
virginia tech
blacksburg va 24061
the electronic posting of this book along with a set of lecture notes for use in
class can be obtained at
httpwwwcsvtedushafferbookhtml
the code examples used in the book are available at the same site online web
pages for virginia techs sophomorelevel data structures class can be found at


$$@@$$PAGE: 17
xviii

preface

httpcoursescsvteducs3114
readers of this textbook will be interested in our opensource online etextbook project opendsa httpalgovizorgopendsa the opendsa
projects goal is to ceate a complete collection of tutorials that combine textbookquality content with algorithm visualizations for every algorithm and data structure
and a rich collection of interactive exercises when complete opendsa will replace this book
this book was typeset by the author using latex the bibliography was prepared using bibtex the index was prepared using makeindex the figures were
mostly drawn with xfig figures 31 and 910 were partially created using mathematica
acknowledgments it takes a lot of help from a lot of people to make a book
i wish to acknowledge a few of those who helped to make this book possible i
apologize for the inevitable omissions
virginia tech helped make this whole thing possible through sabbatical research leave during fall 1994 enabling me to get the project off the ground my department heads during the time i have written the various editions of this book dennis kafura and jack carroll provided unwavering moral support for this project
mike keenan lenny heath and jeff shaffer provided valuable input on early versions of the chapters i also wish to thank lenny heath for many years of stimulating discussions about algorithms and analysis and how to teach both to students
steve edwards deserves special thanks for spending so much time helping me on
various redesigns of the c and java code versions for the second and third editions and many hours of discussion on the principles of program design thanks
to layne watson for his help with mathematica and to bo begole philip isenhour
jeff nielsen and craig struble for much technical assistance thanks to bill mcquain mark abrams and dennis kafura for answering lots of silly questions about
c and java
i am truly indebted to the many reviewers of the various editions of this manuscript for the first edition these reviewers included j david bezek university of
evansville douglas campbell brigham young university karen davis university of cincinnati vijay kumar garg university of texas  austin jim miller
university of kansas bruce maxim university of michigan  dearborn jeff
parker agile networksharvard dana richards george mason university jack
tan university of houston and lixin tao concordia university without their
help this book would contain many more technical errors and many fewer insights
for the second edition i wish to thank these reviewers gurdip singh kansas
state university peter allen columbia university robin hill university of
wyoming norman jacobson university of california  irvine ben keller eastern michigan university and ken bosworth idaho state university in addition


$$@@$$PAGE: 18
preface

xix

i wish to thank neil stewart and frank j thesen for their comments and ideas for
improvement
third edition reviewers included randall lechlitner university of houstin
clear lake and brian c hipp york technical college i thank them for their
comments
prentice hall was the original print publisher for the first and second editions
without the hard work of many people there none of this would be possible authors simply do not create printerready books on their own foremost thanks go to
kate hargett petra rector laura steele and alan apt my editors over the years
my production editors irwin zucker for the second edition kathleen caren for
the original c version and ed defelippis for the java version kept everything
moving smoothly during that horrible rush at the end thanks to bill zobrist and
bruce gregory i think for getting me into this in the first place others at prentice
hall who helped me along the way include truly donovan linda behrens and
phyllis bregman thanks to tracy dunkelberger for her help in returning the copyright to me thus enabling the electronic future of this work i am sure i owe thanks
to many others at prentice hall for their help in ways that i am not even aware of
i am thankful to shelley kronzek at dover publications for her faith in taking
on the print publication of this third edition much expanded with both java and
c versions and many inconsistencies corrected i am confident that this is the
best edition yet but none of us really knows whether students will prefer a free
online textbook or a lowcost printed bound version in the end we believe that
the two formats will be mutually supporting by offering more choices production
editor james miller and design manager marie zaczkiewicz have worked hard to
ensure that the production is of the highest quality
i wish to express my appreciation to hanan samet for teaching me about data
structures i learned much of the philosophy presented here from him as well
though he is not responsible for any problems with the result thanks to my wife
terry for her love and support and to my daughters irena and kate for pleasant
diversions from working too hard finally and most importantly to all of the data
structures students over the years who have taught me what is important and what
should be skipped in a data structures course and the many new insights they have
provided this book is dedicated to them
cliff shaffer
blacksburg virginia


$$@@$$PAGE: 19

$$@@$$PAGE: 20
part i
preliminaries

1


$$@@$$PAGE: 21

$$@@$$PAGE: 22
1
data structures and algorithms

how many cities with more than 250000 people lie within 500 miles of dallas
texas how many people in my company make over 100000 per year can we
connect all of our telephone customers with less than 1000 miles of cable to
answer questions like these it is not enough to have the necessary information we
must organize that information in a way that allows us to find the answers in time
to satisfy our needs
representing information is fundamental to computer science the primary
purpose of most computer programs is not to perform calculations but to store and
retrieve information  usually as fast as possible for this reason the study of
data structures and the algorithms that manipulate them is at the heart of computer
science and that is what this book is about  helping you to understand how to
structure information to support efficient processing
this book has three primary goals the first is to present the commonly used
data structures these form a programmers basic data structure toolkit for
many problems some data structure in the toolkit provides a good solution
the second goal is to introduce the idea of tradeoffs and reinforce the concept
that there are costs and benefits associated with every data structure this is done
by describing for each data structure the amount of space and time required for
typical operations
the third goal is to teach how to measure the effectiveness of a data structure or
algorithm only through such measurement can you determine which data structure
in your toolkit is most appropriate for a new problem the techniques presented
also allow you to judge the merits of new data structures that you or others might
invent
there are often many approaches to solving a problem how do we choose
between them at the heart of computer program design are two sometimes conflicting goals
1 to design an algorithm that is easy to understand code and debug
2 to design an algorithm that makes efficient use of the computers resources
3


$$@@$$PAGE: 23
4

chap 1 data structures and algorithms

ideally the resulting program is true to both of these goals we might say that
such a program is elegant while the algorithms and program code examples presented here attempt to be elegant in this sense it is not the purpose of this book to
explicitly treat issues related to goal 1 these are primarily concerns of the discipline of software engineering rather this book is mostly about issues relating to
goal 2
how do we measure efficiency chapter 3 describes a method for evaluating
the efficiency of an algorithm or computer program called asymptotic analysis
asymptotic analysis also allows you to measure the inherent difficulty of a problem
the remaining chapters use asymptotic analysis techniques to estimate the time cost
for every algorithm presented this allows you to see how each algorithm compares
to other algorithms for solving the same problem in terms of its efficiency
this first chapter sets the stage for what is to follow by presenting some higherorder issues related to the selection and use of data structures we first examine the
process by which a designer selects a data structure appropriate to the task at hand
we then consider the role of abstraction in program design we briefly consider
the concept of a design pattern and see some examples the chapter ends with an
exploration of the relationship between problems algorithms and programs

11

a philosophy of data structures

111

the need for data structures

you might think that with ever more powerful computers program efficiency is
becoming less important after all processor speed and memory size still continue to improve wont any efficiency problem we might have today be solved by
tomorrows hardware
as we develop more powerful computers our history so far has always been to
use that additional computing power to tackle more complex problems be it in the
form of more sophisticated user interfaces bigger problem sizes or new problems
previously deemed computationally infeasible more complex problems demand
more computation making the need for efficient programs even greater worse yet
as tasks become more complex they become less like our everyday experience
todays computer scientists must be trained to have a thorough understanding of the
principles behind efficient program design because their ordinary life experiences
often do not apply when designing computer programs
in the most general sense a data structure is any data representation and its
associated operations even an integer or floating point number stored on the computer can be viewed as a simple data structure more commonly people use the
term data structure to mean an organization or structuring for a collection of data
items a sorted list of integers stored in an array is an example of such a structuring


$$@@$$PAGE: 24
sec 11 a philosophy of data structures

5

given sufficient space to store a collection of data items it is always possible to
search for specified items within the collection print or otherwise process the data
items in any desired order or modify the value of any particular data item thus
it is possible to perform all necessary operations on any data structure however
using the proper data structure can make the difference between a program running
in a few seconds and one requiring many days
a solution is said to be efficient if it solves the problem within the required
resource constraints examples of resource constraints include the total space
available to store the data  possibly divided into separate main memory and disk
space constraints  and the time allowed to perform each subtask a solution is
sometimes said to be efficient if it requires fewer resources than known alternatives
regardless of whether it meets any particular requirements the cost of a solution is
the amount of resources that the solution consumes most often cost is measured
in terms of one key resource such as time with the implied assumption that the
solution meets the other resource constraints
it should go without saying that people write programs to solve problems however it is crucial to keep this truism in mind when selecting a data structure to solve
a particular problem only by first analyzing the problem to determine the performance goals that must be achieved can there be any hope of selecting the right data
structure for the job poor program designers ignore this analysis step and apply a
data structure that they are familiar with but which is inappropriate to the problem
the result is typically a slow program conversely there is no sense in adopting
a complex representation to improve a program that can meet its performance
goals when implemented using a simpler design
when selecting a data structure to solve a problem you should follow these
steps
1 analyze your problem to determine the basic operations that must be supported examples of basic operations include inserting a data item into the
data structure deleting a data item from the data structure and finding a
specified data item
2 quantify the resource constraints for each operation
3 select the data structure that best meets these requirements
this threestep approach to selecting a data structure operationalizes a datacentered view of the design process the first concern is for the data and the operations to be performed on them the next concern is the representation for those
data and the final concern is the implementation of that representation
resource constraints on certain key operations such as search inserting data
records and deleting data records normally drive the data structure selection process many issues relating to the relative importance of these operations are addressed by the following three questions which you should ask yourself whenever
you must choose a data structure


$$@@$$PAGE: 25
6

chap 1 data structures and algorithms

 are all data items inserted into the data structure at the beginning or are
insertions interspersed with other operations static applications where the
data are loaded at the beginning and never change typically require only
simpler data structures to get an efficient implementation than do dynamic
applications
 can data items be deleted if so this will probably make the implementation
more complicated
 are all data items processed in some welldefined order or is search for specific data items allowed random access search generally requires more
complex data structures
112

costs and benefits

each data structure has associated costs and benefits in practice it is hardly ever
true that one data structure is better than another for use in all situations if one
data structure or algorithm is superior to another in all respects the inferior one
will usually have long been forgotten for nearly every data structure and algorithm
presented in this book you will see examples of where it is the best choice some
of the examples might surprise you
a data structure requires a certain amount of space for each data item it stores
a certain amount of time to perform a single basic operation and a certain amount
of programming effort each problem has constraints on available space and time
each solution to a problem makes use of the basic operations in some relative proportion and the data structure selection process must account for this only after a
careful analysis of your problems characteristics can you determine the best data
structure for the task
example 11 a bank must support many types of transactions with its
customers but we will examine a simple model where customers wish to
open accounts close accounts and add money or withdraw money from
accounts we can consider this problem at two distinct levels 1 the requirements for the physical infrastructure and workflow process that the
bank uses in its interactions with its customers and 2 the requirements
for the database system that manages the accounts
the typical customer opens and closes accounts far less often than he
or she accesses the account customers are willing to wait many minutes
while accounts are created or deleted but are typically not willing to wait
more than a brief time for individual account transactions such as a deposit
or withdrawal these observations can be considered as informal specifications for the time constraints on the problem
it is common practice for banks to provide two tiers of service human tellers or automated teller machines atms support customer access


$$@@$$PAGE: 26
sec 11 a philosophy of data structures

to account balances and updates such as deposits and withdrawals special service representatives are typically provided during restricted hours
to handle opening and closing accounts teller and atm transactions are
expected to take little time opening or closing an account can take much
longer perhaps up to an hour from the customers perspective
from a database perspective we see that atm transactions do not modify the database significantly for simplicity assume that if money is added
or removed this transaction simply changes the value stored in an account
record adding a new account to the database is allowed to take several
minutes deleting an account need have no time constraint because from
the customers point of view all that matters is that all the money be returned equivalent to a withdrawal from the banks point of view the
account record might be removed from the database system after business
hours or at the end of the monthly account cycle
when considering the choice of data structure to use in the database
system that manages customer accounts we see that a data structure that
has little concern for the cost of deletion but is highly efficient for search
and moderately efficient for insertion should meet the resource constraints
imposed by this problem records are accessible by unique account number
sometimes called an exactmatch query one data structure that meets
these requirements is the hash table described in chapter 94 hash tables
allow for extremely fast exactmatch search a record can be modified
quickly when the modification does not affect its space requirements hash
tables also support efficient insertion of new records while deletions can
also be supported efficiently too many deletions lead to some degradation
in performance for the remaining operations however the hash table can
be reorganized periodically to restore the system to peak efficiency such
reorganization can occur offline so as not to affect atm transactions

example 12 a company is developing a database system containing information about cities and towns in the united states there are many
thousands of cities and towns and the database program should allow users
to find information about a particular place by name another example of
an exactmatch query users should also be able to find all places that
match a particular value or range of values for attributes such as location or
population size this is known as a range query
a reasonable database system must answer queries quickly enough to
satisfy the patience of a typical user for an exactmatch query a few seconds is satisfactory if the database is meant to support range queries that
can return many cities that match the query specification the entire opera

7


$$@@$$PAGE: 27
8

chap 1 data structures and algorithms

tion may be allowed to take longer perhaps on the order of a minute to
meet this requirement it will be necessary to support operations that process range queries efficiently by processing all cities in the range as a batch
rather than as a series of operations on individual cities
the hash table suggested in the previous example is inappropriate for
implementing our city database because it cannot perform efficient range
queries the b tree of section 1051 supports large databases insertion
and deletion of data records and range queries however a simple linear index as described in section 101 would be more appropriate if the database
is created once and then never changed such as an atlas distributed on a
cd or accessed from a website

12

abstract data types and data structures

the previous section used the terms data item and data structure without properly defining them this section presents terminology and motivates the design
process embodied in the threestep approach to selecting a data structure this motivation stems from the need to manage the tremendous complexity of computer
programs
a type is a collection of values for example the boolean type consists of the
values true and false the integers also form a type an integer is a simple
type because its values contain no subparts a bank account record will typically
contain several pieces of information such as name address account number and
account balance such a record is an example of an aggregate type or composite
type a data item is a piece of information or a record whose value is drawn from
a type a data item is said to be a member of a type
a data type is a type together with a collection of operations to manipulate
the type for example an integer variable is a member of the integer data type
addition is an example of an operation on the integer data type
a distinction should be made between the logical concept of a data type and its
physical implementation in a computer program for example there are two traditional implementations for the list data type the linked list and the arraybased
list the list data type can therefore be implemented using a linked list or an array even the term array is ambiguous in that it can refer either to a data type
or an implementation array is commonly used in computer programming to
mean a contiguous block of memory locations where each memory location stores
one fixedlength data item by this meaning an array is a physical data structure
however array can also mean a logical data type composed of a typically homogeneous collection of data items with each data item identified by an index
number it is possible to implement arrays in many different ways for exam


$$@@$$PAGE: 28
sec 12 abstract data types and data structures

9

ple section 122 describes the data structure used to implement a sparse matrix a
large twodimensional array that stores only a relatively few nonzero values this
implementation is quite different from the physical representation of an array as
contiguous memory locations
an abstract data type adt is the realization of a data type as a software
component the interface of the adt is defined in terms of a type and a set of
operations on that type the behavior of each operation is determined by its inputs
and outputs an adt does not specify how the data type is implemented these
implementation details are hidden from the user of the adt and protected from
outside access a concept referred to as encapsulation
a data structure is the implementation for an adt in an objectoriented language such as c  an adt and its implementation together make up a class
each operation associated with the adt is implemented by a member function or
method the variables that define the space required by a data item are referred
to as data members an object is an instance of a class that is something that is
created and takes up storage during the execution of a computer program
the term data structure often refers to data stored in a computers main memory the related term file structure often refers to the organization of data on
peripheral storage such as a disk drive or cd
example 13 the mathematical concept of an integer along with operations that manipulate integers form a data type the c int variable
type is a physical representation of the abstract integer the int variable
type along with the operations that act on an int variable form an adt
unfortunately the int implementation is not completely true to the abstract integer as there are limitations on the range of values an int variable
can store if these limitations prove unacceptable then some other representation for the adt integer must be devised and a new implementation
must be used for the associated operations

example 14 an adt for a list of integers might specify the following
operations
 insert a new integer at a particular position in the list
 return true if the list is empty
 reinitialize the list
 return the number of integers currently in the list
 delete the integer at a particular position in the list
from this description the input and output of each operation should be
clear but the implementation for lists has not been specified


$$@@$$PAGE: 29
10

chap 1 data structures and algorithms

one application that makes use of some adt might use particular member
functions of that adt more than a second application or the two applications might
have different time requirements for the various operations these differences in the
requirements of applications are the reason why a given adt might be supported
by more than one implementation
example 15 two popular implementations for large diskbased database
applications are hashing section 94 and the b tree section 105 both
support efficient insertion and deletion of records and both support exactmatch queries however hashing is more efficient than the b tree for
exactmatch queries on the other hand the b tree can perform range
queries efficiently while hashing is hopelessly inefficient for range queries
thus if the database application limits searches to exactmatch queries
hashing is preferred on the other hand if the application requires support
for range queries the b tree is preferred despite these performance issues both implementations solve versions of the same problem updating
and searching a large collection of records
the concept of an adt can help us to focus on key issues even in noncomputing applications
example 16 when operating a car the primary activities are steering
accelerating and braking on nearly all passenger cars you steer by turning the steering wheel accelerate by pushing the gas pedal and brake by
pushing the brake pedal this design for cars can be viewed as an adt
with operations steer accelerate and brake two cars might implement these operations in radically different ways say with different types
of engine or front versus rearwheel drive yet most drivers can operate many different cars because the adt presents a uniform method of
operation that does not require the driver to understand the specifics of any
particular engine or drive design these differences are deliberately hidden
the concept of an adt is one instance of an important principle that must be
understood by any successful computer scientist managing complexity through
abstraction a central theme of computer science is complexity and techniques
for handling it humans deal with complexity by assigning a label to an assembly
of objects or concepts and then manipulating the label in place of the assembly
cognitive psychologists call such a label a metaphor a particular label might be
related to other pieces of information or other labels this collection can in turn be
given a label forming a hierarchy of concepts and labels this hierarchy of labels
allows us to focus on important issues while ignoring unnecessary details


$$@@$$PAGE: 30
sec 12 abstract data types and data structures

11

example 17 we apply the label hard drive to a collection of hardware
that manipulates data on a particular type of storage device and we apply the label cpu to the hardware that controls execution of computer
instructions these and other labels are gathered together under the label
computer because even the smallest home computers today have millions of components some form of abstraction is necessary to comprehend
how a computer operates
consider how you might go about the process of designing a complex computer
program that implements and manipulates an adt the adt is implemented in
one part of the program by a particular data structure while designing those parts
of the program that use the adt you can think in terms of operations on the data
type without concern for the data structures implementation without this ability
to simplify your thinking about a complex program you would have no hope of
understanding or implementing it
example 18 consider the design for a relatively simple database system
stored on disk typically records on disk in such a program are accessed
through a buffer pool see section 83 rather than directly variable length
records might use a memory manager see section 123 to find an appropriate location within the disk file to place the record multiple index structures see chapter 10 will typically be used to access records in various
ways thus we have a chain of classes each with its own responsibilities and access privileges a database query from a user is implemented
by searching an index structure this index requests access to the record
by means of a request to the buffer pool if a record is being inserted or
deleted such a request goes through the memory manager which in turn
interacts with the buffer pool to gain access to the disk file a program such
as this is far too complex for nearly any human programmer to keep all of
the details in his or her head at once the only way to design and implement such a program is through proper use of abstraction and metaphors
in objectoriented programming such abstraction is handled using classes
data types have both a logical and a physical form the definition of the data
type in terms of an adt is its logical form the implementation of the data type as
a data structure is its physical form figure 11 illustrates this relationship between
logical and physical forms for data types when you implement an adt you
are dealing with the physical form of the associated data type when you use an
adt elsewhere in your program you are concerned with the associated data types
logical form some sections of this book focus on physical implementations for a


$$@@$$PAGE: 31
12

chap 1 data structures and algorithms

data type
adt
type
operations

data items
logical form

data structure
storage space
subroutines

data items
physical form

figure 11 the relationship between data items abstract data types and data
structures the adt defines the logical form of the data type the data structure
implements the physical form of the data type

given data structure other sections use the logical adt for the data structure in
the context of a higherlevel task
example 19 a particular c environment might provide a library that
includes a list class the logical form of the list is defined by the public
functions their inputs and their outputs that define the class this might be
all that you know about the list class implementation and this should be all
you need to know within the class a variety of physical implementations
for lists is possible several are described in section 41

13

design patterns

at a higher level of abstraction than adts are abstractions for describing the design
of programs  that is the interactions of objects and classes experienced software
designers learn and reuse patterns for combining software components these have
come to be referred to as design patterns
a design pattern embodies and generalizes important design concepts for a
recurring problem a primary goal of design patterns is to quickly transfer the
knowledge gained by expert designers to newer programmers another goal is
to allow for efficient communication between programmers it is much easier to
discuss a design issue when you share a technical vocabulary relevant to the topic
specific design patterns emerge from the realization that a particular design
problem appears repeatedly in many contexts they are meant to solve real problems design patterns are a bit like templates they describe the structure for a
design solution with the details filled in for any given problem design patterns
are a bit like data structures each one provides costs and benefits which implies


$$@@$$PAGE: 32
sec 13 design patterns

13

that tradeoffs are possible therefore a given design pattern might have variations
on its application to match the various tradeoffs inherent in a given situation
the rest of this section introduces a few simple design patterns that are used
later in the book
131

flyweight

the flyweight design pattern is meant to solve the following problem you have an
application with many objects some of these objects are identical in the information that they contain and the role that they play but they must be reached from
various places and conceptually they really are distinct objects because there is
so much duplication of the same information we would like to take advantage of
the opportunity to reduce memory cost by sharing that space an example comes
from representing the layout for a document the letter c might reasonably be
represented by an object that describes that characters strokes and bounding box
however we do not want to create a separate c object everywhere in the document that a c appears the solution is to allocate a single copy of the shared
representation for c objects then every place in the document that needs a
c in a given font size and typeface will reference this single copy the various
instances of references to a specific form of c are called flyweights
we could describe the layout of text on a page by using a tree structure the
root of the tree represents the entire page the page has multiple child nodes one
for each column the column nodes have child nodes for each row and the rows
have child nodes for each character these representations for characters are the flyweights the flyweight includes the reference to the shared shape information and
might contain additional information specific to that instance for example each
instance for c will contain a reference to the shared information about strokes
and shapes and it might also contain the exact location for that instance of the
character on the page
flyweights are used in the implementation for the pr quadtree data structure
for storing collections of point objects described in section 133 in a pr quadtree
we again have a tree with leaf nodes many of these leaf nodes represent empty
areas and so the only information that they store is the fact that they are empty
these identical nodes can be implemented using a reference to a single instance of
the flyweight for better memory efficiency
132

visitor

given a tree of objects to describe a page layout we might wish to perform some
activity on every node in the tree section 52 discusses tree traversal which is the
process of visiting every node in the tree in a defined order a simple example for
our text composition application might be to count the number of nodes in the tree


$$@@$$PAGE: 33
14

chap 1 data structures and algorithms

that represents the page at another time we might wish to print a listing of all the
nodes for debugging purposes
we could write a separate traversal function for each such activity that we intend to perform on the tree a better approach would be to write a generic traversal
function and pass in the activity to be performed at each node this organization
constitutes the visitor design pattern the visitor design pattern is used in sections 52 tree traversal and 113 graph traversal
133

composite

there are two fundamental approaches to dealing with the relationship between
a collection of actions and a hierarchy of object types first consider the typical
procedural approach say we have a base class for page layout entities with a subclass hierarchy to define specific subtypes page columns rows figures characters etc and say there are actions to be performed on a collection of such objects
such as rendering the objects to the screen the procedural design approach is for
each action to be implemented as a method that takes as a parameter a pointer to
the base class type each action such method will traverse through the collection
of objects visiting each object in turn each action method contains something
like a switch statement that defines the details of the action for each subclass in the
collection eg page column row character we can cut the code down some by
using the visitor design pattern so that we only need to write the traversal once and
then write a visitor subroutine for each action that might be applied to the collection of objects but each such visitor subroutine must still contain logic for dealing
with each of the possible subclasses
in our page composition application there are only a few activities that we
would like to perform on the page representation we might render the objects in
full detail or we might want a rough draft rendering that prints only the bounding boxes of the objects if we come up with a new activity to apply to the collection
of objects we do not need to change any of the code that implements the existing
activities but adding new activities wont happen often for this application in
contrast there could be many object types and we might frequently add new object types to our implementation unfortunately adding a new object type requires
that we modify each activity and the subroutines implementing the activities get
rather long switch statements to distinguish the behavior of the many subclasses
an alternative design is to have each object subclass in the hierarchy embody
the action for each of the various activities that might be performed each subclass
will have code to perform each activity such as full rendering or bounding box
rendering then if we wish to apply the activity to the collection we simply call
the first object in the collection and specify the action as a method call on that
object in the case of our page layout and its hierarchical collection of objects
those objects that contain other objects such as a row objects that contains letters


$$@@$$PAGE: 34
sec 13 design patterns

15

will call the appropriate method for each child if we want to add a new activity
with this organization we have to change the code for every subclass but this is
relatively rare for our text compositing application in contrast adding a new object
into the subclass hierarchy which for this application is far more likely than adding
a new rendering function is easy adding a new subclass does not require changing
any of the existing subclasses it merely requires that we define the behavior of each
activity that can be performed on the new subclass
this second design approach of burying the functional activity in the subclasses
is called the composite design pattern a detailed example for using the composite
design pattern is presented in section 531
134

strategy

our final example of a design pattern lets us encapsulate and make interchangeable
a set of alternative actions that might be performed as part of some larger activity
again continuing our text compositing example each output device that we wish
to render to will require its own function for doing the actual rendering that is
the objects will be broken down into constituent pixels or strokes but the actual
mechanics of rendering a pixel or stroke will depend on the output device we
dont want to build this rendering functionality into the object subclasses instead
we want to pass to the subroutine performing the rendering action a method or class
that does the appropriate rendering details for that output device that is we wish
to hand to the object the appropriate strategy for accomplishing the details of the
rendering task thus this approach is called the strategy design pattern
the strategy design pattern will be discussed further in chapter 7 there a
sorting function is given a class called a comparator that understands how to
extract and compare the key values for records to be sorted in this way the sorting
function does not need to know any details of how its record type is implemented
one of the biggest challenges to understanding design patterns is that sometimes one is only subtly different from another for example you might be confused about the difference between the composite pattern and the visitor pattern
the distinction is that the composite design pattern is about whether to give control
of the traversal process to the nodes of the tree or to the tree itself both approaches
can make use of the visitor design pattern to avoid rewriting the traversal function
many times by encapsulating the activity performed at each node
but isnt the strategy design pattern doing the same thing the difference between the visitor pattern and the strategy pattern is more subtle here the difference
is primarily one of intent and focus in both the strategy design pattern and the visitor design pattern an activity is being passed in as a parameter the strategy design
pattern is focused on encapsulating an activity that is part of a larger process so
that different ways of performing that activity can be substituted the visitor design pattern is focused on encapsulating an activity that will be performed on all


$$@@$$PAGE: 35
16

chap 1 data structures and algorithms

members of a collection so that completely different activities can be substituted
within a generic method that accesses all of the collection members

14

problems algorithms and programs

programmers commonly deal with problems algorithms and computer programs
these are three distinct concepts
problems as your intuition would suggest a problem is a task to be performed
it is best thought of in terms of inputs and matching outputs a problem definition
should not include any constraints on how the problem is to be solved the solution
method should be developed only after the problem is precisely defined and thoroughly understood however a problem definition should include constraints on
the resources that may be consumed by any acceptable solution for any problem
to be solved by a computer there are always such constraints whether stated or
implied for example any computer program may use only the main memory and
disk space available and it must run in a reasonable amount of time
problems can be viewed as functions in the mathematical sense a function
is a matching between inputs the domain and outputs the range an input
to a function might be a single value or a collection of information the values
making up an input are called the parameters of the function a specific selection
of values for the parameters is called an instance of the problem for example
the input parameter to a sorting function might be an array of integers a particular
array of integers with a given size and specific values for each position in the array
would be an instance of the sorting problem different instances might generate the
same output however any problem instance must always result in the same output
every time the function is computed using that particular input
this concept of all problems behaving like mathematical functions might not
match your intuition for the behavior of computer programs you might know of
programs to which you can give the same input value on two separate occasions
and two different outputs will result for example if you type date to a typical
unix command line prompt you will get the current date naturally the date will
be different on different days even though the same command is given however
there is obviously more to the input for the date program than the command that you
type to run the program the date program computes a function in other words
on any particular day there can only be a single answer returned by a properly
running date program on a completely specified input for all computer programs
the output is completely determined by the programs full set of inputs even a
random number generator is completely determined by its inputs although some
random number generating systems appear to get around this by accepting a random
input from a physical process beyond the users control the relationship between
programs and functions is explored further in section 173


$$@@$$PAGE: 36
sec 14 problems algorithms and programs

17

algorithms an algorithm is a method or a process followed to solve a problem
if the problem is viewed as a function then an algorithm is an implementation for
the function that transforms an input to the corresponding output a problem can be
solved by many different algorithms a given algorithm solves only one problem
ie computes a particular function this book covers many problems and for
several of these problems i present more than one algorithm for the important
problem of sorting i present nearly a dozen algorithms
the advantage of knowing several solutions to a problem is that solution a
might be more efficient than solution b for a specific variation of the problem
or for a specific class of inputs to the problem while solution b might be more
efficient than a for another variation or class of inputs for example one sorting
algorithm might be the best for sorting a small collection of integers which is
important if you need to do this many times another might be the best for sorting
a large collection of integers a third might be the best for sorting a collection of
variablelength strings
by definition something can only be called an algorithm if it has all of the
following properties
1 it must be correct in other words it must compute the desired function
converting each input to the correct output note that every algorithm implements some function because every algorithm maps every input to some
output even if that output is a program crash at issue here is whether a
given algorithm implements the intended function
2 it is composed of a series of concrete steps concrete means that the action
described by that step is completely understood  and doable  by the
person or machine that must perform the algorithm each step must also be
doable in a finite amount of time thus the algorithm gives us a recipe for
solving the problem by performing a series of steps where each such step
is within our capacity to perform the ability to perform a step can depend
on who or what is intended to execute the recipe for example the steps of
a cookie recipe in a cookbook might be considered sufficiently concrete for
instructing a human cook but not for programming an automated cookiemaking factory
3 there can be no ambiguity as to which step will be performed next often it
is the next step of the algorithm description selection eg the if statement
in c  is normally a part of any language for describing algorithms selection allows a choice for which step will be performed next but the selection
process is unambiguous at the time when the choice is made
4 it must be composed of a finite number of steps if the description for the algorithm were made up of an infinite number of steps we could never hope to
write it down nor implement it as a computer program most languages for
describing algorithms including english and pseudocode provide some


$$@@$$PAGE: 37
18

chap 1 data structures and algorithms

way to perform repeated actions known as iteration examples of iteration
in programming languages include the while and for loop constructs of
c  iteration allows for short descriptions with the number of steps actually performed controlled by the input
5 it must terminate in other words it may not go into an infinite loop
programs we often think of a computer program as an instance or concrete
representation of an algorithm in some programming language in this book
nearly all of the algorithms are presented in terms of programs or parts of programs naturally there are many programs that are instances of the same algorithm because any modern computer programming language can be used to implement the same collection of algorithms although some programming languages
can make life easier for the programmer to simplify presentation i often use
the terms algorithm and program interchangeably despite the fact that they are
really separate concepts by definition an algorithm must provide sufficient detail
that it can be converted into a program when needed
the requirement that an algorithm must terminate means that not all computer
programs meet the technical definition of an algorithm your operating system is
one such program however you can think of the various tasks for an operating system each with associated inputs and outputs as individual problems each solved
by specific algorithms implemented by a part of the operating system program and
each one of which terminates once its output is produced
to summarize a problem is a function or a mapping of inputs to outputs
an algorithm is a recipe for solving a problem whose steps are concrete and unambiguous algorithms must be correct of finite length and must terminate for all
inputs a program is an instantiation of an algorithm in a programming language

15

further reading

an early authoritative work on data structures and algorithms was the series of
books the art of computer programming by donald e knuth with volumes 1
and 3 being most relevant to the study of data structures knu97 knu98 a modern encyclopedic approach to data structures and algorithms that should be easy
to understand once you have mastered this book is algorithms by robert sedgewick sed11 for an excellent and highly readable but more advanced teaching
introduction to algorithms their design and their analysis see introduction to algorithms a creative approach by udi manber man89 for an advanced encyclopedic approach see introduction to algorithms by cormen leiserson and
rivest clrs09 steven s skienas the algorithm design manual ski10 provides pointers to many implementations for data structures and algorithms that are
available on the web


$$@@$$PAGE: 38
sec 15 further reading

19

the claim that all modern programming languages can implement the same
algorithms stated more precisely any function that is computable by one programming language is computable by any programming language with certain standard
capabilities is a key result from computability theory for an easy introduction to
this field see james l hein discrete structures logic and computability hei09
much of computer science is devoted to problem solving indeed this is what
attracts many people to the field how to solve it by george polya pol57 is considered to be the classic work on how to improve your problemsolving abilities if
you want to be a better student as well as a better problem solver in general see
strategies for creative problem solving by folger and leblanc fl95 effective
problem solving by marvin levine lev94 and problem solving  comprehension by arthur whimbey and jack lochhead wl99 and puzzlebased learning
by zbigniew and matthew michaelewicz mm08
see the origin of consciousness in the breakdown of the bicameral mind by
julian jaynes jay90 for a good discussion on how humans use the concept of
metaphor to handle complexity more directly related to computer science education and programming see cogito ergo sum cognitive processes of students
dealing with data structures by dan aharoni aha00 for a discussion on moving from programmingcontext thinking to higherlevel and more designoriented
programmingfree thinking
on a more pragmatic level most people study data structures to write better
programs if you expect your program to work correctly and efficiently it must
first be understandable to yourself and your coworkers kernighan and pikes the
practice of programming kp99 discusses a number of practical issues related to
programming including good coding and documentation style for an excellent
and entertaining introduction to the difficulties involved with writing large programs read the classic the mythical manmonth essays on software engineering
by frederick p brooks bro95
if you want to be a successful c programmer you need good reference
manuals close at hand the standard reference for c is the c  programming language by bjarne stroustrup str00 with further information provided in
the annotated c  reference manual by ellis and stroustrup es90 no c
programmer should be without stroustrups book as it provides the definitive description of the language and also includes a great deal of information about the
principles of objectoriented design unfortunately it is a poor text for learning
how to program in c  a good gentle introduction to the basics of the language
is patrick henry winstons on to c  win94 a good introductory teaching text
for a wider range of c is deitel and deitels c  how to program dd08
after gaining proficiency in the mechanics of program writing the next step
is to become proficient in program design good design is difficult to learn in any
discipline and good design for objectoriented software is one of the most difficult


$$@@$$PAGE: 39
20

chap 1 data structures and algorithms

of arts the novice designer can jumpstart the learning process by studying wellknown and wellused design patterns the classic reference on design patterns
is design patterns elements of reusable objectoriented software by gamma
helm johnson and vlissides ghjv95 this is commonly referred to as the gang
of four book unfortunately this is an extremely difficult book to understand
in part because the concepts are inherently difficult a number of web sites are
available that discuss design patterns and which provide study guides for the design patterns book two other books that discuss objectoriented software design
are objectoriented software design and construction with c  by dennis kafura kaf98 and objectoriented design heuristics by arthur j riel rie96

16

exercises

the exercises for this chapter are different from those in the rest of the book most
of these exercises are answered in the following chapters however you should
not look up the answers in other parts of the book these exercises are intended to
make you think about some of the issues to be covered later on answer them to
the best of your ability with your current knowledge
11 think of a program you have used that is unacceptably slow identify the specific operations that make the program slow identify other basic operations
that the program performs quickly enough
12 most programming languages have a builtin integer data type normally
this representation has a fixed size thus placing a limit on how large a value
can be stored in an integer variable describe a representation for integers
that has no size restriction other than the limits of the computers available
main memory and thus no practical limit on how large an integer can be
stored briefly show how your representation can be used to implement the
operations of addition multiplication and exponentiation
13 define an adt for character strings your adt should consist of typical
functions that can be performed on strings with each function defined in
terms of its input and output then define two different physical representations for strings
14 define an adt for a list of integers first decide what functionality your
adt should provide example 14 should give you some ideas then specify your adt in c in the form of an abstract class declaration showing
the functions their parameters and their return types
15 briefly describe how integer variables are typically represented on a computer look up ones complement and twos complement arithmetic in an
introductory computer science textbook if you are not familiar with these


$$@@$$PAGE: 40
sec 16 exercises

16

17

18

19
110
111

112

113

114

21

why does this representation for integers qualify as a data structure as defined in section 12
define an adt for a twodimensional array of integers specify precisely
the basic operations that can be performed on such arrays next imagine an
application that stores an array with 1000 rows and 1000 columns where less
than 10000 of the array values are nonzero describe two different implementations for such arrays that would be more space efficient than a standard
twodimensional array implementation requiring one million positions
imagine that you have been assigned to implement a sorting program the
goal is to make this program general purpose in that you dont want to define
in advance what record or key types are used describe ways to generalize
a simple sorting algorithm such as insertion sort or any other sort you are
familiar with to support this generalization
imagine that you have been assigned to implement a simple sequential search
on an array the problem is that you want the search to be as general as possible this means that you need to support arbitrary record and key types
describe ways to generalize the search function to support this goal consider the possibility that the function will be used multiple times in the same
program on differing record types consider the possibility that the function will need to be used on different keys possibly with the same or different types of the same record for example a student data record might be
searched by zip code by name by salary or by gpa
does every problem have an algorithm
does every algorithm have a c program
consider the design for a spelling checker program meant to run on a home
computer the spelling checker should be able to handle quickly a document
of less than twenty pages assume that the spelling checker comes with a
dictionary of about 20000 words what primitive operations must be implemented on the dictionary and what is a reasonable time constraint for each
operation
imagine that you have been hired to design a database service containing
information about cities and towns in the united states as described in example 12 suggest two possible implementations for the database
imagine that you are given an array of records that is sorted with respect to
some key field contained in each record give two different algorithms for
searching the array to find the record with a specified key value which one
do you consider better and why
how would you go about comparing two proposed algorithms for sorting an
array of integers in particular
a what would be appropriate measures of cost to use as a basis for comparing the two sorting algorithms


$$@@$$PAGE: 41
22

chap 1 data structures and algorithms

b what tests or analysis would you conduct to determine how the two
algorithms perform under these cost measures
115 a common problem for compilers and text editors is to determine if the
parentheses or other brackets in a string are balanced and properly nested
for example the string  contains properly nested pairs of parentheses but the string  does not and the string  does not contain
properly matching parentheses
a give an algorithm that returns true if a string contains properly nested
and balanced parentheses and false if otherwise hint at no time
while scanning a legal string from left to right will you have encountered more right parentheses than left parentheses
b give an algorithm that returns the position in the string of the first offending parenthesis if the string is not properly nested and balanced
that is if an excess right parenthesis is found return its position if
there are too many left parentheses return the position of the first excess left parenthesis return 1 if the string is properly balanced and
nested
116 a graph consists of a set of objects called vertices and a set of edges where
each edge connects two vertices any given pair of vertices can be connected
by only one edge describe at least two different ways to represent the connections defined by the vertices and edges of a graph
117 imagine that you are a shipping clerk for a large company you have just
been handed about 1000 invoices each of which is a single sheet of paper
with a large number in the upper right corner the invoices must be sorted by
this number in order from lowest to highest write down as many different
approaches to sorting the invoices as you can think of
118 how would you sort an array of about 1000 integers from lowest value to
highest value write down at least five approaches to sorting the array do
not write algorithms in c or pseudocode just write a sentence or two for
each approach to describe how it would work
119 think of an algorithm to find the maximum value in an unsorted array
now think of an algorithm to find the second largest value in the array
which is harder to implement which takes more time to run as measured
by the number of comparisons performed now think of an algorithm to
find the third largest value finally think of an algorithm to find the middle
value which is the most difficult of these problems to solve
120 an unsorted list allows for constanttime insert by adding a new element at
the end of the list unfortunately searching for the element with key value x
requires a sequential search through the unsorted list until x is found which
on average requires looking at half the list element on the other hand a


$$@@$$PAGE: 42
sec 16 exercises

23

sorted arraybased list of n elements can be searched in log n time with a
binary search unfortunately inserting a new element requires a lot of time
because many elements might be shifted in the array if we want to keep it
sorted how might data be organized to support both insertion and search in
log n time


$$@@$$PAGE: 43

$$@@$$PAGE: 44
2
mathematical preliminaries

this chapter presents mathematical notation background and techniques used
throughout the book this material is provided primarily for review and reference
you might wish to return to the relevant sections when you encounter unfamiliar
notation or mathematical techniques in later chapters
section 27 on estimation might be unfamiliar to many readers estimation is
not a mathematical technique but rather a general engineering skill it is enormously useful to computer scientists doing design work because any proposed
solution whose estimated resource requirements fall well outside the problems resource constraints can be discarded immediately allowing time for greater analysis
of more promising solutions

21

sets and relations

the concept of a set in the mathematical sense has wide application in computer
science the notations and techniques of set theory are commonly used when describing and implementing algorithms because the abstractions associated with sets
often help to clarify and simplify algorithm design
a set is a collection of distinguishable members or elements the members
are typically drawn from some larger population known as the base type each
member of a set is either a primitive element of the base type or is a set itself
there is no concept of duplication in a set each value from the base type is either
in the set or not in the set for example a set named p might consist of the three
integers 7 11 and 42 in this case ps members are 7 11 and 42 and the base
type is integer
figure 21 shows the symbols commonly used to express sets and their relationships here are some examples of this notation in use first define two sets p
and q
p  2 3 5
q  5 10
25


$$@@$$PAGE: 45
26

chap 2 mathematical preliminaries

1 4
x  x is a positive integer
xp
x
p

p
p  q q  p

pq
pq
pq

a set composed of the members 1 and 4
a set definition using a set former
example the set of all positive integers
x is a member of set p
x is not a member of set p
the null or empty set
cardinality size of set p
or number of members for set p
set p is included in set q
set p is a subset of set q
set q is a superset of set p
set union
all elements appearing in p or q
set intersection
all elements appearing in p and q
set difference
all elements of set p not in set q

figure 21 set notation

p  3 because p has three members and q  2 because q has two members
the union of p and q written p  q is the set of elements in either p or q which
is 2 3 5 10 the intersection of p and q written p  q is the set of elements
that appear in both p and q which is 5 the set difference of p and q written
p  q is the set of elements that occur in p but not in q which is 2 3 note
that p  q  q  p and that p  q  q  p but in general p  q 6 q  p
in this example q  p  10 note that the set 4 3 5 is indistinguishable
from set p because sets have no concept of order likewise set 4 3 4 5 is also
indistinguishable from p because sets have no concept of duplicate elements
the powerset of a set s is the set of all possible subsets for s consider the set
s  a b c the powerset of s is
 a b c a b a c b c a b c
a collection of elements with no order like a set but with duplicatevalued elements is called a bag1 to distinguish bags from sets i use square brackets 
around a bags elements for example bag 3 4 5 4 is distinct from bag 3 4 5
while set 3 4 5 4 is indistinguishable from set 3 4 5 however bag 3 4 5
4 is indistinguishable from bag 3 4 4 5
1

the object referred to here as a bag is sometimes called a multilist but i reserve the term
multilist for a list that may contain sublists see section 121


$$@@$$PAGE: 46
27

sec 21 sets and relations

a sequence is a collection of elements with an order and which may contain
duplicatevalued elements a sequence is also sometimes called a tuple or a vector in a sequence there is a 0th element a 1st element 2nd element and so
on i indicate a sequence by using angle brackets hi to enclose its elements for
example h3 4 5 4i is a sequence note that sequence h3 5 4 4i is distinct from
sequence h3 4 5 4i and both are distinct from sequence h3 4 5i
a relation r over set s is a set of ordered pairs from s as an example of a
relation if s is a b c then
ha ci hb ci hc bi
is a relation and
ha ai ha ci hb bi hb ci hc ci
is a different relation if tuple hx yi is in relation r we may use the infix notation
xry we often use relations such as the less than operator  on the natural
numbers which includes ordered pairs such as h1 3i and h2 23i but not h3 2i or
h2 2i rather than writing the relationship in terms of ordered pairs we typically
use an infix notation for such relations writing 1  3
define the properties of relations as follows with r a binary relation over set s





r is reflexive if ara for all a  s
r is symmetric if whenever arb then bra for all a b  s
r is antisymmetric if whenever arb and bra then a  b for all a b  s
r is transitive if whenever arb and brc then arc for all a b c  s

as examples for the natural numbers  is antisymmetric because there is
no case where arb and bra and transitive  is reflexive antisymmetric and
transitive and  is reflexive symmetric and antisymmetric and transitive for
people the relation is a sibling of is symmetric and transitive if we define a
person to be a sibling of himself then it is reflexive if we define a person not to be
a sibling of himself then it is not reflexive
r is an equivalence relation on set s if it is reflexive symmetric and transitive
an equivalence relation can be used to partition a set into equivalence classes if
two elements a and b are equivalent to each other we write a  b a partition of
a set s is a collection of subsets that are disjoint from each other and whose union
is s an equivalence relation on set s partitions the set into subsets whose elements
are equivalent see section 62 for a discussion on how to represent equivalence
classes on a set one application for disjoint sets appears in section 1152
example 21 for the integers  is an equivalence relation that partitions
each element into a distinct subset in other words for any integer a three
things are true
1 a  a


$$@@$$PAGE: 47
28

chap 2 mathematical preliminaries

2 if a  b then b  a and
3 if a  b and b  c then a  c
of course for distinct integers a b and c there are never cases where
a  b b  a or b  c so the claims that  is symmetric and transitive are
vacuously true there are never examples in the relation where these events
occur but because the requirements for symmetry and transitivity are not
violated the relation is symmetric and transitive

example 22 if we clarify the definition of sibling to mean that a person
is a sibling of him or herself then the sibling relation is an equivalence
relation that partitions the set of people

example 23 we can use the modulus function defined in the next section to define an equivalence relation for the set of integers use the modulus function to define a binary relation such that two numbers x and y are
in the relation if and only if x mod m  y mod m thus for m  4
h1 5i is in the relation because 1 mod 4  5 mod 4 we see that modulus
used in this way defines an equivalence relation on the integers and this relation can be used to partition the integers into m equivalence classes this
relation is an equivalence relation because
1 x mod m  x mod m for all x
2 if x mod m  y mod m then y mod m  x mod m and
3 if x mod m  y mod m and y mod m  z mod m then x mod
m  z mod m
a binary relation is called a partial order if it is antisymmetric and transitive2
the set on which the partial order is defined is called a partially ordered set or a
poset elements x and y of a set are comparable under a given relation if either
xry or yrx if every pair of distinct elements in a partial order are comparable
then the order is called a total order or linear order
example 24 for the integers relations  and  define partial orders
operation  is a total order because for every pair of integers x and y such
that x 6 y either x  y or y  x likewise  is a total order because for
every pair of integers x and y such that x 6 y either x  y or y  x
2

not all authors use this definition for partial order i have seen at least three significantly different
definitions in the literature i have selected the one that lets  and  both define partial orders on the
integers because this seems the most natural to me


$$@@$$PAGE: 48
sec 22 miscellaneous notation

29

example 25 for the powerset of the integers the subset operator defines
a partial order because it is antisymmetric and transitive for example
1 2  1 2 3 however sets 1 2 and 1 3 are not comparable by
the subset operator because neither is a subset of the other therefore the
subset operator does not define a total order on the powerset of the integers

22

miscellaneous notation

units of measure i use the following notation for units of measure b will
be used as an abbreviation for bytes b for bits kb for kilobytes 210 
1024 bytes mb for megabytes 220 bytes gb for gigabytes 230 bytes and
1
ms for milliseconds a millisecond is 1000
of a second spaces are not placed between the number and the unit abbreviation when a power of two is intended thus
a disk drive of size 25 gigabytes where a gigabyte is intended as 230 bytes will be
written as 25gb spaces are used when a decimal value is intended an amount
of 2000 bits would therefore be written 2 kb while 2kb represents 2048 bits
2000 milliseconds is written as 2000 ms note that in this book large amounts of
storage are nearly always measured in powers of two and times in powers of ten
factorial function the factorial function written n for n an integer greater
than 0 is the product of the integers between 1 and n inclusive thus 5 
1  2  3  4  5  120 as a special case 0  1 the factorial function grows
quickly as n becomes larger because computing the factorial function directly
is a timeconsuming process it can be useful to have an equation
 that provides a
good approximation stirlings approximation states that n  2n ne n  where
e  271828 e is the base for the systemof natural logarithms3 thus we see that
while n grows slower than nn because 2nen  1 it grows faster than cn for
any positive integer constant c
permutations a permutation of a sequence s is simply the members of s arranged in some order for example a permutation of the integers 1 through n
would be those values arranged in some order if the sequence contains n distinct
members then there are n different permutations for the sequence this is because
there are n choices for the first member in the permutation for each choice of first
member there are n  1 choices for the second member and so on sometimes
one would like to obtain a random permutation for a sequence that is one of the
n possible permutations is selected in such a way that each permutation has equal
probability of being selected a simple c function for generating a random permutation is as follows here the n values of the sequence are stored in positions 0
3

the symbol  means approximately equal


$$@@$$PAGE: 49
30

chap 2 mathematical preliminaries

through n  1 of array a function swapa i j exchanges elements i and
j in array a and randomn returns an integer value in the range 0 to n  1 see
the appendix for more information on swap and random
 randomly permute the n values of array a
templatetypename e
void permutee a int n 
for int in i0 i
swapa i1 randomi


boolean variables a boolean variable is a variable of type bool in c 
that takes on one of the two values true and false these two values are often
associated with the values 1 and 0 respectively although there is no reason why
this needs to be the case it is poor programming practice to rely on the correspondence between 0 and false because these are logically distinct objects of
different types
logic notation we will occasionally make use of the notation of symbolic or
boolean logic a  b means a implies b or if a then b a  b means a
if and only if b or a is equivalent to b a  b means a or b useful both in
the context of symbolic logic or when performing a boolean operation a  b
means a and b  a and a both mean not a or the negation of a where a is a
boolean variable
floor and ceiling the floor of x written bxc takes real value x and returns the
greatest integer  x for example b34c  3 as does b30c while b34c  4
and b30c  3 the ceiling of x written dxe takes real value x and returns
the least integer  x for example d34e  4 as does d40e while d34e 
d30e  3
modulus operator the modulus or mod function returns the remainder of an
integer division sometimes written n mod m in mathematical expressions the
syntax for the c modulus operator is n  m from the definition of remainder
n mod m is the integer r such that n  qm  r for q an integer and r  m
therefore the result of n mod m must be between 0 and m  1 when n and m are
positive integers for example 5 mod 3  2 25 mod 3  1 5 mod 7  5 and
5 mod 5  0
there is more than one way to assign values to q and r depending on how integer division is interpreted the most common mathematical definition computes
the mod function as n mod m  n  mbnmc in this case 3 mod 5  2
however java and c compilers typically use the underlying processors machine instruction for computing integer arithmetic on many computers this is done
by truncating the resulting fraction meaning n mod m  n  mtruncnm
under this definition 3 mod 5  3


$$@@$$PAGE: 50
sec 23 logarithms

31

unfortunately for many applications this is not what the user wants or expects
for example many hash systems will perform some computation on a records key
value and then take the result modulo the hash table size the expectation here
would be that the result is a legal index into the hash table not a negative number
implementers of hash functions must either insure that the result of the computation
is always positive or else add the hash table size to the result of the modulo function
when that result is negative

23

logarithms

a logarithm of base b for value y is the power to which b is raised to get y normally this is written as logb y  x thus if logb y  x then bx  y and blogb y  y
logarithms are used frequently by programmers here are two typical uses
example 26 many programs require an encoding for a collection of objects what is the minimum number of bits needed to represent n distinct
code values the answer is dlog2 ne bits for example if you have 1000
codes to store you will require at least dlog2 1000e  10 bits to have 1000
different codes 10 bits provide 1024 distinct code values

example 27 consider the binary search algorithm for finding a given
value within an array sorted by value from lowest to highest binary search
first looks at the middle element and determines if the value being searched
for is in the upper half or the lower half of the array the algorithm then
continues splitting the appropriate subarray in half until the desired value
is found binary search is described in more detail in section 35 how
many times can an array of size n be split in half until only one element
remains in the final subarray the answer is dlog2 ne times
in this book nearly all logarithms used have a base of two this is because
data structures and algorithms most often divide things in half or store codes with
binary bits whenever you see the notation log n in this book either log2 n is meant
or else the term is being used asymptotically and so the actual base does not matter
logarithms using any base other than two will show the base explicitly
logarithms have the following properties for any positive values of m n and
r and any positive integers a and b
1
2
3
4

lognm  log n  log m
lognm  log n  log m
lognr   r log n
loga n  logb n logb a


$$@@$$PAGE: 51
32

chap 2 mathematical preliminaries

the first two properties state that the logarithm of two numbers multiplied or
divided can be found by adding or subtracting the logarithms of the two numbers4 property 3 is simply an extension of property 1 property 4 tells us that
for variable n and any two integer constants a and b loga n and logb n differ by
the constant factor logb a regardless of the value of n most runtime analyses in
this book are of a type that ignores constant factors in costs property 4 says that
such analyses need not be concerned with the base of the logarithm because this
can change the total cost only by a constant factor note that 2log n  n
when discussing logarithms exponents often lead to confusion property 3
tells us that log n2  2 log n how do we indicate the square of the logarithm
as opposed to the logarithm of n2  this could be written as log n2  but it is
traditional to use log2 n on the other hand we might want to take the logarithm of
the logarithm of n this is written log log n
a special notation is used in the rare case when we need to know how many
times we must take the log of a number before we reach a value  1 this quantity
is written log n for example log 1024  4 because log 1024  10 log 10 
333 log 333  174 and log 174  1 which is a total of 4 log operations

24

summations and recurrences

most programs contain loop constructs when analyzing running time costs for
programs with loops we need to add up the costs for each time the loop is executed
this is an example of a summation summations are simply the sum of costs for
some function applied to a range of parameter values summations are typically
written with the following sigma notation
n
x

f i

i1

this notation indicates that we are summing the value of f i over some range of
integer values
the parameter to the expression and its initial value are indicated
p
below the
symbol here the notation i  1 indicates
that the parameter is i and
p
that it begins with the value 1 at the top of the symbol is the expression n this
indicates the maximum value for the parameter i thus this notation means to sum
the values of f i as i ranges across the integers from 1 through n this can also be
4
these properties are the idea behind the slide rule adding two numbers can be viewed as
joining two lengths together and measuring their combined length multiplication is not so easily
done however if the numbers are first converted to the lengths of their logarithms then those lengths
can be added and the inverse logarithm of the resulting length gives the answer for the multiplication
this is simply logarithm property 1 a slide rule measures the length of the logarithm for the
numbers lets you slide bars representing these lengths to add up the total length and finally converts
this total length to the correct numeric answer by taking the inverse of the logarithm for the result


$$@@$$PAGE: 52
33

sec 24 summations and recurrences

written f 1 p
 f 2      f n  1  f n within a sentence sigma notation
is typeset as ni1 f i
given a summation you often wish to replace it with an algebraic equation
with the same value as the summation this is known as a closedform solution
and the process of replacing the summation with its closedform
solution is known
p
as solving the summation for example the summation ni1 1 is simply the expression 1 summed n times remember that i ranges from 1 to n because the
sum of n 1s is n the closedform solution is n the following is a list of useful
summations along with their closedform solutions
n
x

i 

i1
n
x

i2 

i1
log
xn
i1

x

21

2n3  3n2  n
n2n  1n  1


6
6

22

n  n log n

23

ai 

1
for 0  a  1
1a

24

ai 

an1  1
for a 6 1
a1

25

i0
n
x

nn  1

2

i0

as special cases to equation 25
n
x
1
1
 1  n
i
2
2

26

i1

and

n
x

2i  2n1  1

27

i0

as a corollary to equation 27
log
xn
2i  2log n1  1  2n  1

28

i0

finally
n
x
i
2i
i1

 2

n2

2n

29

the sum of reciprocals from 1 to n called the harmonic series and written
hn  has a value between loge n and loge n  1 to be more precise as n grows the


$$@@$$PAGE: 53
34

chap 2 mathematical preliminaries

summation grows closer to
hn  loge n   

1

2n

210

where  is eulers constant and has the value 05772
most of these equalities can be proved easily by mathematical induction see
section 263 unfortunately induction does not help us derive a closedform solution it only confirms when a proposed closedform solution is correct techniques
for deriving closedform solutions are discussed in section 141
the running time for a recursive algorithm is most easily expressed by a recursive expression because the total time for the recursive algorithm includes the time
to run the recursive calls a recurrence relation defines a function by means
of an expression that includes one or more smaller instances of itself a classic
example is the recursive definition for the factorial function
n  n  1  n for n  1

1  0  1

another standard example of a recurrence is the fibonacci sequence
fibn  fibn  1  fibn  2 for n  2

fib1  fib2  1

from this definition the first seven numbers of the fibonacci sequence are
1 1 2 3 5 8 and 13
notice that this definition contains two parts the general definition for fibn and
the base cases for fib1 and fib2 likewise the definition for factorial contains
a recursive part and base cases
recurrence relations are often used to model the cost of recursive functions for
example the number of multiplications required by function fact of section 25
for an input of size n will be zero when n  0 or n  1 the base cases and it will
be one plus the cost of calling fact on a value of n  1 this can be defined using
the following recurrence
tn  tn  1  1 for n  1

t0  t1  0

as with summations we typically wish to replace the recurrence relation with
a closedform solution one approach is to expand the recurrence by replacing any
occurrences of t on the righthand side with its definition
example 28 if we expand the recurrence tn  tn  1  1 we get
tn  tn  1  1
 tn  2  1  1


$$@@$$PAGE: 54
35

sec 24 summations and recurrences

we can expand the recurrence as many steps as we like but the goal is
to detect some pattern that will permit us to rewrite the recurrence in terms
of a summation in this example we might notice that
tn  2  1  1  tn  2  2
and if we expand the recurrence again we get
tn  tn  2  2  tn  3  1  2  tn  3  3
which generalizes to the pattern tn  tn  i  i we might conclude
that
tn  tn  n  1  n  1
 t1  n  1
 n  1
because we have merely guessed at a pattern and not actually proved
that this is the correct closed form solution we should use an induction
proof to complete the process see example 213

example 29 a slightly more complicated recurrence is
tn  tn  1  n

t 1  1

expanding this recurrence a few steps we get
tn  tn  1  n
 tn  2  n  1  n
 tn  3  n  2  n  1  n

we should then observe that this recurrence appears to have a pattern that
leads to
tn  tn  n  1  n  n  2      n  1  n
 1  2      n  1  n
p
this is equivalent to the summation ni1 i for which we already know the
closedform solution
techniques to find closedform solutions for recurrence relations are discussed
in section 142 prior to chapter 14 recurrence relations are used infrequently in
this book and the corresponding closedform solution and an explanation for how
it was derived will be supplied at the time of use


$$@@$$PAGE: 55
36

25

chap 2 mathematical preliminaries

recursion

an algorithm is recursive if it calls itself to do part of its work for this approach
to be successful the call to itself must be on a smaller problem then the one
originally attempted in general a recursive algorithm must have two parts the
base case which handles a simple input that can be solved without resorting to
a recursive call and the recursive part which contains one or more recursive calls
to the algorithm where the parameters are in some sense closer to the base case
than those of the original call here is a recursive c function to compute the
factorial of n a trace of facts execution for a small value of n is presented in
section 424
long factint n 
 compute n recursively
 to fit n into a long variable we require n  12
assertn  0  n  12 input out of range
if n  1 return 1  base case return base solution
return n  factn1  recursive call for n  1


the first two lines of the function constitute the base cases if n  1 then one
of the base cases computes a solution for the problem if n  1 then fact calls
a function that knows how to find the factorial of n  1 of course the function
that knows how to compute the factorial of n  1 happens to be fact itself but
we should not think too hard about this while writing the algorithm the design
for recursive algorithms can always be approached in this way first write the base
cases then think about solving the problem by combining the results of one or
more smaller  but similar  subproblems if the algorithm you write is correct
then certainly you can rely on it recursively to solve the smaller subproblems
the secret to success is do not worry about how the recursive call solves the
subproblem simply accept that it will solve it correctly and use this result to in
turn correctly solve the original problem what could be simpler
recursion has no counterpart in everyday physicalworld problem solving the
concept can be difficult to grasp because it requires you to think about problems in
a new way to use recursion effectively it is necessary to train yourself to stop
analyzing the recursive process beyond the recursive call the subproblems will
take care of themselves you just worry about the base cases and how to recombine
the subproblems
the recursive version of the factorial function might seem unnecessarily complicated to you because the same effect can be achieved by using a while loop
here is another example of recursion based on a famous puzzle called towers of
hanoi the natural algorithm to solve this problem has multiple recursive calls it
cannot be rewritten easily using while loops
the towers of hanoi puzzle begins with three poles and n rings where all rings
start on the leftmost pole labeled pole 1 the rings each have a different size and


$$@@$$PAGE: 56
37

sec 25 recursion

a

b

figure 22 towers of hanoi example a the initial conditions for a problem
with six rings b a necessary intermediate step on the road to a solution

are stacked in order of decreasing size with the largest ring at the bottom as shown
in figure 22a the problem is to move the rings from the leftmost pole to the
rightmost pole labeled pole 3 in a series of steps at each step the top ring on
some pole is moved to another pole there is one limitation on where rings may be
moved a ring can never be moved on top of a smaller ring
how can you solve this problem it is easy if you dont think too hard about
the details instead consider that all rings are to be moved from pole 1 to pole 3
it is not possible to do this without first moving the bottom largest ring to pole 3
to do that pole 3 must be empty and only the bottom ring can be on pole 1
the remaining n  1 rings must be stacked up in order on pole 2 as shown in
figure 22b how can you do this assume that a function x is available to
solve the problem of moving the top n  1 rings from pole 1 to pole 2 then move
the bottom ring from pole 1 to pole 3 finally again use function x to move the
remaining n  1 rings from pole 2 to pole 3 in both cases function x is simply
the towers of hanoi function called on a smaller version of the problem
the secret to success is relying on the towers of hanoi algorithm to do the
work for you you need not be concerned about the gory details of how the towers
of hanoi subproblem will be solved that will take care of itself provided that two
things are done first there must be a base case what to do if there is only one
ring so that the recursive process will not go on forever second the recursive call
to towers of hanoi can only be used to solve a smaller problem and then only one
of the proper form one that meets the original definition for the towers of hanoi
problem assuming appropriate renaming of the poles
here is an implementation for the recursive towers of hanoi algorithm function movestart goal takes the top ring from pole start and moves it to
pole goal if move were to print the values of its parameters then the result of
calling toh would be a list of ringmoving instructions that solves the problem


$$@@$$PAGE: 57
38

chap 2 mathematical preliminaries

void tohint n pole start pole goal pole temp 
if n  0 return
 base case
tohn1 start temp goal  recursive call n1 rings
movestart goal
 move bottom disk to goal
tohn1 temp goal start  recursive call n1 rings


those who are unfamiliar with recursion might find it hard to accept that it is
used primarily as a tool for simplifying the design and description of algorithms
a recursive algorithm usually does not yield the most efficient computer program
for solving the problem because recursion involves function calls which are typically more expensive than other alternatives such as a while loop however the
recursive approach usually provides an algorithm that is reasonably efficient in the
sense discussed in chapter 3 but not always see exercise 211 if necessary
the clear recursive solution can later be modified to yield a faster implementation
as described in section 424
many data structures are naturally recursive in that they can be defined as being made up of selfsimilar parts tree structures are an example of this thus
the algorithms to manipulate such data structures are often presented recursively
many searching and sorting algorithms are based on a strategy of divide and conquer that is a solution is found by breaking the problem into smaller similar
subproblems solving the subproblems then combining the subproblem solutions
to form the solution to the original problem this process is often implemented
using recursion thus recursion plays an important role throughout this book and
many more examples of recursive functions will be given

26

mathematical proof techniques

solving any problem has two distinct parts the investigation and the argument
students are too used to seeing only the argument in their textbooks and lectures
but to be successful in school and in life after school one needs to be good at
both and to understand the differences between these two phases of the process
to solve the problem you must investigate successfully that means engaging the
problem and working through until you find a solution then to give the answer
to your client whether that client be your instructor when writing answers on
a homework assignment or exam or a written report to your boss you need to
be able to make the argument in a way that gets the solution across clearly and
succinctly the argument phase involves good technical writing skills  the ability
to make a clear logical argument
being conversant with standard proof techniques can help you in this process
knowing how to write a good proof helps in many ways first it clarifies your
thought process which in turn clarifies your explanations second if you use one of
the standard proof structures such as proof by contradiction or an induction proof


$$@@$$PAGE: 58
sec 26 mathematical proof techniques

39

then both you and your reader are working from a shared understanding of that
structure that makes for less complexity to your reader to understand your proof
because the reader need not decode the structure of your argument from scratch
this section briefly introduces three commonly used proof techniques i deduction or direct proof ii proof by contradiction and iii proof by mathematical
induction
261

direct proof

in general a direct proof is just a logical explanation a direct proof is sometimes referred to as an argument by deduction this is simply an argument in terms
of logic often written in english with words such as if  then it could also
be written with logic notation such as p  q even if we dont wish to use
symbolic logic notation we can still take advantage of fundamental theorems of
logic to structure our arguments for example if we want to prove that p and q
are equivalent we can first prove p  q and then prove q  p 
in some domains proofs are essentially a series of state changes from a start
state to an end state formal predicate logic can be viewed in this way with the various rules of logic being used to make the changes from one formula or combining
a couple of formulas to make a new formula on the route to the destination symbolic manipulations to solve integration problems in introductory calculus classes
are similar in spirit as are high school geometry proofs
262

proof by contradiction

the simplest way to disprove a theorem or statement is to find a counterexample
to the theorem unfortunately no number of examples supporting a theorem is
sufficient to prove that the theorem is correct however there is an approach that
is vaguely similar to disproving by counterexample called proof by contradiction
to prove a theorem by contradiction we first assume that the theorem is false we
then find a logical contradiction stemming from this assumption if the logic used
to find the contradiction is correct then the only way to resolve the contradiction is
to recognize that the assumption that the theorem is false must be incorrect that
is we conclude that the theorem must be true
example 210 here is a simple proof by contradiction
theorem 21 there is no largest integer
proof proof by contradiction
step 1 contrary assumption assume that there is a largest integer
call it b for biggest
step 2 show this assumption leads to a contradiction consider
c  b  1 c is an integer because it is the sum of two integers also


$$@@$$PAGE: 59
40

chap 2 mathematical preliminaries

c  b which means that b is not the largest integer after all thus we
have reached a contradiction the only flaw in our reasoning is the initial
assumption that the theorem is false thus we conclude that the theorem is
correct
2
a related proof technique is proving the contrapositive we can prove that
p  q by proving not q  not p 
263

proof by mathematical induction

mathematical induction can be used to prove a wide variety of theorems induction
also provides a useful way to think about algorithm design because it encourages
you to think about solving a problem by building up from simple subproblems
induction can help to prove that a recursive function produces the correct result
understanding recursion is a big step toward understanding induction and vice
versa since they work by essentially the same process
within the context of algorithm analysis one of the most important uses for
mathematical induction is as a method to test a hypothesis as explained in section 24 when seeking a closedform solution for a summation or recurrence we
might first guess or otherwise acquire evidence that a particular formula is the correct solution if the formula is indeed correct it is often an easy matter to prove
that fact with an induction proof
let thrm be a theorem to prove and express thrm in terms of a positive
integer parameter n mathematical induction states that thrm is true for any value
of parameter n for n  c where c is some constant if the following two conditions
are true
1 base case thrm holds for n  c and
2 induction step if thrm holds for n  1 then thrm holds for n
proving the base case is usually easy typically requiring that some small value
such as 1 be substituted for n in the theorem and applying simple algebra or logic
as necessary to verify the theorem proving the induction step is sometimes easy
and sometimes difficult an alternative formulation of the induction step is known
as strong induction the induction step for strong induction is
2a induction step if thrm holds for all k c  k  n then thrm holds for n
proving either variant of the induction step in conjunction with verifying the base
case yields a satisfactory proof by mathematical induction
the two conditions that make up the induction proof combine to demonstrate
that thrm holds for n  2 as an extension of the fact that thrm holds for n  1
this fact combined again with condition 2 or 2a indicates that thrm also holds


$$@@$$PAGE: 60
41

sec 26 mathematical proof techniques

for n  3 and so on thus thrm holds for all values of n larger than the base
cases once the two conditions have been proved
what makes mathematical induction so powerful and so mystifying to most
people at first is that we can take advantage of the assumption that thrm holds
for all values less than n as a tool to help us prove that thrm holds for n this is
known as the induction hypothesis having this assumption to work with makes
the induction step easier to prove than tackling the original theorem itself being
able to rely on the induction hypothesis provides extra information that we can
bring to bear on the problem
recursion and induction have many similarities both are anchored on one or
more base cases a recursive function relies on the ability to call itself to get the
answer for smaller instances of the problem likewise induction proofs rely on the
truth of the induction hypothesis to prove the theorem the induction hypothesis
does not come out of thin air it is true if and only if the theorem itself is true and
therefore is reliable within the proof context using the induction hypothesis it do
work is exactly the same as using a recursive call to do work
example 211 here is a sample proof by mathematical induction call
the sum of the first n positive integers sn
theorem 22 sn  nn  12
proof the proof is by mathematical induction
1 check the base case for n  1 verify that s1  11  12 s1
is simply the sum of the first positive number which is 1 because
11  12  1 the formula is correct for the base case
2 state the induction hypothesis the induction hypothesis is
sn  1 

n1
x

i

i1

n  1n  1  1
n  1n


2
2

3 use the assumption from the induction hypothesis for n  1 to
show that the result is true for n the induction hypothesis states
that sn  1  n  1n2 and because sn  sn  1  n
we can substitute for sn  1 to get

n
n1
x
x
n  1n
n
i 
i n
2
i1

i1

n2  n  2n
nn  1



2
2
thus by mathematical induction
sn 

n
x
i1

i  nn  12


$$@@$$PAGE: 61
42

chap 2 mathematical preliminaries

2
note carefully what took place in this example first we cast sn in terms
of a smaller occurrence of the problem sn  sn  1  n this is important
because once sn  1 comes into the picture we can use the induction hypothesis
to replace sn  1 with n  1n2 from here it is simple algebra to prove
that sn  1  n equals the righthand side of the original theorem
example 212 here is another simple proof by induction that illustrates
choosing the proper variable for induction we wish to prove by induction
that the sum of the first n positive odd numbers is n2  first we need a way
to describe the nth odd number which is simply 2n  1 this also allows
us to cast the theorem as a summation
pn
2
theorem 23
i1 2i  1  n 
proof the base case of n  1 yields 1  12  which is true the induction
hypothesis is
n1
x
2i  1  n  12 
i1

we now use the induction hypothesis to show that the theorem holds true
for n the sum of the first n odd numbers is simply the sum of the first
n  1 odd numbers plus the nth odd number in the second line below we
will use the induction hypothesis to replace the partial summation shown
in brackets in the first line with its closedform solution after that algebra
takes care of the rest
n1

n
x
x
2i  1 
2i  1  2n  1
i1

i1

 n  12   2n  1
 n2  2n  1  2n  1
 n2 

thus by mathematical induction

pn

i1 2i

 1  n2 

2

example 213 this example shows how we can use induction to prove
that a proposed closedform solution for a recurrence relation is correct
theorem 24 the recurrence relation tn  tn11
has closedform solution tn  n  1

t1  0


$$@@$$PAGE: 62
sec 26 mathematical proof techniques

proof to prove the base case we observe that t1  1  1  0 the
induction hypothesis is that tn  1  n  2 combining the definition
of the recurrence with the induction hypothesis we see immediately that
tn  tn  1  1  n  2  1  n  1
for n  1 thus we have proved the theorem correct by mathematical
induction
2

example 214 this example uses induction without involving summations or other equations it also illustrates a more flexible use of base cases
theorem 25 2 and 5 stamps can be used to form any value for values
 4
proof the theorem defines the problem for values  4 because it does
not hold for the values 1 and 3 using 4 as the base case a value of 4
can be made from two 2 stamps the induction hypothesis is that a value
of n  1 can be made from some combination of 2 and 5 stamps we
now use the induction hypothesis to show how to get the value n from 2
and 5 stamps either the makeup for value n  1 includes a 5 stamp or
it does not if so then replace a 5 stamp with three 2 stamps if not
then the makeup must have included at least two 2 stamps because it is
at least of size 4 and contains only 2 stamps in this case replace two of
the 2 stamps with a single 5 stamp in either case we now have a value
of n made up of 2 and 5 stamps thus by mathematical induction the
theorem is correct
2

example 215 here is an example using strong induction
theorem 26 for n  1 n is divisible by some prime number
proof for the base case choose n  2 2 is divisible by the prime number 2 the induction hypothesis is that any value a 2  a  n is divisible
by some prime number there are now two cases to consider when proving
the theorem for n if n is a prime number then n is divisible by itself if n
is not a prime number then n  a  b for a and b both integers less than
n but greater than 1 the induction hypothesis tells us that a is divisible by
some prime number that same prime number must also divide n thus
by mathematical induction the theorem is correct
2

43


$$@@$$PAGE: 63
44

chap 2 mathematical preliminaries

figure 23 a twocoloring for the regions formed by three lines in the plane

our next example of mathematical induction proves a theorem from geometry
it also illustrates a standard technique of induction proof where we take n objects
and remove some object to use the induction hypothesis
example 216 define a twocoloring for a set of regions as a way of assigning one of two colors to each region such that no two regions sharing a
side have the same color for example a chessboard is twocolored figure 23 shows a twocoloring for the plane with three lines we will assume
that the two colors to be used are black and white
theorem 27 the set of regions formed by n infinite lines in the plane can
be twocolored
proof consider the base case of a single infinite line in the plane this line
splits the plane into two regions one region can be colored black and the
other white to get a valid twocoloring the induction hypothesis is that the
set of regions formed by n  1 infinite lines can be twocolored to prove
the theorem for n consider the set of regions formed by the n  1 lines
remaining when any one of the n lines is removed by the induction hypothesis this set of regions can be twocolored now put the nth line back
this splits the plane into two halfplanes each of which independently
has a valid twocoloring inherited from the twocoloring of the plane with
n  1 lines unfortunately the regions newly split by the nth line violate
the rule for a twocoloring take all regions on one side of the nth line and
reverse their coloring after doing so this halfplane is still twocolored
those regions split by the nth line are now properly twocolored because
the part of the region to one side of the line is now black and the region
to the other side is now white thus by mathematical induction the entire
plane is twocolored
2


$$@@$$PAGE: 64
sec 26 mathematical proof techniques

45

compare the proof of theorem 27 with that of theorem 25 for theorem 25
we took a collection of stamps of size n  1 which by the induction hypothesis
must have the desired property and from that built a collection of size n that
has the desired property we therefore proved the existence of some collection of
stamps of size n with the desired property
for theorem 27 we must prove that any collection of n lines has the desired
property thus our strategy is to take an arbitrary collection of n lines and reduce it so that we have a set of lines that must have the desired property because
it matches the induction hypothesis from there we merely need to show that reversing the original reduction process preserves the desired property
in contrast consider what is required if we attempt to build from a set of lines
of size n  1 to one of size n we would have great difficulty justifying that all
possible collections of n lines are covered by our building process by reducing
from an arbitrary collection of n lines to something less we avoid this problem
this sections final example shows how induction can be used to prove that a
recursive function produces the correct result
example 217 we would like to prove that function fact does indeed
compute the factorial function there are two distinct steps to such a proof
the first is to prove that the function always terminates the second is to
prove that the function returns the correct value
theorem 28 function fact will terminate for any value of n
proof for the base case we observe that fact will terminate directly
whenever n  0 the induction hypothesis is that fact will terminate for
n  1 for n we have two possibilities one possibility is that n  12
in that case fact will terminate directly because it will fail its assertion
test otherwise fact will make a recursive call to factn1 by the
induction hypothesis factn1 must terminate
2
theorem 29 function fact does compute the factorial function for any
value in the range 0 to 12
proof to prove the base case observe that when n  0 or n  1
factn returns the correct value of 1 the induction hypothesis is that
factn1 returns the correct value of n  1 for any value n within
the legal range factn returns n  factn1 by the induction hypothesis factn1  n  1 and because n  n  1  n we have
proved that factn produces the correct result
2
we can use a similar process to prove many recursive programs correct the
general form is to show that the base cases perform correctly and then to use the
induction hypothesis to show that the recursive step also produces the correct result


$$@@$$PAGE: 65
46

chap 2 mathematical preliminaries

prior to this we must prove that the function always terminates which might also
be done using an induction proof

27

estimation

one of the most useful life skills that you can gain from your computer science
training is the ability to perform quick estimates this is sometimes known as back
of the napkin or back of the envelope calculation both nicknames suggest
that only a rough estimate is produced estimation techniques are a standard part
of engineering curricula but are often neglected in computer science estimation
is no substitute for rigorous detailed analysis of a problem but it can serve to
indicate when a rigorous analysis is warranted if the initial estimate indicates that
the solution is unworkable then further analysis is probably unnecessary
estimation can be formalized by the following threestep process
1 determine the major parameters that affect the problem
2 derive an equation that relates the parameters to the problem
3 select values for the parameters and apply the equation to yield an estimated
solution
when doing estimations a good way to reassure yourself that the estimate is
reasonable is to do it in two different ways in general if you want to know what
comes out of a system you can either try to estimate that directly or you can
estimate what goes into the system assuming that what goes in must later come
out if both approaches independently give similar answers then this should
build confidence in the estimate
when calculating be sure that your units match for example do not add feet
and pounds verify that the result is in the correct units always keep in mind that
the output of a calculation is only as good as its input the more uncertain your
valuation for the input parameters in step 3 the more uncertain the output value
however back of the envelope calculations are often meant only to get an answer
within an order of magnitude or perhaps within a factor of two before doing an
estimate you should decide on acceptable error bounds such as within 25 within
a factor of two and so forth once you are confident that an estimate falls within
your error bounds leave it alone do not try to get a more precise estimate than
necessary for your purpose
example 218 how many library bookcases does it take to store books
containing one million pages i estimate that a 500page book requires
one inch on the library shelf it will help to look at the size of any handy
book yielding about 200 feet of shelf space for one million pages if a
shelf is 4 feet wide then 50 shelves are required if a bookcase contains


$$@@$$PAGE: 66
sec 28 further reading

47

5 shelves this yields about 10 library bookcases to reach this conclusion
i estimated the number of pages per inch the width of a shelf and the
number of shelves in a bookcase none of my estimates are likely to be
precise but i feel confident that my answer is correct to within a factor of
two after writing this i went to virginia techs library and looked at
some real bookcases they were only about 3 feet wide but typically had
7 shelves for a total of 21 shelffeet so i was correct to within 10 on
bookcase capacity far better than i expected or needed one of my selected
values was too high and the other too low which canceled out the errors

example 219 is it more economical to buy a car that gets 20 miles per
gallon or one that gets 30 miles per gallon but costs 3000 more the
typical car is driven about 12000 miles per year if gasoline costs 3gallon
then the yearly gas bill is 1800 for the less efficient car and 1200 for the
more efficient car if we ignore issues such as the payback that would be
received if we invested 3000 in a bank it would take 5 years to make
up the difference in price at this point the buyer must decide if price is
the only criterion and if a 5year payback time is acceptable naturally
a person who drives more will make up the difference more quickly and
changes in gasoline prices will also greatly affect the outcome

example 220 when at the supermarket doing the weeks shopping can
you estimate about how much you will have to pay at the checkout one
simple way is to round the price of each item to the nearest dollar and add
this value to a mental running total as you put the item in your shopping
cart this will likely give an answer within a couple of dollars of the true
total

28

further reading

most of the topics covered in this chapter are considered part of discrete mathematics an introduction to this field is discrete mathematics with applications
by susanna s epp epp10 an advanced treatment of many mathematical topics
useful to computer scientists is concrete mathematics a foundation for computer
science by graham knuth and patashnik gkp94
see technically speaking from the february 1995 issue of ieee spectrum
sel95 for a discussion on the standard for indicating units of computer storage
used in this book


$$@@$$PAGE: 67
48

chap 2 mathematical preliminaries

introduction to algorithms by udi manber man89 makes extensive use of
mathematical induction as a technique for developing algorithms
for more information on recursion see thinking recursively by eric s roberts
rob86 to learn recursion properly it is worth your while to learn the programming languages lisp or scheme even if you never intend to write a program in
either language in particular friedman and felleisens little books including
the little lisperff89 and the little schemerffbs95 are designed to teach
you how to think recursively as well as teach you the language these books are
entertaining reading as well
a good book on writing mathematical proofs is daniel solows how to read
and do proofs sol09 to improve your general mathematical problemsolving
abilities see the art and craft of problem solving by paul zeitz zei07 zeitz
also discusses the three proof techniques presented in section 26 and the roles of
investigation and argument in problem solving
for more about estimation techniques see two programming pearls by john
louis bentley entitled the back of the envelope and the envelope is back ben84
ben00 ben86 ben88 genius the life and science of richard feynman by
james gleick gle92 gives insight into how important back of the envelope calculation was to the developers of the atomic bomb and to modern theoretical physics
in general

29

exercises

21 for each relation below explain why the relation does or does not satisfy
each of the properties reflexive symmetric antisymmetric and transitive
a isbrotherof on the set of people
b isfatherof on the set of people
c the relation r  hx yi  x2  y 2  1 for real numbers x and y
d the relation r  hx yi  x2  y 2  for real numbers x and y
e the relation r  hx yi  x mod y  0 for x y  1 2 3 4
f the empty relation  ie the relation with no ordered pairs for which
it is true on the set of integers
g the empty relation  ie the relation with no ordered pairs for which
it is true on the empty set
22 for each of the following relations either prove that it is an equivalence
relation or prove that it is not an equivalence relation
a for integers a and b a  b if and only if a  b is even
b for integers a and b a  b if and only if a  b is odd
c for nonzero rational numbers a and b a  b if and only if a  b  0
d for nonzero rational numbers a and b a  b if and only if ab is an
integer


$$@@$$PAGE: 68
sec 29 exercises

23

24
25

26

27

28

29
210
211

49

e for rational numbers a and b a  b if and only if a  b is an integer
f for rational numbers a and b a  b if and only if a  b  2
state whether each of the following relations is a partial ordering and explain
why or why not
a isfatherof on the set of people
b isancestorof on the set of people
c isolderthan on the set of people
d issisterof on the set of people
e ha bi ha ai hb ai on the set a b
f h2 1i h1 3i h2 3i on the set 1 2 3
how many total orderings can be defined on a set with n elements explain
your answer
define an adt for a set of integers remember that a set has no concept of
duplicate elements and has no concept of order your adt should consist
of the functions that can be performed on a set to control its membership
check the size check if a given element is in the set and so on each function
should be defined in terms of its input and output
define an adt for a bag of integers remember that a bag may contain duplicates and has no concept of order your adt should consist of the functions that can be performed on a bag to control its membership check the
size check if a given element is in the set and so on each function should
be defined in terms of its input and output
define an adt for a sequence of integers remember that a sequence may
contain duplicates and supports the concept of position for its elements
your adt should consist of the functions that can be performed on a sequence to control its membership check the size check if a given element is
in the set and so on each function should be defined in terms of its input
and output
an investor places 30000 into a stock fund 10 years later the account has
a value of 69000 using logarithms and antilogarithms present a formula
for calculating the average annual rate of increase then use your formula to
determine the average annual growth rate for this fund
rewrite the factorial function of section 25 without using recursion
rewrite the for loop for the random permutation generator of section 22
as a recursive function
here is a simple recursive function to compute the fibonacci sequence
long fibrint n   recursive fibonacci generator
 fibr46 is largest value that fits in a long
assertn  0  n  47 input out of range
if n  1  n  2 return 1  base cases
return fibrn1  fibrn2
 recursion



$$@@$$PAGE: 69
50

chap 2 mathematical preliminaries

this algorithm turns out to be very slow calling fibr a total of fibn times
contrast this with the following iterative algorithm
long fibiint n   iterative fibonacci generator
 fibi46 is largest value that fits in a long
assertn  0  n  47 input out of range
long past prev curr  store temporary values
past  prev  curr  1
 initialize
for int i3 in i   compute next value
past  prev
 past holds fibii2
prev  curr
 prev holds fibii1
curr  past  prev
 curr now holds fibii

return curr


function fibi executes the for loop n  2 times
a which version is easier to understand why
b explain why fibr is so much slower than fibi
212 write a recursive function to solve a generalization of the towers of hanoi
problem where each ring may begin on any pole so long as no ring sits on
top of a smaller ring
213 revise the recursive implementation for towers of hanoi from section 25
to return the list of moves needed to solve the problem
214 consider the following function
void foo double val 
if val  00
fooval20


this function makes progress towards the base case on every recursive call
in theory that is if double variables acted like true real numbers would
this function ever terminate for input val a nonzero number in practice an
actual computer implementation will it terminate
215 write a function to print all of the permutations for the elements of an array
containing n distinct integer values
216 write a recursive algorithm to print all of the subsets for the set of the first n
positive integers
217 the largest common factor lcf for two positive integers n and m is
the largest integer that divides both n and m evenly lcfn m is at least
one and at most m assuming that n  m over two thousand years ago
euclid provided an efficient algorithm based on the observation that when
n mod m 6 0 lcfn m  lcfm n mod m use this fact to write two
algorithms to find the lcf for two positive integers the first version should
compute the value iteratively the second version should compute the value
using recursion


$$@@$$PAGE: 70
51

sec 29 exercises

218 prove by contradiction that the number of primes is infinite
219 a use induction to show that n2  n is always even
b give a direct proof in one or two sentences that n2  n is always even
c show that n3  n is always divisible by three
d is n5  n aways divisible by 5 explain your answer

220 prove that 2 is irrational
221 explain why
n
n
n1
x
x
x
i
n  i  1 
n  i
i1

222
223
224
225

i1

i0

prove equation 22 using mathematical induction
prove equation 26 using mathematical induction
prove equation 27 using mathematical induction
find a closedform solution and prove using induction that your solution is
correct for the summation
n
x
3i 
i1

226 prove that the sum of the first n even numbers is n2  n
a by assuming that the sum of the first n odd numbers is n2 
b by mathematical induction
p
227 give a closedform formula for the summation nia i where a is an integer
between 1 and n
228 prove that fibn   53 n 
229 prove for n  1 that
n
x
n2 n  12
i3 

4
i1

230 the following theorem is called the pigeonhole principle
theorem 210 when n  1 pigeons roost in n holes there must be some
hole containing at least two pigeons
a prove the pigeonhole principle using proof by contradiction
b prove the pigeonhole principle using mathematical induction
231 for this problem you will consider arrangements of infinite lines in the plane
such that three or more lines never intersect at a single point and no two lines
are parallel
a give a recurrence relation that expresses the number of regions formed
by n lines and explain why your recurrence is correct
b give the summation that results from expanding your recurrence


$$@@$$PAGE: 71
52

chap 2 mathematical preliminaries

c give a closedform solution for the summation
232 prove using induction that the recurrence tn  tn  1  n t1  1
has as its closedform solution tn  nn  12
233 expand the following recurrence to help you find a closedform solution and
then use induction to prove your answer is correct
tn  2tn  1  1 for n  0 t0  0
234 expand the following recurrence to help you find a closedform solution and
then use induction to prove your answer is correct
tn  tn  1  3n  1 for n  0 t0  1
235 assume that an nbit integer represented by standard binary notation takes
any value in the range 0 to 2n  1 with equal probability
a for each bit position what is the probability of its value being 1 and
what is the probability of its value being 0
b what is the average number of 1 bits for an nbit random number
c what is the expected value for the position of the leftmost 1 bit in
other words how many positions on average must we examine when
moving from left to right before encountering a 1 bit show the
appropriate summation
236 what is the total volume of your body in liters or if you prefer gallons
237 an art historian has a database of 20000 fullscreen color images
a about how much space will this require how many cds would be
required to store the database a cd holds about 600mb of data be
sure to explain all assumptions you made to derive your answer
b now assume that you have access to a good image compression technique that can store the images in only 110 of the space required for
an uncompressed image will the entire database fit onto a single cd
if the images are compressed
238 how many cubic miles of water flow out of the mouth of the mississippi
river each day do not look up the answer or any supplemental facts be
sure to describe all assumptions made in arriving at your answer
239 when buying a home mortgage you often have the option of paying some
money in advance called discount points to get a lower interest rate assume that you have the choice between two 15year fixedrate mortgages
one at 8 with no upfront charge and the other at 7 34  with an upfront
charge of 1 of the mortgage value how long would it take to recover the
1 charge when you take the mortgage at the lower rate as a second more


$$@@$$PAGE: 72
sec 29 exercises

240

241

242

243
244

245
246
247

53

precise estimate how long would it take to recover the charge plus the interest you would have received if you had invested the equivalent of the 1
charge in the bank at 5 interest while paying the higher rate do not use
a calculator to help you answer this question
when you build a new house you sometimes get a construction loan which
is a temporary line of credit out of which you pay construction costs as they
occur at the end of the construction period you then replace the construction loan with a regular mortgage on the house during the construction loan
you only pay each month for the interest charged against the actual amount
borrowed so far assume that your house construction project starts at the
beginning of april and is complete at the end of six months assume that
the total construction cost will be 300000 with the costs occurring at the beginning of each month in 50000 increments the construction loan charges
6 interest estimate the total interest payments that must be paid over the
life of the construction loan
here are some questions that test your working knowledge of how fast computers operate is disk drive access time normally measured in milliseconds
thousandths of a second or microseconds millionths of a second does
your ram memory access a word in more or less than one microsecond
how many instructions can your cpu execute in one year if the machine is
left running at full speed all the time do not use paper or a calculator to
derive your answers
does your home contain enough books to total one million pages how
many total pages are stored in your school library building explain how
you got your answer
how many words are in this book explain how you got your answer
how many hours are one million seconds how many days answer these
questions doing all arithmetic in your head explain how you got your answer
how many cities and towns are there in the united states explain how you
got your answer
how many steps would it take to walk from boston to san francisco explain how you got your answer
a man begins a car trip to visit his inlaws the total distance is 60 miles
and he starts off at a speed of 60 miles per hour after driving exactly 1 mile
he loses some of his enthusiasm for the journey and instantaneously slows
down to 59 miles per hour after traveling another mile he again slows to
58 miles per hour this continues progressively slowing by 1 mile per hour
for each mile traveled until the trip is complete
a how long does it take the man to reach his inlaws


$$@@$$PAGE: 73
54

chap 2 mathematical preliminaries

b how long would the trip take in the continuous case where the speed
smoothly diminishes with the distance yet to travel


$$@@$$PAGE: 74
3
algorithm analysis

how long will it take to process the company payroll once we complete our planned
merger should i buy a new payroll program from vendor x or vendor y if a
particular program is slow is it badly implemented or is it solving a hard problem
questions like these ask us to consider the difficulty of a problem or the relative
efficiency of two or more approaches to solving a problem
this chapter introduces the motivation basic notation and fundamental techniques of algorithm analysis we focus on a methodology known as asymptotic
algorithm analysis or simply asymptotic analysis asymptotic analysis attempts
to estimate the resource consumption of an algorithm it allows us to compare the
relative costs of two or more algorithms for solving the same problem asymptotic
analysis also gives algorithm designers a tool for estimating whether a proposed
solution is likely to meet the resource constraints for a problem before they implement an actual program after reading this chapter you should understand
 the concept of a growth rate the rate at which the cost of an algorithm grows
as the size of its input grows
 the concept of upper and lower bounds for a growth rate and how to estimate
these bounds for a simple program algorithm or problem and
 the difference between the cost of an algorithm or program and the cost of
a problem
the chapter concludes with a brief discussion of the practical difficulties encountered when empirically measuring the cost of a program and some principles for
code tuning to improve program efficiency

31

introduction

how do you compare two algorithms for solving some problem in terms of efficiency we could implement both algorithms as computer programs and then run
55


$$@@$$PAGE: 75
56

chap 3 algorithm analysis

them on a suitable range of inputs measuring how much of the resources in question each program uses this approach is often unsatisfactory for four reasons
first there is the effort involved in programming and testing two algorithms when
at best you want to keep only one second when empirically comparing two algorithms there is always the chance that one of the programs was better written
than the other and therefor the relative qualities of the underlying algorithms are
not truly represented by their implementations this can easily occur when the
programmer has a bias regarding the algorithms third the choice of empirical
test cases might unfairly favor one algorithm fourth you could find that even the
better of the two algorithms does not fall within your resource budget in that case
you must begin the entire process again with yet another program implementing a
new algorithm but how would you know if any algorithm can meet the resource
budget perhaps the problem is simply too difficult for any implementation to be
within budget
these problems can often be avoided by using asymptotic analysis asymptotic analysis measures the efficiency of an algorithm or its implementation as a
program as the input size becomes large it is actually an estimating technique and
does not tell us anything about the relative merits of two programs where one is
always slightly faster than the other however asymptotic analysis has proved
useful to computer scientists who must determine if a particular algorithm is worth
considering for implementation
the critical resource for a program is most often its running time however
you cannot pay attention to running time alone you must also be concerned with
other factors such as the space required to run the program both main memory and
disk space typically you will analyze the time required for an algorithm or the
instantiation of an algorithm in the form of a program and the space required for
a data structure
many factors affect the running time of a program some relate to the environment in which the program is compiled and run such factors include the speed of
the computers cpu bus and peripheral hardware competition with other users
for the computers or the networks resources can make a program slow to a crawl
the programming language and the quality of code generated by a particular compiler can have a significant effect the coding efficiency of the programmer who
converts the algorithm to a program can have a tremendous impact as well
if you need to get a program working within time and space constraints on a
particular computer all of these factors can be relevant yet none of these factors
address the differences between two algorithms or data structures to be fair programs derived from two algorithms for solving the same problem should both be
compiled with the same compiler and run on the same computer under the same
conditions as much as possible the same amount of care should be taken in the
programming effort devoted to each program to make the implementations equally


$$@@$$PAGE: 76
sec 31 introduction

57

efficient in this sense all of the factors mentioned above should cancel out of the
comparison because they apply to both algorithms equally
if you truly wish to understand the running time of an algorithm there are other
factors that are more appropriate to consider than machine speed programming
language compiler and so forth ideally we would measure the running time of
the algorithm under standard benchmark conditions however we have no way
to calculate the running time reliably other than to run an implementation of the
algorithm on some computer the only alternative is to use some other measure as
a surrogate for running time
of primary consideration when estimating an algorithms performance is the
number of basic operations required by the algorithm to process an input of a
certain size the terms basic operations and size are both rather vague and
depend on the algorithm being analyzed size is often the number of inputs processed for example when comparing sorting algorithms the size of the problem
is typically measured by the number of records to be sorted a basic operation
must have the property that its time to complete does not depend on the particular
values of its operands adding or comparing two integer variables are examples
of basic operations in most programming languages summing the contents of an
array containing n integers is not because the cost depends on the value of n ie
the size of the input
example 31 consider a simple algorithm to solve the problem of finding
the largest value in an array of n integers the algorithm looks at each
integer in turn saving the position of the largest value seen so far this
algorithm is called the largestvalue sequential search and is illustrated by
the following function
 return position of largest value in a of size n
int largestint a int n 
int currlarge  0  holds largest element position
for int i1 in i
 for each array element
if acurrlarge  ai  if ai is larger
currlarge  i

remember its position
return currlarge
 return largest position


here the size of the problem is alength the number of integers stored
in array a the basic operation is to compare an integers value to that of
the largest value seen so far it is reasonable to assume that it takes a fixed
amount of time to do one such comparison regardless of the value of the
two integers or their positions in the array
because the most important factor affecting running time is normally
size of the input for a given input size n we often express the time t to run


$$@@$$PAGE: 77
58

chap 3 algorithm analysis

the algorithm as a function of n written as tn we will always assume
tn is a nonnegative value
let us call c the amount of time required to compare two integers in
function largest we do not care right now what the precise value of c
might be nor are we concerned with the time required to increment variable i because this must be done for each value in the array or the time
for the actual assignment when a larger value is found or the little bit of
extra time taken to initialize currlarge we just want a reasonable approximation for the time taken to execute the algorithm the total time
to run largest is therefore approximately cn because we must make n
comparisons with each comparison costing c time we say that function
largest and by extension the largestvalue sequential search algorithm
for any typical implementation has a running time expressed by the equation
tn  cn
this equation describes the growth rate for the running time of the largestvalue sequential search algorithm

example 32 the running time of a statement that assigns the first value
of an integer array to a variable is simply the time required to copy the value
of the first array value we can assume this assignment takes a constant
amount of time regardless of the value let us call c1 the amount of time
necessary to copy an integer no matter how large the array on a typical
computer given reasonable conditions for memory and array size the time
to copy the value from the first position of the array is always c1  thus the
equation for this algorithm is simply
tn  c1 
indicating that the size of the input n has no effect on the running time
this is called a constant running time

example 33 consider the following code
sum  0
for i1 in i
for j1 jn j
sum

what is the running time for this code fragment clearly it takes longer
to run when n is larger the basic operation in this example is the increment


$$@@$$PAGE: 78
59

sec 31 introduction

n

1400

2n

2n2

5n log n

1200
20n

1000
800
600

10n

400
200
0

0

10

20

n

30

40

50

2n2

2n

400
20n

300

5n log n
200
10n

100

0

0

5

10

15

input size n
figure 31 two views of a graph illustrating the growth rates for six equations
the bottom view shows in detail the lowerleft portion of the top view the horizontal axis represents input size the vertical axis can represent time space or
any other measure of cost

operation for variable sum we can assume that incrementing takes constant
time call this time c2  we can ignore the time required to initialize sum
and to increment the loop counters i and j in practice these costs can
safely be bundled into time c2  the total number of increment operations
is n2  thus we say that the running time is tn  c2 n2 


$$@@$$PAGE: 79
60

chap 3 algorithm analysis

n
16
256
1024
64k
1m
1g

log log n log n
2
3
 33
4
 43
 49

4
8
10
16
20
30

n

n log n

n2

n3

2n

24
28
210
216
220
230

4  24  26
8  28  211
10  210  213
16  216  220
20  220  224
30  230  235

28
216
220
232
240
260

212 216
224 2256
230 21024
248 264k
260 21m
290 21g

figure 32 costs for growth rates representative of most computer algorithms

the growth rate for an algorithm is the rate at which the cost of the algorithm
grows as the size of its input grows figure 31 shows a graph for six equations each
meant to describe the running time for a particular program or algorithm a variety
of growth rates representative of typical algorithms are shown the two equations
labeled 10n and 20n are graphed by straight lines a growth rate of cn for c any
positive constant is often referred to as a linear growth rate or running time this
means that as the value of n grows the running time of the algorithm grows in the
same proportion doubling the value of n roughly doubles the running time an
algorithm whose runningtime equation has a highestorder term containing a factor
of n2 is said to have a quadratic growth rate in figure 31 the line labeled 2n2
represents a quadratic growth rate the line labeled 2n represents an exponential
growth rate this name comes from the fact that n appears in the exponent the
line labeled n is also growing exponentially
as you can see from figure 31 the difference between an algorithm whose
running time has cost tn  10n and another with cost tn  2n2 becomes
tremendous as n grows for n  5 the algorithm with running time tn  2n2 is
already much slower this is despite the fact that 10n has a greater constant factor
than 2n2  comparing the two curves marked 20n and 2n2 shows that changing the
constant factor for one of the equations only shifts the point at which the two curves
cross for n  10 the algorithm with cost tn  2n2 is slower than the algorithm
with cost tn  20n this graph also shows that the equation tn  5n log n
grows somewhat more quickly than both tn  10n and tn  20n but not
nearly so quickly as the equation tn  2n2  for constants a b  1 na grows
faster than either logb n or log nb  finally algorithms with cost tn  2n or
tn  n are prohibitively expensive for even modest values of n note that for
constants a b  1 an grows faster than nb 
we can get some further insight into relative growth rates for various algorithms
from figure 32 most of the growth rates that appear in typical algorithms are
shown along with some representative input sizes once again we see that the
growth rate has a tremendous effect on the resources consumed by an algorithm


$$@@$$PAGE: 80
sec 32 best worst and average cases

32

61

best worst and average cases

consider the problem of finding the factorial of n for this problem there is only
one input of a given size that is there is only a single instance for each size of
n now consider our largestvalue sequential search algorithm of example 31
which always examines every array value this algorithm works on many inputs of
a given size n that is there are many possible arrays of any given size however
no matter what array of size n that the algorithm looks at its cost will always be
the same in that it always looks at every element in the array one time
for some algorithms different inputs of a given size require different amounts
of time for example consider the problem of searching an array containing n
integers to find the one with a particular value k assume that k appears exactly
once in the array the sequential search algorithm begins at the first position in
the array and looks at each value in turn until k is found once k is found the
algorithm stops this is different from the largestvalue sequential search algorithm
of example 31 which always examines every array value
there is a wide range of possible running times for the sequential search algorithm the first integer in the array could have value k and so only one integer
is examined in this case the running time is short this is the best case for this
algorithm because it is not possible for sequential search to look at less than one
value alternatively if the last position in the array contains k then the running
time is relatively long because the algorithm must examine n values this is the
worst case for this algorithm because sequential search never looks at more than
n values if we implement sequential search as a program and run it many times
on many different arrays of size n or search for many different values of k within
the same array we expect the algorithm on average to go halfway through the array
before finding the value we seek on average the algorithm examines about n2
values we call this the average case for this algorithm
when analyzing an algorithm should we study the best worst or average case
normally we are not interested in the best case because this might happen only
rarely and generally is too optimistic for a fair characterization of the algorithms
running time in other words analysis based on the best case is not likely to be
representative of the behavior of the algorithm however there are rare instances
where a bestcase analysis is useful  in particular when the best case has high
probability of occurring in chapter 7 you will see some examples where taking
advantage of the bestcase running time for one sorting algorithm makes a second
more efficient
how about the worst case the advantage to analyzing the worst case is that
you know for certain that the algorithm must perform at least that well this is especially important for realtime applications such as for the computers that monitor
an air traffic control system here it would not be acceptable to use an algorithm


$$@@$$PAGE: 81
62

chap 3 algorithm analysis

that can handle n airplanes quickly enough most of the time but which fails to
perform quickly enough when all n airplanes are coming from the same direction
for other applications  particularly when we wish to aggregate the cost of
running the program many times on many different inputs  worstcase analysis might not be a representative measure of the algorithms performance often
we prefer to know the averagecase running time this means that we would like
to know the typical behavior of the algorithm on inputs of size n unfortunately
averagecase analysis is not always possible averagecase analysis first requires
that we understand how the actual inputs to the program and their costs are distributed with respect to the set of all possible inputs to the program for example it
was stated previously that the sequential search algorithm on average examines half
of the array values this is only true if the element with value k is equally likely
to appear in any position in the array if this assumption is not correct then the
algorithm does not necessarily examine half of the array values in the average case
see section 92 for further discussion regarding the effects of data distribution on
the sequential search algorithm
the characteristics of a data distribution have a significant effect on many
search algorithms such as those based on hashing section 94 and search trees
eg see section 54 incorrect assumptions about data distribution can have disastrous consequences on a programs space or time performance unusual data
distributions can also be used to advantage as shown in section 92
in summary for realtime applications we are likely to prefer a worstcase analysis of an algorithm otherwise we often desire an averagecase analysis if we
know enough about the distribution of our input to compute the average case if
not then we must resort to worstcase analysis

33

a faster computer or a faster algorithm

imagine that you have a problem to solve and you know of an algorithm whose
running time is proportional to n2  unfortunately the resulting program takes ten
times too long to run if you replace your current computer with a new one that
is ten times faster will the n2 algorithm become acceptable if the problem size
remains the same then perhaps the faster computer will allow you to get your work
done quickly enough even with an algorithm having a high growth rate but a funny
thing happens to most people who get a faster computer they dont run the same
problem faster they run a bigger problem say that on your old computer you
were content to sort 10000 records because that could be done by the computer
during your lunch break on your new computer you might hope to sort 100000
records in the same time you wont be back from lunch any sooner so you are
better off solving a larger problem and because the new machine is ten times
faster you would like to sort ten times as many records


$$@@$$PAGE: 82
sec 33 a faster computer or a faster algorithm

63

fn
n
n0
change
n0 n
0
10n
1000 10 000 n  10n
10
0
20n
500
5000 n
10
  10n 0
5n log n 250
1842
10n n  10n 737
70
223 n0  10n
316
2n2
n
2
13
16 n0  n  3

figure 33 the increase in problem size that can be run in a fixed period of time
on a computer that is ten times faster the first column lists the righthand sides
for each of five growth rate equations from figure 31 for the purpose of this
example arbitrarily assume that the old machine can run 10000 basic operations
in one hour the second column shows the maximum value for n that can be run
in 10000 basic operations on the old machine the third column shows the value
for n0  the new maximum size for the problem that can be run in the same time
on the new machine that is ten times faster variable n0 is the greatest size for the
problem that can run in 100000 basic operations the fourth column shows how
the size of n changed to become n0 on the new machine the fifth column shows
the increase in the problem size as the ratio of n0 to n

if your algorithms growth rate is linear ie if the equation that describes the
running time on input size n is tn  cn for some constant c then 100000
records on the new machine will be sorted in the same time as 10000 records on
the old machine if the algorithms growth rate is greater than cn such as c1 n2 
then you will not be able to do a problem ten times the size in the same amount of
time on a machine that is ten times faster
how much larger a problem can be solved in a given amount of time by a faster
computer assume that the new machine is ten times faster than the old say that
the old machine could solve a problem of size n in an hour what is the largest
problem that the new machine can solve in one hour figure 33 shows how large
a problem can be solved on the two machines for five of the runningtime functions
from figure 31
this table illustrates many important points the first two equations are both
linear only the value of the constant factor has changed in both cases the machine
that is ten times faster gives an increase in problem size by a factor of ten in other
words while the value of the constant does affect the absolute size of the problem
that can be solved in a fixed amount of time it does not affect the improvement in
problem size as a proportion to the original size gained by a faster computer this
relationship holds true regardless of the algorithms growth rate constant factors
never affect the relative improvement gained by a faster computer
an algorithm with time equation tn  2n2 does not receive nearly as great
an improvement from the faster machine as an algorithm with linear growth rate
instead of an improvement by a factor of ten the improvement is only the square


$$@@$$PAGE: 83
64

chap 3 algorithm analysis


root of that 10  316 thus the algorithm with higher growth rate not only
solves a smaller problem in a given time in the first place it also receives less of
a speedup from a faster computer as computers get ever faster the disparity in
problem sizes becomes ever greater
the algorithm with growth rate tn  5n log n improves by a greater amount
than the one with quadratic growth rate but not by as great an amount as the algorithms with linear growth rates
note that something special happens in the case of the algorithm whose running
time grows exponentially in figure 31 the curve for the algorithm whose time is
proportional to 2n goes up very quickly in figure 33 the increase in problem
size on the machine ten times as fast is shown to be about n  3 to be precise
it is n  log2 10 the increase in problem size for an algorithm with exponential
growth rate is by a constant addition not by a multiplicative factor because the
old value of n was 13 the new problem size is 16 if next year you buy another
computer ten times faster yet then the new computer 100 times faster than the
original computer will only run a problem of size 19 if you had a second program
whose growth rate is 2n and for which the original computer could run a problem
of size 1000 in an hour than a machine ten times faster can run a problem only of
size 1003 in an hour thus an exponential growth rate is radically different than
the other growth rates shown in figure 33 the significance of this difference is
explored in chapter 17
instead of buying a faster computer consider what happens if you replace an
algorithm whose running time is proportional to n2 with a new algorithm whose
running time is proportional to n log n in the graph of figure 31 a fixed amount of
time would appear as a horizontal line if the line for the amount of time available
to solve your problem is above the point at which the curves for the two growth
rates in question meet then the algorithm whose running time grows less quickly
is faster an algorithm with running time tn  n2 requires 1024  1024 
1 048 576 time steps for an input of size n  1024 an algorithm with running
time tn  n log n requires 1024  10  10 240 time steps for an input of
size n  1024 which is an improvement of much more than a factor of ten when
compared to the algorithm with running time tn  n2  because n2  10n log n
whenever n  58 if the typical problem size is larger than 58 for this example then
you would be much better off changing algorithms instead of buying a computer
ten times faster furthermore when you do buy a faster computer an algorithm
with a slower growth rate provides a greater benefit in terms of larger problem size
that can run in a certain time on the new computer


$$@@$$PAGE: 84
sec 34 asymptotic analysis

34

65

asymptotic analysis

despite the larger constant for the curve labeled 10n in figure 31 2n2 crosses
it at the relatively small value of n  5 what if we double the value of the
constant in front of the linear equation as shown in the graph 20n is surpassed
by 2n2 once n  10 the additional factor of two for the linear growth rate does
not much matter it only doubles the xcoordinate for the intersection point in
general changes to a constant factor in either equation only shift where the two
curves cross not whether the two curves cross
when you buy a faster computer or a faster compiler the new problem size
that can be run in a given amount of time for a given growth rate is larger by the
same factor regardless of the constant on the runningtime equation the time
curves for two algorithms with different growth rates still cross regardless of their
runningtime equation constants for these reasons we usually ignore the constants when we want an estimate of the growth rate for the running time or other
resource requirements of an algorithm this simplifies the analysis and keeps us
thinking about the most important aspect the growth rate this is called asymptotic algorithm analysis to be precise asymptotic analysis refers to the study of
an algorithm as the input size gets big or reaches a limit in the calculus sense
however it has proved to be so useful to ignore all constant factors that asymptotic
analysis is used for most algorithm comparisons
it is not always reasonable to ignore the constants when comparing algorithms
meant to run on small values of n the constant can have a large effect for example if the problem is to sort a collection of exactly five records then an algorithm
designed for sorting thousands of records is probably not appropriate even if its
asymptotic analysis indicates good performance there are rare cases where the
constants for two algorithms under comparison can differ by a factor of 1000 or
more making the one with lower growth rate impractical for most purposes due to
its large constant asymptotic analysis is a form of back of the envelope estimation for algorithm resource consumption it provides a simplified model of the
running time or other resource needs of an algorithm this simplification usually
helps you understand the behavior of your algorithms just be aware of the limitations to asymptotic analysis in the rare situation where the constant is important

341

upper bounds

several terms are used to describe the runningtime equation for an algorithm
these terms  and their associated symbols  indicate precisely what aspect of
the algorithms behavior is being described one is the upper bound for the growth
of the algorithms running time it indicates the upper or highest growth rate that
the algorithm can have


$$@@$$PAGE: 85
66

chap 3 algorithm analysis

because the phrase has an upper bound to its growth rate of f n is long and
often used when discussing algorithms we adopt a special notation called bigoh
notation if the upper bound for an algorithms growth rate for say the worst
case is f n then we would write that this algorithm is in the set of nin the
worst case or just in of nin the worst case for example if n2 grows as
fast as tn the running time of our algorithm for the worstcase input we would
say the algorithm is in on2  in the worst case
the following is a precise definition for an upper bound tn represents the
true running time of the algorithm f n is some expression for the upper bound
for tn a nonnegatively valued function tn is in set of n
if there exist two positive constants c and n0 such that tn  cf n
for all n  n0 
constant n0 is the smallest value of n for which the claim of an upper bound holds
true usually n0 is small such as 1 but does not need to be you must also be
able to pick some constant c but it is irrelevant what the value for c actually is
in other words the definition says that for all inputs of the type in question such
as the worst case for all inputs of size n that are large enough ie n  n0  the
algorithm always executes in less than cf n steps for some constant c
example 34 consider the sequential search algorithm for finding a specified value in an array of integers if visiting and examining one value in
the array requires cs steps where cs is a positive number and if the value
we search for has equal probability of appearing in any position in the array then in the average case tn  cs n2 for all values of n  1
cs n2  cs n therefore by the definition tn is in on for n0  1 and
c  cs 

example 35 for a particular algorithm tn  c1 n2  c2 n in the average case where c1 and c2 are positive numbers then c1 n2  c2 n 
c1 n2  c2 n2  c1  c2 n2 for all n  1 so tn  cn2 for c  c1  c2 
and n0  1 therefore tn is in on2  by the second definition

example 36 assigning the value from the first position of an array to
a variable takes constant time regardless of the size of the array thus
tn  c for the best worst and average cases we could say in this
case that tn is in oc however it is traditional to say that an algorithm
whose running time has a constant upper bound is in o1


$$@@$$PAGE: 86
sec 34 asymptotic analysis

67

if someone asked you out of the blue who is the best your natural reaction
should be to reply best at what in the same way if you are asked what is
the growth rate of this algorithm you would need to ask when best case
average case or worst case some algorithms have the same behavior no matter
which input instance they receive an example is finding the maximum in an array
of integers but for many algorithms it makes a big difference such as when
searching an unsorted array for a particular value so any statement about the
upper bound of an algorithm must be in the context of some class of inputs of size
n we measure this upper bound nearly always on the bestcase averagecase or
worstcase inputs thus we cannot say this algorithm has an upper bound to
its growth rate of n2  we must say something like this algorithm has an upper
bound to its growth rate of n2 in the average case
knowing that something is in of n says only how bad things can be perhaps things are not nearly so bad because sequential search is in on in the worst
case it is also true to say that sequential search is in on2  but sequential search
is practical for large n in a way that is not true for some other algorithms in on2 
we always seek to define the running time of an algorithm with the tightest lowest possible upper bound thus we prefer to say that sequential search is in on
this also explains why the phrase is in of n or the notation  of n is
used instead of is of n or  of n there is no strict equality to the use
of bigoh notation on is in on2  but on2  is not in on
342

lower bounds

bigoh notation describes an upper bound in other words bigoh notation states
a claim about the greatest amount of some resource usually time that is required
by an algorithm for some class of inputs of size n typically the worst such input
the average of all possible inputs or the best such input
similar notation is used to describe the least amount of a resource that an algorithm needs for some class of input like bigoh notation this is a measure of the
algorithms growth rate like bigoh notation it works for any resource but we
most often measure the least amount of time required and again like bigoh notation we are measuring the resource required for some particular class of inputs
the worst average or bestcase input of size n
the lower bound for an algorithm or a problem as explained later is denoted
by the symbol  pronounced bigomega or just omega the following definition for  is symmetric with the definition of bigoh
for tn a nonnegatively valued function tn is in set gn
if there exist two positive constants c and n0 such that tn  cgn
for all n  n0 1
1

an alternate nonequivalent definition for  is


$$@@$$PAGE: 87
68

chap 3 algorithm analysis

example 37 assume tn  c1 n2  c2 n for c1 and c2  0 then
c1 n2  c2 n  c1 n2
for all n  1 so tn  cn2 for c  c1 and n0  1 therefore tn is
in n2  by the definition
it is also true that the equation of example 37 is in n however as with
bigoh notation we wish to get the tightest for  notation the largest bound
possible thus we prefer to say that this running time is in n2 
recall the sequential search algorithm to find a value k within an array of
integers in the average and worst cases this algorithm is in n because in both
the average and worst cases we must examine at least cn values where c is 12 in
the average case and 1 in the worst case
343

 notation

the definitions for bigoh and  give us ways to describe the upper bound for an
algorithm if we can find an equation for the maximum cost of a particular class of
inputs of size n and the lower bound for an algorithm if we can find an equation
for the minimum cost for a particular class of inputs of size n when the upper
and lower bounds are the same within a constant factor we indicate this by using
 bigtheta notation an algorithm is said to be hn if it is in ohn and
tn is in the set gn if there exists a positive constant c such that tn 
cgn for an infinite number of values for n
this definition says that for an interesting number of cases the algorithm takes at least cgn
time note that this definition is not symmetric with the definition of bigoh for gn to be a lower
bound this definition does not require that tn  cgn for all values of n greater than some
constant it only requires that this happen often enough in particular that it happen for an infinite
number of values for n motivation for this alternate definition can be found in the following example
assume a particular algorithm has the following behavior

n
for all odd n  1
tn 
n2 100 for all even n  0
1
n2 for all even n  0 so tn  cn2 for an infinite number
from this definition n2 100  100
of values of n ie for all even n for c  1100 therefore tn is in n2  by the definition
for this equation for tn it is true that all inputs of size n take at least cn time but an infinite
number of inputs of size n take cn2 time so we would like to say that the algorithm is in n2 
unfortunately using our first definition will yield a lower bound of n because it is not possible to
pick constants c and n0 such that tn  cn2 for all n  n0  the alternative definition does result
in a lower bound of n2  for this algorithm which seems to fit common sense more closely fortunately few real algorithms or computer programs display the pathological behavior of this example
our first definition for  generally yields the expected result
as you can see from this discussion asymptotic bounds notation is not a law of nature it is merely
a powerful modeling tool used to describe the behavior of algorithms


$$@@$$PAGE: 88
sec 34 asymptotic analysis

69

it is in hn note that we drop the word in for  notation because there
is a strict equality for two equations with the same  in other words if f n is
gn then gn is f n
because the sequential search algorithm is both in on and in n in the
average case we say it is n in the average case
given an algebraic equation describing the time requirement for an algorithm
the upper and lower bounds always meet that is because in some sense we have
a perfect analysis for the algorithm embodied by the runningtime equation for
many algorithms or their instantiations as programs it is easy to come up with
the equation that defines their runtime behavior most algorithms presented in this
book are well understood and we can almost always give a  analysis for them
however chapter 17 discusses a whole class of algorithms for which we have no
 analysis just some unsatisfying bigoh and  analyses exercise 314 presents
a short simple program fragment for which nobody currently knows the true upper
or lower bounds
while some textbooks and programmers will casually say that an algorithm is
order of or bigoh of some cost function it is generally better to use  notation
rather than bigoh notation whenever we have sufficient knowledge about an algorithm to be sure that the upper and lower bounds indeed match throughout this
book  notation will be used in preference to bigoh notation whenever our state
of knowledge makes that possible limitations on our ability to analyze certain
algorithms may require use of bigoh or  notations in rare occasions when the
discussion is explicitly about the upper or lower bound of a problem or algorithm
the corresponding notation will be used in preference to  notation
344

simplifying rules

once you determine the runningtime equation for an algorithm it really is a simple
matter to derive the bigoh  and  expressions from the equation you do not
need to resort to the formal definitions of asymptotic analysis instead you can use
the following rules to determine the simplest form
1 if f n is in ogn and gn is in ohn then f n is in ohn
2 if f n is in okgn for any constant k  0 then f n is in ogn
3 if f1 n is in og1 n and f2 n is in og2 n then f1 n  f2 n is in
omaxg1 n g2 n
4 if f1 n is in og1 n and f2 n is in og2 n then f1 nf2 n is in
og1 ng2 n
the first rule says that if some function gn is an upper bound for your cost
function then any upper bound for gn is also an upper bound for your cost function a similar property holds true for  notation if gn is a lower bound for your


$$@@$$PAGE: 89
70

chap 3 algorithm analysis

cost function then any lower bound for gn is also a lower bound for your cost
function likewise for  notation
the significance of rule 2 is that you can ignore any multiplicative constants
in your equations when using bigoh notation this rule also holds true for  and
 notations
rule 3 says that given two parts of a program run in sequence whether two
statements or two sections of code you need consider only the more expensive
part this rule applies to  and  notations as well for both you need consider
only the more expensive part
rule 4 is used to analyze simple loops in programs if some action is repeated
some number of times and each repetition has the same cost then the total cost is
the cost of the action multiplied by the number of times that the action takes place
this rule applies to  and  notations as well
taking the first three rules collectively you can ignore all constants and all
lowerorder terms to determine the asymptotic growth rate for any cost function
the advantages and dangers of ignoring constants were discussed near the beginning of this section ignoring lowerorder terms is reasonable when performing an
asymptotic analysis the higherorder terms soon swamp the lowerorder terms in
their contribution to the total cost as n becomes larger thus if tn  3n4  5n2 
then tn is in on4  the n2 term contributes relatively little to the total cost for
large n
throughout the rest of this book these simplifying rules are used when discussing the cost for a program or algorithm
345

classifying functions

given functions f n and gn whose growth rates are expressed as algebraic equations we might like to determine if one grows faster than the other the best way
to do this is to take the limit of the two functions as n grows towards infinity
f n

n gn
lim

if the limit goes to  then f n is in gn because f n grows faster if the
limit goes to zero then f n is in ogn because gn grows faster if the limit
goes to some constant other than zero then f n  gn because both grow at
the same rate
example 38 if f n  2n log n and gn  n2  is f n in ogn
gn or gn because
n2
n


2n log n
2 log n


$$@@$$PAGE: 90
sec 35 calculating the running time for a program

we easily see that
n2

n 2n log n
lim

because n grows faster than 2 log n thus n2 is in 2n log n

35

calculating the running time for a program

this section presents the analysis for several simple code fragments
example 39 we begin with an analysis of a simple assignment to an
integer variable
a  b

because the assignment statement takes constant time it is 1

example 310 consider a simple for loop
sum  0
for i1 in i
sum  n

the first line is 1 the for loop is repeated n times the third
line takes constant time so by simplifying rule 4 of section 344 the
total cost for executing the two lines making up the for loop is n by
rule 3 the cost of the entire code fragment is also n

example 311 we now analyze a code fragment with several for loops
some of which are nested
sum  0
for i1 in i
for j1 ji j
sum
for k0 kn k
ak  k

 first for loop

is a double loop
 second for loop

this code fragment has three separate statements the first assignment
statement and the two for loops again the assignment statement takes
constant time call it c1  the second for loop is just like the one in example 310 and takes c2 n  n time
the first for loop is a double loop and requires a special technique we
work from the inside of the loop outward the expression sum requires
constant time call it c3  because the inner for loop is executed i times by

71


$$@@$$PAGE: 91
72

chap 3 algorithm analysis

simplifying rule 4 it has cost c3 i the outer for loop is executed n times
but each time the cost of the inner loop is different because it costs c3 i with
i changing each time you should see that for the first execution of the outer
loop i is 1 for the second execution of the outer loop i is 2 each time
through the outer loop i becomes one greater until the last time through
the loop when i  n thus the total cost of the loop is c3 times the sum of
the integers 1 through n from equation 21 we know that
n
x
i1

which is
n2 

n2 

i

nn  1

2

by simplifying rule 3 c1  c2 n  c3 n2  is simply

example 312 compare the asymptotic analysis for the following two
code fragments
sum1  0
for i1 in i
for j1 jn j
sum1

 first double loop

do n times

sum2  0
for i1 in i
for j1 ji j
sum2

 second double loop

do i times

in the first double loop the inner for loop always executes n times
because the outer loop executes n times it should be obvious that the statement sum1 is executed precisely n2 times the second
pn loop is similar
to the one analyzed in the previous example with cost j1 j this is approximately 12 n2  thus both double loops cost n2  though the second
requires about half the time of the first

example 313 not all doubly nested for loops are n2  the following pair of nested loops illustrates this fact
sum1  0
for k1 kn k2
for j1 jn j
sum1

 do log n times
 do n times

sum2  0
for k1 kn k2
for j1 jk j
sum2

 do log n times
 do k times


$$@@$$PAGE: 92
sec 35 calculating the running time for a program

73

when analyzing these two code fragments we will assume that n is
a power of two the first code fragment has its outer for loop executed
log n  1 times because on each iteration k is multiplied by two until it
reaches n because the inner loop always executes
plog nn times the total cost for
the first code fragment can be expressed as i0 n note that a variable
substitution takes place here to create the summation with k  2i  from
equation 23 the solution for this summation is n log n in the second
code fragment the outer loop is also executed log n  1 times the inner
loop
cost k which doubles each time the summation can be expressed
phas
log n i
as i0 2 where n is assumed to be a power of two and again k  2i 
from equation 28 we know that this summation is simply n
what about other control statements while loops are analyzed in a manner
similar to for loops the cost of an if statement in the worst case is the greater
of the costs for the then and else clauses this is also true for the average case
assuming that the size of n does not affect the probability of executing one of the
clauses which is usually but not necessarily true for switch statements the
worstcase cost is that of the most expensive branch for subroutine calls simply
add the cost of executing the subroutine
there are rare situations in which the probability for executing the various
branches of an if or switch statement are functions of the input size for example for input of size n the then clause of an if statement might be executed with
probability 1n an example would be an if statement that executes the then
clause only for the smallest of n values to perform an averagecase analysis for
such programs we cannot simply count the cost of the if statement as being the
cost of the more expensive branch in such situations the technique of amortized
analysis see section 143 can come to the rescue
determining the execution time of a recursive subroutine can be difficult the
running time for a recursive subroutine is typically best expressed by a recurrence
relation for example the recursive factorial function fact of section 25 calls
itself with a value one less than its input value the result of this recursive call is
then multiplied by the input value which takes constant time thus the cost of
the factorial function if we wish to measure cost in terms of the number of multiplication operations is one more than the number of multiplications made by the
recursive call on the smaller input because the base case does no multiplications
its cost is zero thus the running time for this function can be expressed as
tn  tn  1  1 for n  1 t 1  0
we know from examples 28 and 213 that the closedform solution for this recurrence relation is n


$$@@$$PAGE: 93
74

chap 3 algorithm analysis

position 0

1

2

3

4

5

6

7

8

9 10 11 12 13 14 15

key 11 13 21 26 29 36 40 41 45 51 54 56 65 72 77 83

figure 34 an illustration of binary search on a sorted array of 16 positions
consider a search for the position with value k  45 binary search first checks
the value at position 7 because 41  k the desired value cannot appear in any
position below 7 in the array next binary search checks the value at position 11
because 56  k the desired value if it exists must be between positions 7
and 11 position 9 is checked next again its value is too great the final search
is at position 8 which contains the desired value thus function binary returns
position 8 alternatively if k were 44 then the same series of record accesses
would be made after checking position 8 binary would return a value of n
indicating that the search is unsuccessful

the final example of algorithm analysis for this section will compare two algorithms for performing search in an array earlier we determined that the running
time for sequential search on an array where the search value k is equally likely
to appear in any location is n in both the average and worst cases we would
like to compare this running time to that required to perform a binary search on
an array whose values are stored in order from lowest to highest
binary search begins by examining the value in the middle position of the array call this position mid and the corresponding value kmid  if kmid  k then
processing can stop immediately this is unlikely to be the case however fortunately knowing the middle value provides useful information that can help guide
the search process in particular if kmid  k then you know that the value k
cannot appear in the array at any position greater than mid thus you can eliminate future search in the upper half of the array conversely if kmid  k then
you know that you can ignore all positions in the array less than mid either way
half of the positions are eliminated from further consideration binary search next
looks at the middle position in that part of the array where value k may exist the
value at this position again allows us to eliminate half of the remaining positions
from consideration this process repeats until either the desired value is found or
there are no positions remaining in the array that might contain the value k figure 34 illustrates the binary search method figure 35 shows an implementation
for binary search
to find the cost of this algorithm in the worst case we can model the running
time as a recurrence and then find the closedform solution each recursive call
to binary cuts the size of the array approximately in half so we can model the
worstcase cost as follows assuming for simplicity that n is a power of two
tn  tn2  1 for n  1

t1  1


$$@@$$PAGE: 94
sec 35 calculating the running time for a program

75

 return the position of an element in sorted array a of
 size n with value k if k is not in a return
 the value n
int binaryint a int n int k 
int l  1
int r  n
 l and r are beyond array bounds
while l1  r   stop when l and r meet
int i  lr2  check middle of remaining subarray
if k  ai r  i
 in left half
if k  ai return i  found it
if k  ai l  i
 in right half

return n  search value not in a

figure 35 implementation for binary search

if we expand the recurrence we find that we can do so only log n times before
we reach the base case and each expansion adds one to the cost thus the closedform solution for the recurrence is tn  log n
function binary is designed to find the single occurrence of k and return
its position a special value is returned if k does not appear in the array this
algorithm can be modified to implement variations such as returning the position
of the first occurrence of k in the array if multiple occurrences are allowed and
returning the position of the greatest value less than k when k is not in the array
comparing sequential search to binary search we see that as n grows the n
running time for sequential search in the average and worst cases quickly becomes
much greater than the log n running time for binary search taken in isolation
binary search appears to be much more efficient than sequential search this is
despite the fact that the constant factor for binary search is greater than that for
sequential search because the calculation for the next search position in binary
search is more expensive than just incrementing the current position as sequential
search does
note however that the running time for sequential search will be roughly the
same regardless of whether or not the array values are stored in order in contrast
binary search requires that the array values be ordered from lowest to highest depending on the context in which binary search is to be used this requirement for a
sorted array could be detrimental to the running time of a complete program because maintaining the values in sorted order requires to greater cost when inserting
new elements into the array this is an example of a tradeoff between the advantage of binary search during search and the disadvantage related to maintaining a
sorted array only in the context of the complete problem to be solved can we know
whether the advantage outweighs the disadvantage


$$@@$$PAGE: 95
76

36

chap 3 algorithm analysis

analyzing problems

you most often use the techniques of algorithm analysis to analyze an algorithm
or the instantiation of an algorithm as a program you can also use these same
techniques to analyze the cost of a problem it should make sense to you to say that
the upper bound for a problem cannot be worse than the upper bound for the best
algorithm that we know for that problem but what does it mean to give a lower
bound for a problem
consider a graph of cost over all inputs of a given size n for some algorithm
for a given problem define a to be the collection of all algorithms that solve
the problem theoretically there are an infinite number of such algorithms now
consider the collection of all the graphs for all of the infinitely many algorithms
in a the worst case lower bound is the least of all the highest points on all the
graphs
it is much easier to show that an algorithm or program is in f n than it
is to show that a problem is in f n for a problem to be in f n means
that every algorithm that solves the problem is in f n even algorithms that we
have not thought of
so far all of our examples of algorithm analysis give obvious results with
bigoh always matching  to understand how bigoh  and  notations are
properly used to describe our understanding of a problem or an algorithm it is best
to consider an example where you do not already know a lot about the problem
let us look ahead to analyzing the problem of sorting to see how this process
works what is the least possible cost for any sorting algorithm in the worst case
the algorithm must at least look at every element in the input just to determine
that the input is truly sorted thus any sorting algorithm must take at least cn time
for many problems this observation that each of the n inputs must be looked at
leads to an easy n lower bound
in your previous study of computer science you have probably seen an example
of a sorting algorithm whose running time is in on2  in the worst case the simple
bubble sort and insertion sort algorithms typically given as examples in a first year
programming course have worst case running times in on2  thus the problem
of sorting can be said to have an upper bound in on2  how do we close the
gap between n and on2  can there be a better sorting algorithm if you can
think of no algorithm whose worstcase growth rate is better than on2  and if you
have discovered no analysis technique to show that the least cost for the problem
of sorting in the worst case is greater than n then you cannot know for sure
whether or not there is a better algorithm
chapter 7 presents sorting algorithms whose running time is in on log n for
the worst case this greatly narrows the gap with this new knowledge we now
have a lower bound in n and an upper bound in on log n should we search


$$@@$$PAGE: 96
sec 37 common misunderstandings

77

for a faster algorithm many have tried without success fortunately or perhaps
unfortunately chapter 7 also includes a proof that any sorting algorithm must
have running time in n log n in the worst case2 this proof is one of the most
important results in the field of algorithm analysis and it means that no sorting
algorithm can possibly run faster than cn log n for the worstcase input of size n
thus we can conclude that the problem of sorting is n log n in the worst case
because the upper and lower bounds have met
knowing the lower bound for a problem does not give you a good algorithm
but it does help you to know when to stop looking if the lower bound for the
problem matches the upper bound for the algorithm within a constant factor then
we know that we can find an algorithm that is better only by a constant factor

37

common misunderstandings

asymptotic analysis is one of the most intellectually difficult topics that undergraduate computer science majors are confronted with most people find growth rates
and asymptotic analysis confusing and so develop misconceptions about either the
concepts or the terminology it helps to know what the standard points of confusion
are in hopes of avoiding them
one problem with differentiating the concepts of upper and lower bounds is
that for most algorithms that you will encounter it is easy to recognize the true
growth rate for that algorithm given complete knowledge about a cost function
the upper and lower bound for that cost function are always the same thus the
distinction between an upper and a lower bound is only worthwhile when you have
incomplete knowledge about the thing being measured if this distinction is still not
clear reread section 36 we use notation to indicate that there is no meaningful
difference between what we know about the growth rates of the upper and lower
bound which is usually the case for simple algorithms
it is a common mistake to confuse the concepts of upper bound or lower bound
on the one hand and worst case or best case on the other the best worst or
average cases each give us a concrete input instance or concrete set of instances
that we can apply to an algorithm description to get a cost measure the upper and
lower bounds describe our understanding of the growth rate for that cost measure
so to define the growth rate for an algorithm or problem we need to determine
what we are measuring the best worst or average case and also our description
for what we know about the growth rate of that cost measure bigoh  or 
the upper bound for an algorithm is not the same as the worst case for that
algorithm for a given input of size n what is being bounded is not the actual cost
which you can determine for a given value of n but rather the growth rate for the
2

while it is fortunate to know the truth it is unfortunate that sorting is n log n rather than
n


$$@@$$PAGE: 97
78

chap 3 algorithm analysis

cost there cannot be a growth rate for a single point such as a particular value
of n the growth rate applies to the change in cost as a change in input size occurs
likewise the lower bound is not the same as the best case for a given size n
another common misconception is thinking that the best case for an algorithm
occurs when the input size is as small as possible or that the worst case occurs
when the input size is as large as possible what is correct is that best and worsecase instances exist for each possible size of input that is for all inputs of a given
size say i one or more of the inputs of size i is the best and one or more of the
inputs of size i is the worst often but not always we can characterize the best
input case for an arbitrary size and we can characterize the worst input case for an
arbitrary size ideally we can determine the growth rate for the characterized best
worst and average cases as the input size grows
example 314 what is the growth rate of the best case for sequential
search for any array of size n the best case occurs when the value we
are looking for appears in the first position of the array this is true regardless of the size of the array thus the best case for arbitrary size n occurs
when the desired value is in the first of n positions and its cost is 1 it is
not correct to say that the best case occurs when n  1

example 315 imagine drawing a graph to show the cost of finding the
maximum value among n values as n grows that is the x axis would
be n and the y value would be the cost of course this is a diagonal line
going up to the right as n increases you might want to sketch this graph
for yourself before reading further
now imagine the graph showing the cost for each instance of the problem of finding the maximum value among say 20 elements in an array
the first position along the x axis of the graph might correspond to having
the maximum element in the first position of the array the second position
along the x axis of the graph might correspond to having the maximum element in the second position of the array and so on of course the cost is
always 20 therefore the graph would be a horizontal line with value 20
you should sketch this graph for yourself
now let us switch to the problem of doing a sequential search for a
given value in an array think about the graph showing all the problem
instances of size 20 the first problem instance might be when the value
we search for is in the first position of the array this has cost 1 the second
problem instance might be when the value we search for is in the second
position of the array this has cost 2 and so on if we arrange the problem
instances of size 20 from least expensive on the left to most expensive on


$$@@$$PAGE: 98
79

sec 38 multiple parameters

the right we see that the graph forms a diagonal line from lower left with
value 0 to upper right with value 20 sketch this graph for yourself
finally let us consider the cost for performing sequential search as the
size of the array n gets bigger what will this graph look like unfortunately theres not one simple answer as there was for finding the maximum
value the shape of this graph depends on whether we are considering the
best case cost that would be a horizontal line with value 1 the worst case
cost that would be a diagonal line with value i at position i along the x
axis or the average cost that would be a a diagonal line with value i2 at
position i along the x axis this is why we must always say that function
f n is in ogn in the best average or worst case if we leave off which
class of inputs we are discussing we cannot know which cost measure we
are referring to for most algorithms

38

multiple parameters

sometimes the proper analysis for an algorithm requires multiple parameters to describe the cost to illustrate the concept consider an algorithm to compute the rank
ordering for counts of all pixel values in a picture pictures are often represented by
a twodimensional array and a pixel is one cell in the array the value of a pixel is
either the code value for the color or a value for the intensity of the picture at that
pixel assume that each pixel can take any integer value in the range 0 to c  1
the problem is to find the number of pixels of each color value and then sort the
color values with respect to the number of times each value appears in the picture
assume that the picture is a rectangle with p pixels a pseudocode algorithm to
solve the problem follows
for i0 ic i
counti  0
for i0 ip i
countvaluei
sortcount c

 initialize count
 look at all of the pixels
 increment a pixel value count
 sort pixel value counts

in this example count is an array of size c that stores the number of pixels for
each color value function valuei returns the color value for pixel i
the time for the first for loop which initializes count is based on the number of colors c the time for the second loop which determines the number of
pixels with each color is p  the time for the final line the call to sort depends on the cost of the sorting algorithm used from the discussion of section 36
we can assume that the sorting algorithm has cost p log p  if p items are sorted
thus yielding p log p  as the total algorithm cost


$$@@$$PAGE: 99
80

chap 3 algorithm analysis

is this a good representation for the cost of this algorithm what is actually being sorted it is not the pixels but rather the colors what if c is much
smaller than p  then the estimate of p log p  is pessimistic because much
fewer than p items are being sorted instead we should use p as our analysis variable for steps that look at each pixel and c as our analysis variable for steps that
look at colors then we get c for the initialization loop p  for the pixel
count loop and c log c for the sorting operation this yields a total cost of
p  c log c
why can we not simply use the value of c for input size and say that the cost
of the algorithm is c log c because c is typically much less than p  for
example a picture might have 1000  1000 pixels and a range of 256 possible
colors so p is one million which is much larger than c log c but if p is
smaller or c larger even if it is still less than p  then c log c can become the
larger quantity thus neither variable should be ignored

39

space bounds

besides time space is the other computing resource that is commonly of concern
to programmers just as computers have become much faster over the years they
have also received greater allotments of memory even so the amount of available
disk space or main memory can be significant constraints for algorithm designers
the analysis techniques used to measure space requirements are similar to those
used to measure time requirements however while time requirements are normally measured for an algorithm that manipulates a particular data structure space
requirements are normally determined for the data structure itself the concepts of
asymptotic analysis for growth rates on input size apply completely to measuring
space requirements
example 316 what are the space requirements for an array of n integers if each integer requires c bytes then the array requires cn bytes
which is n

example 317 imagine that we want to keep track of friendships between
n people we can do this with an array of size n  n each row of the array
represents the friends of an individual with the columns indicating who has
that individual as a friend for example if person j is a friend of person
i then we place a mark in column j of row i in the array likewise we
should also place a mark in column i of row j if we assume that friendship
works both ways for n people the total size of the array is n2 


$$@@$$PAGE: 100
sec 39 space bounds

81

a data structures primary purpose is to store data in a way that allows efficient
access to those data to provide efficient access it may be necessary to store additional information about where the data are within the data structure for example
each node of a linked list must store a pointer to the next value on the list all such
information stored in addition to the actual data values is referred to as overhead
ideally overhead should be kept to a minimum while allowing maximum access
the need to maintain a balance between these opposing goals is what makes the
study of data structures so interesting
one important aspect of algorithm design is referred to as the spacetime tradeoff principle the spacetime tradeoff principle says that one can often achieve a
reduction in time if one is willing to sacrifice space or vice versa many programs
can be modified to reduce storage requirements by packing or encoding information unpacking or decoding the information requires additional time thus the
resulting program uses less space but runs slower conversely many programs can
be modified to prestore results or reorganize information to allow faster running
time at the expense of greater storage requirements typically such changes in time
and space are both by a constant factor
a classic example of a spacetime tradeoff is the lookup table a lookup table
prestores the value of a function that would otherwise be computed each time it is
needed for example 12 is the greatest value for the factorial function that can be
stored in a 32bit int variable if you are writing a program that often computes
factorials it is likely to be much more time efficient to simply precompute and
store the 12 values in a table whenever the program needs the value of n it can
simply check the lookup table if n  12 the value is too large to store as an int
variable anyway compared to the time required to compute factorials it may be
well worth the small amount of additional space needed to store the lookup table
lookup tables can also store approximations for an expensive function such as
sine or cosine if you compute this function only for exact degrees or are willing
to approximate the answer with the value for the nearest degree then a lookup
table storing the computation for exact degrees can be used instead of repeatedly
computing the sine function note that initially building the lookup table requires
a certain amount of time your application must use the lookup table often enough
to make this initialization worthwhile
another example of the spacetime tradeoff is typical of what a programmer
might encounter when trying to optimize space here is a simple code fragment for
sorting an array of integers we assume that this is a special case where there are n
integers whose values are a permutation of the integers from 0 to n  1 this is an
example of a binsort which is discussed in section 77 binsort assigns each value
to an array position corresponding to its value
for i0 in i
bai  ai


$$@@$$PAGE: 101
82

chap 3 algorithm analysis

this is efficient and requires n time however it also requires two arrays
of size n next is a code fragment that places the permutation in order but does so
within the same array thus it is an example of an in place sort
for i0 in i
while ai  i
swapa i ai

function swapa i j exchanges elements i and j in array a it may
not be obvious that the second code fragment actually sorts the array to see that
this does work notice that each pass through the for loop will at least move the
integer with value i to its correct position in the array and that during this iteration
the value of ai must be greater than or equal to i a total of at most n swap
operations take place because an integer cannot be moved out of its correct position
once it has been placed there and each swap operation places at least one integer in
its correct position thus this code fragment has cost n however it requires
more time to run than the first code fragment on my computer the second version
takes nearly twice as long to run as the first but it only requires half the space
a second principle for the relationship between a programs space and time
requirements applies to programs that process information stored on disk as discussed in chapter 8 and thereafter strangely enough the diskbased spacetime
tradeoff principle is almost the reverse of the spacetime tradeoff principle for programs using main memory
the diskbased spacetime tradeoff principle states that the smaller you can
make your disk storage requirements the faster your program will run this is because the time to read information from disk is enormous compared to computation
time so almost any amount of additional computation needed to unpack the data is
going to be less than the diskreading time saved by reducing the storage requirements naturally this principle does not hold true in all cases but it is good to keep
in mind when designing programs that process information stored on disk

310

speeding up your programs

in practice there is not such a big difference in running time between an algorithm
with growth rate n and another with growth rate n log n there is however
an enormous difference in running time between algorithms with growth rates of
n log n and n2  as you shall see during the course of your study of common
data structures and algorithms it is not unusual that a problem whose obvious solution requires n2  time also has a solution requiring n log n time examples
include sorting and searching two of the most important computer problems
example 318 the following is a true story a few years ago one of
my graduate students had a big problem his thesis work involved several


$$@@$$PAGE: 102
sec 310 speeding up your programs

83

intricate operations on a large database he was now working on the final
step dr shaffer he said i am running this program and it seems to
be taking a long time after examining the algorithm we realized that its
running time was n2  and that it would likely take one to two weeks
to complete even if we could keep the computer running uninterrupted
for that long he was hoping to complete his thesis and graduate before
then fortunately we realized that there was a fairly easy way to convert
the algorithm so that its running time was n log n by the next day he
had modified the program it ran in only a few hours and he finished his
thesis on time
while not nearly so important as changing an algorithm to reduce its growth
rate code tuning can also lead to dramatic improvements in running time code
tuning is the art of handoptimizing a program to run faster or require less storage
for many programs code tuning can reduce running time by a factor of ten or
cut the storage requirements by a factor of two or more i once tuned a critical
function in a program  without changing its basic algorithm  to achieve a factor
of 200 speedup to get this speedup however i did make major changes in the
representation of the information converting from a symbolic coding scheme to a
numeric coding scheme on which i was able to do direct computation
here are some suggestions for ways to speed up your programs by code tuning
the most important thing to realize is that most statements in a program do not
have much effect on the running time of that program there are normally just a
few key subroutines possibly even key lines of code within the key subroutines
that account for most of the running time there is little point to cutting in half the
running time of a subroutine that accounts for only 1 of the total running time
focus your attention on those parts of the program that have the most impact
when tuning code it is important to gather good timing statistics many compilers and operating systems include profilers and other special tools to help gather
information on both time and space use these are invaluable when trying to make
a program more efficient because they can tell you where to invest your effort
a lot of code tuning is based on the principle of avoiding work rather than
speeding up work a common situation occurs when we can test for a condition
that lets us skip some work however such a test is never completely free care
must be taken that the cost of the test does not exceed the amount of work saved
while one test might be cheaper than the work potentially saved the test must
always be made and the work can be avoided only some fraction of the time
example 319 a common operation in computer graphics applications is
to find which among a set of complex objects contains a given point in
space many useful data structures and algorithms have been developed to


$$@@$$PAGE: 103
84

chap 3 algorithm analysis

deal with variations of this problem most such implementations involve
the following tuning step directly testing whether a given complex object contains the point in question is relatively expensive instead we can
screen for whether the point is contained within a bounding box for the
object the bounding box is simply the smallest rectangle usually defined
to have sides perpendicular to the x and y axes that contains the object
if the point is not in the bounding box then it cannot be in the object if
the point is in the bounding box only then would we conduct the full comparison of the object versus the point note that if the point is outside the
bounding box we saved time because the bounding box test is cheaper than
the comparison of the full object versus the point but if the point is inside
the bounding box then that test is redundant because we still have to compare the point against the object typically the amount of work avoided by
making this test is greater than the cost of making the test on every object

example 320 section 723 presents a sorting algorithm named selection sort the chief distinguishing characteristic of this algorithm is that
it requires relatively few swaps of records stored in the array to be sorted
however it sometimes performs an unnecessary swap operation where it
tries to swap a record with itself this work could be avoided by testing
whether the two indices being swapped are the same however this event
does not occurr often because the cost of the test is high enough compared
to the work saved when the test is successful adding the test typically will
slow down the program rather than speed it up
be careful not to use tricks that make the program unreadable most code tuning is simply cleaning up a carelessly written program not taking a clear program
and adding tricks in particular you should develop an appreciation for the capabilities of modern compilers to make extremely good optimizations of expressions
optimization of expressions here means a rearrangement of arithmetic or logical
expressions to run more efficiently be careful not to damage the compilers ability
to do such optimizations for you in an effort to optimize the expression yourself
always check that your optimizations really do improve the program by running
the program before and after the change on a suitable benchmark set of input many
times i have been wrong about the positive effects of code tuning in my own programs most often i am wrong when i try to optimize an expression it is hard to
do better than the compiler
the greatest time and space improvements come from a better data structure or
algorithm the final thought for this section is
first tune the algorithm then tune the code


$$@@$$PAGE: 104
sec 311 empirical analysis

311

85

empirical analysis

this chapter has focused on asymptotic analysis this is an analytic tool whereby
we model the key aspects of an algorithm to determine the growth rate of the algorithm as the input size grows as pointed out previously there are many limitations to this approach these include the effects at small problem size determining
the finer distinctions between algorithms with the same growth rate and the inherent difficulty of doing mathematical modeling for more complex problems
an alternative to analytical approaches are empirical ones the most obvious
empirical approach is simply to run two competitors and see which performs better
in this way we might overcome the deficiencies of analytical approaches
be warned that comparative timing of programs is a difficult business often
subject to experimental errors arising from uncontrolled factors system load the
language or compiler used etc the most important point is not to be biased in
favor of one of the programs if you are biased this is certain to be reflected in
the timings one look at competing software or hardware vendors advertisements
should convince you of this the most common pitfall when writing two programs
to compare their performance is that one receives more codetuning effort than the
other as mentioned in section 310 code tuning can often reduce running time by
a factor of ten if the running times for two programs differ by a constant factor
regardless of input size ie their growth rates are the same then differences in
code tuning might account for any difference in running time be suspicious of
empirical comparisons in this situation
another approach to analysis is simulation the idea of simulation is to model
the problem with a computer program and then run it to get a result in the context of algorithm analysis simulation is distinct from empirical comparison of two
competitors because the purpose of the simulation is to perform analysis that might
otherwise be too difficult a good example of this appears in figure 910 this
figure shows the cost for inserting or deleting a record from a hash table under two
different assumptions for the policy used to find a free slot in the table the y axes
is the cost in number of hash table slots evaluated and the x axes is the percentage
of slots in the table that are full the mathematical equations for these curves can
be determined but this is not so easy a reasonable alternative is to write simple
variations on hashing by timing the cost of the program for various loading conditions it is not difficult to construct a plot similar to figure 910 the purpose of
this analysis is not to determine which approach to hashing is most efficient so we
are not doing empirical comparison of hashing alternatives instead the purpose
is to analyze the proper loading factor that would be used in an efficient hashing
system to balance time cost versus hash table size space cost


$$@@$$PAGE: 105
86

chap 3 algorithm analysis

312

further reading

pioneering works on algorithm analysis include the art of computer programming
by donald e knuth knu97 knu98 and the design and analysis of computer
algorithms by aho hopcroft and ullman ahu74 the alternate definition for
 comes from ahu83 the use of the notation tn is in of n rather
than the more commonly used tn  of n i derive from brassard and
bratley bb96 though certainly this use predates them a good book to read for
further information on algorithm analysis techniques is compared to what by
gregory je rawlins raw92
bentley ben88 describes one problem in numerical analysis for which between 1945 and 1988 the complexity of the best known algorithm had decreased
from on7  to on3  for a problem of size n  64 this is roughly equivalent
to the speedup achieved from all advances in computer hardware during the same
time period
while the most important aspect of program efficiency is the algorithm much
improvement can be gained from efficient coding of a program as cited by frederick p brooks in the mythical manmonth bro95 an efficient programmer can often produce programs that run five times faster than an inefficient programmer even
when neither takes special efforts to speed up their code for excellent and enjoyable essays on improving your coding efficiency and ways to speed up your code
when it really matters see the books by jon bentley ben82 ben00 ben88 the
situation described in example 318 arose when we were working on the project
reported on in su92
as an interesting aside writing a correct binary search algorithm is not easy
knuth knu98 notes that while the first binary search was published in 1946 the
first bugfree algorithm was not published until 1962 bentley writing correct
programs in ben00 has found that 90 of the computer professionals he tested
could not write a bugfree binary search in two hours

313

exercises

31 for each of the six expressions of figure 31 give the range of values of n
for which that expression is most efficient
32 graph the following expressions for each expression state the range of
values of n for which that expression is the most efficient
4n2

3n

log3 n

20n

2

log2 n

n23

33 arrange the following expressions by growth rate from slowest to fastest
4n2

log3 n

n

3n

20n

2

log2 n

n23

see stirlings approximation in section 22 for help in classifying n


$$@@$$PAGE: 106
sec 313 exercises

87

a suppose that a particular algorithm has time complexity tn  3 
2n  and that executing an implementation of it on a particular machine
takes t seconds for n inputs now suppose that we are presented with a
machine that is 64 times as fast how many inputs could we process on
the new machine in t seconds
b suppose that another algorithm has time complexity tn  n2  and
that executing an implementation of it on a particular machine takes
t seconds for n inputs now suppose that we are presented with a machine that is 64 times as fast how many inputs could we process on
the new machine in t seconds
c a third algorithm has time complexity tn  8n executing an implementation of it on a particular machine takes t seconds for n inputs
given a new machine that is 64 times as fast how many inputs could
we process in t seconds
35 hardware vendor xyz corp claims that their latest computer will run 100
times faster than that of their competitor prunes inc if the prunes inc
computer can execute a program on input of size n in one hour what size
input can xyzs computer execute in one hour for each algorithm with the
following growth rate equations
n
n2
n3
2n
34

36

a find a growth rate that squares the run time when we double the input
size that is if tn  x then t2n  x2
b find a growth rate that cubes the run time when we double the input
size that is if tn  x then t2n  x3
37 using the definition of bigoh show that 1 is in o1 and that 1 is in on
38 using the definitions of bigoh and  find the upper and lower bounds for
the following expressions be sure to state appropriate values for c and n0 
a
b
c
d

c1 n
c2 n3  c3
c4 n log n  c5 n
c6 2n  c7 n6


a what is the smallest integer k such that n  onk 
b what is the smallest integer k such that n log n  onk 
310 a is 2n  3n explain why or why not
b is 2n  3n  explain why or why not
311 for each of the following pairs of functions either f n is in ogn f n
is in gn or f n  gn for each pair determine which relationship is correct justify your answer using the method of limits discussed in
section 345
39


$$@@$$PAGE: 107
88

chap 3 algorithm analysis

a
b
c
d
e
f
g
h
i
j
k

f n  log n2  gn  log n  5

f n  n gn  log n2 
f n  log2 n gn  log n
f n  n gn  log 2 n
f n  n log n  n gn  log n
f n  log n2  gn  log n2 
f n  10 gn  log 10
f n  2n  gn  10n2 
f n  2n  gn  n log n
f n  2n  gn  3n 
f n  2n  gn  nn 

312 determine  for the following code fragments in the average case assume
that all variables are of type int
a a  b  c
d  a  e

b sum  0
for i0 i3 i
for j0 jn j
sum

c sum0
for i0 inn i
sum

d for i0 i  n1 i
for ji1 j  n j 
tmp  aij
aij  aji
aji  tmp


e sum  0
for i1 in i
for j1 jn j2
sum

f sum  0
for i1 in i2
for j1 jn j
sum

g assume that array a contains n values random takes constant time
and sort takes n log n steps
for i0 in i 
for j0 jn j
aj  randomn
sorta n



$$@@$$PAGE: 108
sec 313 exercises

89

h assume array a contains a random permutation of the values from 0 to
n  1
sum  0
for i0 in i
for j0 aji j
sum

i sum  0
if evenn
for i0 in i
sum
else
sum  sum  n

313 show that bigtheta notation  defines an equivalence relation on the set
of functions
314 give the best lower bound that you can for the following code fragment as a
function of the initial value of n
while n  1
if oddn
n  3  n  1
else
n  n  2

315

316

317

318

319

do you think that the upper bound is likely to be the same as the answer you
gave for the lower bound
does every algorithm have a  runningtime equation in other words are
the upper and lower bounds for the running time on any specified class of
inputs always the same
does every problem for which there exists some algorithm have a  runningtime equation in other words for every problem and for any specified
class of inputs is there some algorithm whose upper bound is equal to the
problems lower bound
given an array storing integers ordered by value modify the binary search
routine to return the position of the first integer with value k in the situation
where k can appear multiple times in the array be sure that your algorithm
is log n that is do not resort to sequential search once an occurrence of
k is found
given an array storing integers ordered by value modify the binary search
routine to return the position of the integer with the greatest value less than
k when k itself does not appear in the array return error if the least
value in the array is greater than k
modify the binary search routine to support search in an array of infinite
size in particular you are given as input a sorted array and a key value
k to search for call n the position of the smallest value in the array that


$$@@$$PAGE: 109
90

chap 3 algorithm analysis

is equal to or larger than x provide an algorithm that can determine n in
olog n comparisons in the worst case explain why your algorithm meets
the required time bound
320 it is possible to change the way that we pick the dividing point in a binary
search and still get a working search routine however where we pick the
dividing point could affect the performance of the algorithm
a if we change the dividing point computation in function binary from
i  l  r2 to i  l  r  l3 what will the worstcase running time be in asymptotic terms if the difference is only a constant
time factor how much slower or faster will the modified program be
compared to the original version of binary
b if we change the dividing point computation in function binary from
i  l  r2 to i  r  2 what will the worstcase running time be in
asymptotic terms if the difference is only a constant time factor how
much slower or faster will the modified program be compared to the
original version of binary
321 design an algorithm to assemble a jigsaw puzzle assume that each piece
has four sides and that each pieces final orientation is known top bottom
etc assume that you have available a function
bool comparepiece a piece b side ad
that can tell in constant time whether piece a connects to piece b on as
side ad and bs opposite side bd the input to your algorithm should consist
of an n  m array of random pieces along with dimensions n and m the
algorithm should put the pieces in their correct positions in the array your
algorithm should be as efficient as possible in the asymptotic sense write
a summation for the running time of your algorithm on n pieces and then
derive a closedform solution for the summation
322 can the average case cost for an algorithm be worse than the worst case cost
can it be better than the best case cost explain why or why not
323 prove that if an algorithm is f n in the average case then it is f n
in the worst case
324 prove that if an algorithm is f n in the average case then it is of n
in the best case

314

projects

31 imagine that you are trying to store 32 boolean values and must access
them frequently compare the time required to access boolean values stored
alternatively as a single bit field a character a short integer or a long integer
there are two things to be careful of when writing your program first be


$$@@$$PAGE: 110
sec 314 projects

91

sure that your program does enough variable accesses to make meaningful
measurements a single access takes much less time than a single unit of
measurement typically milliseconds for all four methods second be sure
that your program spends as much time as possible doing variable accesses
rather than other things such as calling timing functions or incrementing for
loop counters
32 implement sequential search and binary search algorithms on your computer
run timings for each algorithm on arrays of size n  10i for i ranging from
1 to as large a value as your computers memory and compiler will allow for
both algorithms store the values 0 through n  1 in order in the array and
use a variety of random search values in the range 0 to n  1 on each size
n graph the resulting times when is sequential search faster than binary
search for a sorted array
33 implement a program that runs and gives timings for the two fibonacci sequence functions provided in exercise 211 graph the resulting running
times for as many values of n as your computer can handle


$$@@$$PAGE: 111

$$@@$$PAGE: 112
part ii
fundamental data structures

93


$$@@$$PAGE: 113

$$@@$$PAGE: 114
4
lists stacks and queues

if your program needs to store a few things  numbers payroll records or job descriptions for example  the simplest and most effective approach might be to put
them in a list only when you have to organize and search through a large number
of things do more sophisticated data structures usually become necessary we will
study how to organize and search through medium amounts of data in chapters 5 7
and 9 and discuss how to deal with large amounts of data in chapters 810 many
applications dont require any form of search and they do not require that any ordering be placed on the objects being stored some applications require processing
in a strict chronological order processing objects in the order that they arrived or
perhaps processing objects in the reverse of the order that they arrived for all these
situations a simple list structure is appropriate
this chapter describes representations for lists in general as well as two important listlike structures called the stack and the queue along with presenting these
fundamental data structures the other goals of the chapter are to 1 give examples
of separating a logical representation in the form of an adt from a physical implementation for a data structure 2 illustrate the use of asymptotic analysis in the
context of some simple operations that you might already be familiar with in this
way you can begin to see how asymptotic analysis works without the complications that arise when analyzing more sophisticated algorithms and data structures
3 introduce the concept and use of dictionaries
we begin by defining an adt for lists in section 41 two implementations for
the list adt  the arraybased list and the linked list  are covered in detail and
their relative merits discussed sections 42 and 43 cover stacks and queues respectively sample implementations for each of these data structures are presented
section 44 presents the dictionary adt for storing and retrieving data which sets
a context for implementing search structures such as the binary search tree of
section 54

95


$$@@$$PAGE: 115
96

41

chap 4 lists stacks and queues

lists

we all have an intuitive understanding of what we mean by a list our first step is
to define precisely what is meant so that this intuitive understanding can eventually
be converted into a concrete data structure and its operations the most important
concept related to lists is that of position in other words we perceive that there
is a first element in the list a second element and so on we should view a list as
embodying the mathematical concepts of a sequence as defined in section 21
we define a list to be a finite ordered sequence of data items known as elements ordered in this definition means that each element has a position in the
list we will not use ordered in this context to mean that the list elements are
sorted by value each list element has a data type in the simple list implementations discussed in this chapter all elements of the list have the same data type
although there is no conceptual objection to lists whose elements have differing
data types if the application requires it see section 121 the operations defined
as part of the list adt do not depend on the elemental data type for example the
list adt can be used for lists of integers lists of characters lists of payroll records
even lists of lists
a list is said to be empty when it contains no elements the number of elements currently stored is called the length of the list the beginning of the list is
called the head the end of the list is called the tail there might or might not be
some relationship between the value of an element and its position in the list for
example sorted lists have their elements positioned in ascending order of value
while unsorted lists have no particular relationship between element values and
positions this section will consider only unsorted lists chapters 7 and 9 treat the
problems of how to create and search sorted lists efficiently
when presenting the contents of a list we use the same notation as was introduced for sequences in section 21 to be consistent with c array indexing
the first position on the list is denoted as 0 thus if there are n elements in the
list they are given positions 0 through n  1 as ha0  a1   an1 i the subscript
indicates an elements position within the list using this notation the empty list
would appear as hi
before selecting a list implementation a program designer should first consider
what basic operations the implementation must support our common intuition
about lists tells us that a list should be able to grow and shrink in size as we insert
and remove elements we should be able to insert and remove elements from anywhere in the list we should be able to gain access to any elements value either to
read it or to change it we must be able to create and clear or reinitialize lists it
is also convenient to access the next or previous element from the current one
the next step is to define the adt for a list object in terms of a set of operations
on that object we will use the c notation of anabstract class to formally define


$$@@$$PAGE: 116
sec 41 lists

97

the list adt an abstract class is one whose member functions are all declared to be
pure virtual as indicated by the 0 notation at the end of the member function
declarations class list defines the member functions that any list implementation inheriting from it must support along with their parameters and return types
we increase the flexibility of the list adt by writing it as a c template
true to the notion of an adt anabstract class does not specify how operations
are implemented two complete implementations are presented later in this section both of which use the same list adt to define their operations but they are
considerably different in approaches and in their spacetime tradeoffs
figure 41 presents our list adt class list is a template of one parameter
named e for element e serves as a placeholder for whatever element type the
user would like to store in a list the comments given in figure 41 describe precisely what each member function is intended to do however some explanation
of the basic design is in order given that we wish to support the concept of a sequence with access to any position in the list the need for many of the member
functions such as insert and movetopos is clear the key design decision embodied in this adt is support for the concept of a current position for example
member movetostart sets the current position to be the first element on the list
while methods next and prev move the current position to the next and previous elements respectively the intention is that any implementation for this adt
support the concept of a current position the current position is where any action
such as insertion or deletion will take place
since insertions take place at the current position and since we want to be able
to insert to the front or the back of the list as well as anywhere in between there are
actually n  1 possible current positions when there are n elements in the list
it is helpful to modify our list display notation to show the position of the
current element i will use a vertical bar such as h20 23  12 15i to indicate
the list of four elements with the current position being to the right of the bar at
element 12 given this configuration calling insert with value 10 will change
the list to be h20 23  10 12 15i
if you examine figure 41 you should find that the list member functions provided allow you to build a list with elements in any desired order and to access
any desired position in the list you might notice that the clear method is not
necessary in that it could be implemented by means of the other member functions
in the same asymptotic time it is included merely for convenience
method getvalue returns a pointer to the current element it is considered a
violation of getvalues preconditions to ask for the value of a nonexistent element ie there must be something to the right of the vertical bar in our concrete
list implementations assertions are used to enforce such preconditions in a commercial implementation such violations would be best implemented by the c
exception mechanism


$$@@$$PAGE: 117
98

chap 4 lists stacks and queues

template typename e class list   list adt
private
void operator const list 
 protect assignment
listconst list 
 protect copy constructor
public
list 
 default constructor
virtual list   base destructor
 clear contents from the list to make it empty
virtual void clear  0
 insert an element at the current location
 item the element to be inserted
virtual void insertconst e item  0
 append an element at the end of the list
 item the element to be appended
virtual void appendconst e item  0
 remove and return the current element
 return the element that was removed
virtual e remove  0
 set the current position to the start of the list
virtual void movetostart  0
 set the current position to the end of the list
virtual void movetoend  0
 move the current position one step left no change
 if already at beginning
virtual void prev  0
 move the current position one step right no change
 if already at end
virtual void next  0
 return the number of elements in the list
virtual int length const  0
 return the position of the current element
virtual int currpos const  0
 set current position
 pos the position to make current
virtual void movetoposint pos  0
 return the current element
virtual const e getvalue const  0

figure 41 the adt for a list


$$@@$$PAGE: 118
sec 41 lists

99

a list can be iterated through as shown in the following code fragment
for lmovetostart lcurrposllength lnext 
it  lgetvalue
dosomethingit


in this example each element of the list in turn is stored in it and passed to the
dosomething function the loop terminates when the current position reaches
the end of the list
the declaration for abstract class list also makes private the class copy constructor and an overloading for the assignment operator this protects the class
from accidentally being copied this is done in part to simplify the example code
used in this book a fullfeatured list implementation would likely support copying
and assigning list objects
the list class declaration presented here is just one of many possible interpretations for lists figure 41 provides most of the operations that one naturally expects
to perform on lists and serves to illustrate the issues relevant to implementing the
list data structure as an example of using the list adt we can create a function
to return true if there is an occurrence of a given integer in the list and false
otherwise the find method needs no knowledge about the specific list implementation just the list adt
 return true if k is in list l false otherwise
bool findlistint l int k 
int it
for lmovetostart lcurrposllength lnext 
it  lgetvalue
if k  it return true  found k

return false
 k not found


while this implementation for find could be written as a template with respect
to the element type it would still be limited in its ability to handle different data
types stored on the list in particular it only works when the description for the
object being searched for k in the function is of the same type as the objects
themselves and that can meaningfully be compared when using the  comparison
operator a more typical situation is that we are searching for a record that contains
a key field whos value matches k similar functions to find and return a composite
element based on a key value can be created using the list implementation but to
do so requires some agreement between the list adt and the find function on the
concept of a key and on how keys may be compared this topic will be discussed
in section 44


$$@@$$PAGE: 119
100
411

chap 4 lists stacks and queues

arraybased list implementation

there are two standard approaches to implementing lists the arraybased list and
the linked list this section discusses the arraybased approach the linked list is
presented in section 412 time and space efficiency comparisons for the two are
discussed in section 413
figure 42 shows the arraybased list implementation named alist alist
inherits from abstract class list and so must implement all of the member functions of list
class alists private portion contains the data members for the arraybased
list these include listarray the array which holds the list elements because
listarray must be allocated at some fixed size the size of the array must be
known when the list object is created note that an optional parameter is declared
for the alist constructor with this parameter the user can indicate the maximum
number of elements permitted in the list the phrase defaultsize indicates
that the parameter is optional if no parameter is given then it takes the value
defaultsize which is assumed to be a suitably defined constant value
because each list can have a differently sized array each list must remember
its maximum permitted size data member maxsize serves this purpose at any
given time the list actually holds some number of elements that can be less than the
maximum allowed by the array this value is stored in listsize data member
curr stores the current position because listarray maxsize listsize
and curr are all declared to be private they may only be accessed by methods
of class alist
class alist stores the list elements in the first listsize contiguous array
positions array positions correspond to list positions in other words the element
at position i in the list is stored at array cell i the head of the list is always at
position 0 this makes random access to any element in the list quite easy given
some position in the list the value of the element in that position can be accessed
directly thus access to any element using the movetopos method followed by
the getvalue method takes 1 time
because the arraybased list implementation is defined to store list elements in
contiguous cells of the array the insert append and remove methods must
maintain this property inserting or removing elements at the tail of the list is easy
so the append operation takes 1 time but if we wish to insert an element at
the head of the list all elements currently in the list must shift one position toward
the tail to make room as illustrated by figure 43 this process takes n time
if there are n elements already in the list if we wish to insert at position i within
a list of n elements then n  i elements must shift toward the tail removing an
element from the head of the list is similar in that all remaining elements must shift
toward the head by one position to fill in the gap to remove the element at position


$$@@$$PAGE: 120
101

sec 41 lists

template typename e  arraybased list implementation
class alist  public liste 
private
int maxsize
 maximum size of list
int listsize
 number of list items now
int curr
 position of current element
e listarray
 array holding list elements
public
alistint sizedefaultsize   constructor
maxsize  size
listsize  curr  0
listarray  new emaxsize

alist  delete  listarray   destructor
void clear 
delete  listarray
listsize  curr  0
listarray  new emaxsize


 reinitialize the list
 remove the array
 reset the size
 recreate array

 insert it at current position
void insertconst e it 
assertlistsize  maxsize list capacity exceeded
forint ilistsize icurr i  shift elements up
listarrayi  listarrayi1 
to make room
listarraycurr  it
listsize
 increment list size

void appendconst e it 
 append it
assertlistsize  maxsize list capacity exceeded
listarraylistsize  it

 remove and return the current element
e remove 
assertcurr0  curr  listsize no element
e it  listarraycurr
 copy the element
forint icurr ilistsize1 i  shift them down
listarrayi  listarrayi1
listsize
 decrement size
return it

figure 42 an arraybased list implementation


$$@@$$PAGE: 121
102

chap 4 lists stacks and queues

void
void
void
void

movetostart  curr  0 
 reset position
movetoend  curr  listsize 
 set at end
prev  if curr  0 curr 
 back up
next  if curr  listsize curr   next

 return list size
int length const  return listsize 
 return current position
int currpos const  return curr 
 set current list position to pos
void movetoposint pos 
assert pos0poslistsize pos out of range
curr  pos

const e getvalue const   return current element
assertcurr0currlistsizeno current element
return listarraycurr


figure 42 continued

insert 23
13 12 20 8
0

1

2

3

3

13 12 20

4

5

0

1

a

2

3

8

3

4

5

b

23 13 12 20 8
0

1

2

3

4

3
5

c

figure 43 inserting an element at the head of an arraybased list requires shifting all existing elements in the array by one position toward the tail a a list
containing five elements before inserting an element with value 23 b the list
after shifting all existing elements one position to the right c the list after 23
has been inserted in array position 0 shading indicates the unused part of the
array

i n  i  1 elements must shift toward the head in the average case insertion or
removal requires moving half of the elements which is n
most of the other member functions for class alist simply access the current
list element or move the current position such operations all require 1 time
aside from insert and remove the only other operations that might require


$$@@$$PAGE: 122
103

sec 41 lists

 singly linked list node
template typename e class link 
public
e element
 value for this node
link next
 pointer to next node in list
 constructors
linkconst e elemval link nextval null
 element  elemval next  nextval 
linklink nextval null  next  nextval 

figure 44 a simple singly linked list node implementation

more than constant time are the constructor the destructor and clear these
three member functions each make use of the system freestoreoperators new and
delete as discussed further in section 412 system freestore operations can
be expensive in particular the cost to delete listarray depends in part on the
type of elements it stores and whether the delete operator must call a destructor
on each one
412

linked lists

the second traditional approach to implementing lists makes use of pointers and is
usually called a linked list the linked list uses dynamic memory allocation that
is it allocates memory for new list elements as needed
a linked list is made up of a series of objects called the nodes of the list
because a list node is a distinct object as opposed to simply a cell in an array it is
good practice to make a separate list node class an additional benefit to creating a
list node class is that it can be reused by the linked implementations for the stack
and queue data structures presented later in this chapter figure 44 shows the
implementation for list nodes called the link class objects in the link class
contain an element field to store the element value and a next field to store a
pointer to the next node on the list the list built from such nodes is called a singly
linked list or a oneway list because each list node has a single pointer to the next
node on the list
the link class is quite simple there are two forms for its constructor one
with an initial element value and one without because the link class is also
used by the stack and queue implementations presented later its data members are
made public while technically this is breaking encapsulation in practice the link
class should be implemented as a private class of the linked list or stack or queue
implementation and thus not visible to the rest of the program
figure 45a shows a graphical depiction for a linked list storing four integers
the value stored in a pointer variable is indicated by an arrow pointing to something c uses the special symbol null for a pointer value that points nowhere
such as for the last list nodes next field a null pointer is indicated graphically


$$@@$$PAGE: 123
104

chap 4 lists stacks and queues

head
20

23

curr

tail

12

15

a

head
20

curr
23

10

tail
12

15

b

figure 45 illustration of a faulty linkedlist implementation where curr points
directly to the current node a linked list prior to inserting element with
value 10 b desired effect of inserting element with value 10

by a diagonal slash through a pointer variables box the vertical line between the
nodes labeled 23 and 12 in figure 45a indicates the current position immediately
to the right of this line
the lists first node is accessed from a pointer named head to speed access
to the end of the list and to allow the append method to be performed in constant
time a pointer named tail is also kept to the last link of the list the position of
the current element is indicated by another pointer named curr finally because
there is no simple way to compute the length of the list simply from these three
pointers the list length must be stored explicitly and updated by every operation
that modifies the list size the value cnt stores the length of the list
class llist also includes private helper methods init and removeall
they are used by llists constructor destructor and clear methods
note that llists constructor maintains the optional parameter for minimum
list size introduced for class alist this is done simply to keep the calls to the
constructor the same for both variants because the linked list class does not need
to declare a fixedsize array when the list is created this parameter is unnecessary
for linked lists it is ignored by the implementation
a key design decision for the linked list implementation is how to represent
the current position the most reasonable choices appear to be a pointer to the
current element but there is a big advantage to making curr point to the element
preceding the current element
figure 45a shows the lists curr pointer pointing to the current element the
vertical line between the nodes containing 23 and 12 indicates the logical position
of the current element consider what happens if we wish to insert a new node with
value 10 into the list the result should be as shown in figure 45b however
there is a problem to splice the list node containing the new element into the
list the list node storing 23 must have its next pointer changed to point to the new


$$@@$$PAGE: 124
105

sec 41 lists

head

curr
20

tail

23

12

15

a

head

curr
20

tail

23

10

12

15

b

figure 46 insertion using a header node with curr pointing one node head of
the current element a linked list before insertion the current node contains 12
b linked list after inserting the node containing 10

node unfortunately there is no convenient access to the node preceding the one
pointed to by curr
there is an easy solution to this problem if we set curr to point directly to
the preceding element there is no difficulty in adding a new element after curr
figure 46 shows how the list looks when pointer variable curr is set to point to the
node preceding the physical current node see exercise 45 for further discussion
of why making curr point directly to the current element fails
we encounter a number of potential special cases when the list is empty or
when the current position is at an end of the list in particular when the list is empty
we have no element for head tail and curr to point to implementing special
cases for insert and remove increases code complexity making it harder to
understand and thus increases the chance of introducing a programming bug
these special cases can be eliminated by implementing linked lists with an
additional header node as the first node of the list this header node is a link
node like any other but its value is ignored and it is not considered to be an actual
element of the list the header node saves coding effort because we no longer need
to consider special cases for empty lists or when the current position is at one end
of the list the cost of this simplification is the space for the header node however
there are space savings due to smaller code size because statements to handle the
special cases are omitted in practice this reduction in code size typically saves
more space than that required for the header node depending on the number of
lists created figure 47 shows the state of an initialized or empty list when using a
header node
figure 48 shows the definition for the linked list class named llist class
llist inherits from the abstract list class and thus must implement all of class
lists member functions


$$@@$$PAGE: 125
106

chap 4 lists stacks and queues

curr

tail

head
figure 47 initial state of a linked list when using a header node

implementations for most member functions of the list class are straightforward however insert and remove should be studied carefully
inserting a new element is a threestep process first the new list node is
created and the new element is stored into it second the next field of the new
list node is assigned to point to the current node the one after the node that curr
points to third the next field of node pointed to by curr is assigned to point to
the newly inserted node the following line in the insert method of figure 48
does all three of these steps
currnext  new linkeit currnext

operator new creates the new link node and calls the link class constructor which
takes two parameters the first is the element the second is the value to be placed
in the list nodes next field in this casecurrnext figure 49 illustrates
this threestep process once the new node is added tail is pushed forward if the
new element was added to the end of the list insertion requires 1 time
removing a node from the linked list requires only that the appropriate pointer
be redirected around the node to be deleted the following lines from the remove
method of figure 48 do precisely this
linke ltemp  currnext
 remember link node
currnext  currnextnext  remove from list

we must be careful not to lose the memory for the deleted link node so temporary pointer ltemp is first assigned to point to the node being removed a call
to delete is later used to return the old node to free storage figure 410 illustrates the remove methodassuming that the freestore delete operator requires
constant time removing an element requires 1 time
method next simply moves curr one position toward the tail of the list
which takes 1 time method prev moves curr one position toward the head
of the list but its implementation is more difficult in a singly linked list there is
no pointer to the previous node thus the only alternative is to march down the list
from the beginning until we reach the current node being sure always to remember
the node before it because that is what we really want this takes n time in
the average and worst cases implementation of method movetopos is similar in
that finding the ith position requires marching down i positions from the head of
the list taking i time
implementations for the remaining operations each require 1 time


$$@@$$PAGE: 126
107

sec 41 lists

 linked list implementation
template typename e class llist public liste 
private
linke head
 pointer to list header
linke tail
 pointer to last element
linke curr
 access to current element
int cnt
 size of list
void init 
 intialization helper method
curr  tail  head  new linke
cnt  0

void removeall 
 return link nodes to free store
whilehead  null 
curr  head
head  headnext
delete curr


public
llistint sizedefaultsize  init 
 constructor
llist  removeall 
 destructor
void print const
 print list contents
void clear  removeall init 
 clear list
 insert it at current position
void insertconst e it 
currnext  new linkeit currnext
if tail  curr tail  currnext  new tail
cnt

void appendconst e it   append it to list
tail  tailnext  new linkeit null
cnt

 remove and return current element
e remove 
assertcurrnext  null no element
e it  currnextelement
 remember value
linke ltemp  currnext
 remember link node
if tail  currnext tail  curr  reset tail
currnext  currnextnext
 remove from list
delete ltemp
 reclaim space
cnt
 decrement the count
return it

figure 48 a linked list implementation


$$@@$$PAGE: 127
108

chap 4 lists stacks and queues

void movetostart  place curr at list start
 curr  head 
void movetoend
 place curr at list end
 curr  tail 
 move curr one step left no change if already at front
void prev 
if curr  head return
 no previous element
linke temp  head
 march down list until we find the previous element
while tempnextcurr temptempnext
curr  temp

 move curr one step right no change if already at end
void next
 if curr  tail curr  currnext 
int length const

 return cnt   return length

 return the position of the current element
int currpos const 
linke temp  head
int i
for i0 curr  temp i
temp  tempnext
return i

 move down list to pos position
void movetoposint pos 
assert pos0poscnt position out of range
curr  head
forint i0 ipos i curr  currnext

const e getvalue const   return current element
assertcurrnext  null no value
return currnextelement


figure 48 continued


$$@@$$PAGE: 128
109

sec 41 lists

curr
23



12



insert 10 10
a

curr



23

12



3
10
1

2
b

figure 49 the linked list insertion process a the linked list before insertion
b the linked list after insertion 1 marks the element field of the new link
node 2 marks the next field of the new link node which is set to point to what
used to be the current node the node with value 12 3 marks the next field of
the node preceding the current position it used to point to the node containing 12
now it points to the new node containing 10

curr



23

10

12



12



a

curr
2



23

10
it

1
b

figure 410 the linked list removal process a the linked list before removing
the node with value 10 b the linked list after removal 1 marks the list node
being removed it is set to point to the element 2 marks the next field of
the preceding list node which is set to point to the node following the one being
deleted


$$@@$$PAGE: 129
110

chap 4 lists stacks and queues

freelists
thec freestore management operators new and delete are relatively expensive to use section 123 discusses how generalpurpose memory managers are
implemented the expense comes from the fact that freestore routines must be capable of handling requests to and from free store with no particular pattern as well
as memory requests of vastly different sizes thismakes them inefficient compared
to what might be implemented for more controlled patterns of memory access
list nodes are created and deleted in a linked list implementation in a way
that allows the link class programmer to provide simple but efficient memory
management routines instead of making repeated calls to new and delete the
link class can handle its own freelist a freelist holds those list nodes that are not
currently being used when a node is deleted from a linked list it is placed at the
head of the freelist when a new element is to be added to a linked list the freelist
is checked to see if a list node is available if so the node is taken from the freelist
if the freelist is empty the standard new operator must then be called
freelists are particularly useful for linked lists that periodically grow and then
shrink the freelist will never grow larger than the largest size yet reached by the
linked list requests for new nodes after the list has shrunk can be handled by
the freelist another good opportunity to use a freelist occurs when a program uses
multiple lists so long as they do not all grow and shrink together the free list can
let link nodes move between the lists
one approach to implementing freelists would be to create two new operators
to use instead of the standard freestore routines new and delete this requires
that the users code such as the linked list class implementation of figure 48 be
modified to call these freelist operators a second approach is to use c operator
overloading to replace the meaning of new and delete when operating on link
class objects in this way programs that use the llist class need not be modified
at all to take advantage of a freelist whether the link class is implemented with
freelists or relies on the regular freestore mechanism is entirely hidden from the
list class user figure 411 shows the reimplementation for the link classwith
freelist methods overloading the standard freestore operators note how simple
they are because they need only remove and add an element to the front of the
freelist respectively the freelist versions of new and delete both run in 1
time except in the case where the freelist is exhausted and the new operation must
be called on my computer a call to the overloaded new and delete operators
requires about one tenth of the time required by the system freestore operators
there is an additional efficiency gain to be had from a freelist implementation
the implementation of figure 411 makes a separate call to the system new operator for each link node requested whenever the freelist is empty these link nodes
tend to be small  only a few bytes more than the size of the element field if at
some point in time the program requires thousands of active link nodes these will


$$@@$$PAGE: 130
sec 41 lists

111

 singly linked list node with freelist support
template typename e class link 
private
static linke freelist  reference to freelist head
public
e element
 value for this node
link next
 point to next node in list
 constructors
linkconst e elemval link nextval null
 element  elemval next  nextval 
linklink nextval null  next  nextval 
void operator newsize t   overloaded new operator
if freelist  null return new link  create space
linke temp  freelist  can take from freelist
freelist  freelistnext
return temp
 return the link

 overloaded delete operator
void operator deletevoid ptr 
linkeptrnext  freelist  put on freelist
freelist  linkeptr


 the freelist head pointer is actually created here
template typename e
linke linkefreelist  null
figure 411 implementation for the link class with a freelist note that the
redefinition for new refers to new on the third line this indicates that the
standard c new operator is used rather than the redefined new operator if
the colons had not been used then the link class new operator would be called
setting up an infinite recursion the static declaration for member freelist
means that all link class objects share the same freelist pointer variable instead
of each object storing its own copy

have been created by many calls to the system version of new an alternative is to
allocate many link nodes in a single call to the system version of new anticipating
that if the freelist is exhausted now more nodes will be needed soon it is faster to
make one call to new to get space for 100 link nodes and then load all 100 onto
the freelist at once rather than to make 100 separate calls to new the following
statement will assign ptr to point to an array of 100 link nodes
ptr  new link100

the implementation for the new operator in the link class could then place each
of these 100 nodes onto the freelist


$$@@$$PAGE: 131
112

chap 4 lists stacks and queues

the freelist variable declaration uses the keyword static this creates
a single variable shared among all instances of the link nodes we want only a
single freelist for all link nodes of a given type a program might create multiple
lists if they are all of the same type that is their element types are the same then
they can and should share the same freelist this will happen with the implementation of figure 411 if lists are created that have different element types because
this code is implemented with a template the need for different list implementations will be discovered by the compiler at compile time separate versions of the
list class will be generated for each element type thus each element type will
also get its own separate copy of the link class and each distinct link class
implementation will get a separate freelist
413

comparison of list implementations

now that you have seen two substantially different implementations for lists it is
natural to ask which is better in particular if you must implement a list for some
task which implementation should you choose
arraybased lists have the disadvantage that their size must be predetermined
before the array can be allocated arraybased lists cannot grow beyond their predetermined size whenever the list contains only a few elements a substantial
amount of space might be tied up in a largely empty array linked lists have the
advantage that they only need space for the objects actually on the list there is
no limit to the number of elements on a linked list as long as there is freestore
memory available the amount of space required by a linked list is n while the
space required by the arraybased list implementation is n but can be greater
arraybased lists have the advantage that there is no wasted space for an individual element linked lists require that an extra pointer be added to every list
node if the element size is small then the overhead for links can be a significant
fraction of the total storage when the array for the arraybased list is completely
filled there is no storage overhead the arraybased list will then be more space
efficient by a constant factor than the linked implementation
a simple formula can be used to determine whether the arraybased list or
linked list implementation will be more space efficient in a particular situation
call n the number of elements currently in the list p the size of a pointer in storage units typically four bytes e the size of a data element in storage units this
could be anything from one bit for a boolean variable on up to thousands of bytes
or more for complex records and d the maximum number of list elements that
can be stored in the array the amount of space required for the arraybased list is
de regardless of the number of elements actually stored in the list at any given
time the amount of space required for the linked list is np  e the smaller
of these expressions for a given value n determines the more spaceefficient implementation for n elements in general the linked implementation requires less space


$$@@$$PAGE: 132
113

sec 41 lists

than the arraybased implementation when relatively few elements are in the list
conversely the arraybased implementation becomes more space efficient when
the array is close to full using the equation we can solve for n to determine
the breakeven point beyond which the arraybased implementation is more space
efficient in any particular situation this occurs when
n  dep  e
if p  e then the breakeven point is at d2 this would happen if the element
field is either a fourbyte int value or a pointer and the next field is a typical fourbyte pointer that is the arraybased implementation would be more efficient if
the link field and the element field are the same size whenever the array is more
than half full
as a rule of thumb linked lists are more space efficient when implementing
lists whose number of elements varies widely or is unknown arraybased lists are
generally more space efficient when the user knows in advance approximately how
large the list will become
arraybased lists are faster for random access by position positions can easily
be adjusted forwards or backwards by the next and prev methods these operations always take 1 time in contrast singly linked lists have no explicit access
to the previous element and access by position requires that we march down the
list from the front or the current position to the specified position both of these
operations require n time in the average and worst cases if we assume that
each position on the list is equally likely to be accessed on any call to prev or
movetopos
given a pointer to a suitable location in the list the insert and remove
methods for linked lists require only 1 time arraybased lists must shift the remainder of the list up or down within the array this requires n time in the average and worst cases for many applications the time to insert and delete elements
dominates all other operations for this reason linked lists are often preferred to
arraybased lists
when implementing the arraybased list an implementor could allow the size
of the array to grow and shrink depending on the number of elements that are
actually stored this data structure is known as a dynamic array both the java and
c stl vector classes implement a dynamic array dynamic arrays allow the
programmer to get around the limitation on the standard array that its size cannot
be changed once the array has been created this also means that space need not
be allocated to the dynamic array until it is to be used the disadvantage of this
approach is that it takes time to deal with space adjustments on the array each time
the array grows in size its contents must be copied a good implementation of the
dynamic array will grow and shrink the array in such a way as to keep the overall
cost for a series of insertdelete operations relatively inexpensive even though an


$$@@$$PAGE: 133
114

chap 4 lists stacks and queues

occasional insertdelete operation might be expensive a simple rule of thumb is
to double the size of the array when it becomes full and to cut the array size in
half when it becomes one quarter full to analyze the overall cost of dynamic array
operations over time we need to use a technique known as amortized analysis
which is discussed in section 143
414

element implementations

list users must decide whether they wish to store a copy of any given element on
each list that contains it for small elements such as an integer this makes sense
if the elements are payroll records it might be desirable for the list node to store a
pointer to the record rather than store a copy of the record itself this change would
allow multiple list nodes or other data structures to point to the same record
rather than make repeated copies of the record not only might this save space but
it also means that a modification to an elements value is automatically reflected
at all locations where it is referenced the disadvantage of storing a pointer to
each element is that the pointer requires space of its own if elements are never
duplicated then this additional space adds unnecessary overhead
the c implementations for lists presented in this section give the user of the
list the choice of whether to store copies of elements or pointers to elements the
user can declare e to be for example a pointer to a payroll record in this case
multiple lists can point to the same copy of the record on the other hand if the
user declares e to be the record itself then a new copy of the record will be made
when it is inserted into the list
whether it is more advantageous to use pointers to shared elements or separate
copies depends on the intended application in general the larger the elements and
the more they are duplicated the more likely that pointers to shared elements is the
better approach
a second issue faced by implementors of a list class or any other data structure
that stores a collection of userdefined data elements is whether the elements stored
are all required to be of the same type this is known as homogeneity in a data
structure in some applications the user would like to define the class of the data
element that is stored on a given list and then never permit objects of a different
class to be stored on that same list in other applications the user would like to
permit the objects stored on a single list to be of differing types
for the list implementations presented in this section the compiler requires
that all objects stored on the list be of the same type in fact because the lists are
implemented using templates a new class is created by the compiler for each data
type for implementors who wish to minimize the number of classes created by
the compiler the lists can all store a void pointer with the user performing the
necessary casting to and from the actual object type for each element however this


$$@@$$PAGE: 134
sec 41 lists

115

approach requires that the user do his or her own type checking either to enforce
homogeneity or to differentiate between the various object types
besides c templates there are other techniques that implementors of a list
class can use to ensure that the element type for a given list remains fixed while
still permitting different lists to store different element types one approach is to
store an object of the appropriate type in the header node of the list perhaps an
object of the appropriate type is supplied as a parameter to the list constructor and
then check that all insert operations on that list use the same element type
the third issue that users of the list implementations must face is primarily of
concern when programming in languages that do not support automatic garbage
collection that is how to deal with the memory of the objects stored on the list
when the list is deleted or the clear method is called the list destructor and the
clear method are problematic in that there is a potential that they will bemisused
thus causing a memory leak the type of the element stored determines whether
there is a potential for trouble here if the elements are of a simple type such as an
int then there is no need to delete the elements explicitly if the elements are of a
userdefined class then their own destructor will be called however what if the list
elements are pointers to objects then deleting listarray in the arraybased
implementation or deleting a link node in the linked list implementation might
remove the only reference to an object leaving its memory space inaccessible
unfortunately there is no way for the list implementation to know whether a given
object is pointed to in another part of the program or not thus the user of the list
must be responsible for deleting these objects when that is appropriate
415

doubly linked lists

the singly linked list presented in section 412 allows for direct access from a
list node only to the next node in the list a doubly linked list allows convenient
access from a list node to the next node and also to the preceding node on the list
the doubly linked list node accomplishes this in the obvious way by storing two
pointers one to the node following it as in the singly linked list and a second
pointer to the node preceding it the most common reason to use a doubly linked
list is because it is easier to implement than a singly linked list while the code for
the doubly linked implementation is a little longer than for the singly linked version
it tends to be a bit more obvious in its intention and so easier to implement
and debug figure 412 illustrates the doubly linked list concept whether a list
implementation is doubly or singly linked should be hidden from the list class
user
like our singly linked list implementation the doubly linked list implementation makes use of a header node we also add a tailer node to the end of the list
the tailer is similar to the header in that it is a node that contains no value and it
always exists when the doubly linked list is initialized the header and tailer nodes


$$@@$$PAGE: 135
116

chap 4 lists stacks and queues

head

curr
20

tail
23

12

15

figure 412 a doubly linked list

are created data member head points to the header node and tail points to
the tailer node the purpose of these nodes is to simplify the insert append
and remove methods by eliminating all need for specialcase code when the list
is empty or when we insert at the head or tail of the list
for singly linked lists we set curr to point to the node preceding the node that
contained the actual current element due to lack of access to the previous node
during insertion and deletion since we do have access to the previous node in a
doubly linked list this is no longer necessary we could set curr to point directly
to the node containing the current element however i have chosen to keep the
same convention for the curr pointer as we set up for singly linked lists purely
for the sake of consistency
figure 413 shows the complete implementation for a link class to be used
with doubly linked lists this code is a little longer than that for the singly linked list
node implementation since the doubly linked list nodes have an extra data member
figure 414 shows the implementation for the insert append remove
and prev doubly linked list methods the class declaration and the remaining
member functions for the doubly linked list class are nearly identical to the singly
linked list version
the insert method is especially simple for our doubly linked list implementation because most of the work is done by the nodes constructor figure 415
shows the list before and after insertion of a node with value 10
the three parameters to the new operator allow the list node class constructor
to set the element prev and next fields respectively for the new link node
the new operator returns a pointer to the newly created node the nodes to either
side have their pointers updated to point to the newly created node the existence
of the header and tailer nodes mean that there are no special cases to worry about
when inserting into an empty list
the append method is also simple again the link class constructor sets the
element prev and next fields of the node when the new operator is executed
method remove illustrated by figure 416 is straightforward though the
code is somewhat longer first the variable it is assigned the value being removed note that we must separate the element which is returned to the caller
from the link object the following lines then adjust the list


$$@@$$PAGE: 136
117

sec 41 lists

 doubly linked list link node with freelist support
template typename e class link 
private
static linke freelist  reference to freelist head
public
e element
link next
link prev

 value for this node
 pointer to next node in list
 pointer to previous node

 constructors
linkconst e it link prevp link nextp 
element  it
prev  prevp
next  nextp

linklink prevp null link nextp null 
prev  prevp
next  nextp

void operator newsize t   overloaded new operator
if freelist  null return new link  create space
linke temp  freelist  can take from freelist
freelist  freelistnext
return temp
 return the link

 overloaded delete operator
void operator deletevoid ptr 
linkeptrnext  freelist  put on freelist
freelist  linkeptr


 the freelist head pointer is actually created here
template typename e
linke linkefreelist  null
figure 413 doubly linked list node implementation with a freelist


$$@@$$PAGE: 137
118

chap 4 lists stacks and queues

 insert it at current position
void insertconst e it 
currnext  currnextprev 
new linkeit curr currnext
cnt

 append it to the end of the list
void appendconst e it 
tailprev  tailprevnext 
new linkeit tailprev tail
cnt

 remove and return current element
e remove 
if currnext  tail
 nothing to remove
return null
e it  currnextelement
 remember value
linke ltemp  currnext
 remember link node
currnextnextprev  curr
currnext  currnextnext  remove from list
delete ltemp
 reclaim space
cnt
 decrement cnt
return it

 move fence one step left no change if left is empty
void prev 
if curr  head  cant back up from list head
curr  currprev

figure 414 implementations for doubly linked list insert append
remove and prev methods
linke ltemp  currnext
 remember link node
currnextnextprev  curr
currnext  currnextnext  remove from list
delete ltemp
 reclaim space

the first line sets a temporary pointer to the node being removed the second
line makes the next nodes prev pointer point to the left of the node being removed
finally the next field of the node preceding the one being deleted is adjusted the
final steps of method remove are to update the listlength return the deleted node
to free store and return the value of the deleted element
the only disadvantage of the doubly linked list as compared to the singly linked
list is the additional space used the doubly linked list requires two pointers per
node and so in the implementation presented it requires twice as much overhead
as the singly linked list


$$@@$$PAGE: 138
119

sec 41 lists

curr
20



insert 10

23

12



10
a

curr

5

4
20



23

10

12



3 1 2
b

figure 415 insertion for doubly linked lists the labels 1  2  and 3 correspond to assignments done by the linked list node constructor 4 marks the
assignment to currnext 5 marks the assignment to the prev pointer of
the node following the newly inserted node

curr
20



23

12



12



a

curr



20
it

23
b

figure 416 doubly linked list removal element it stores the element of the
node being removed then the nodes to either side have their pointers adjusted


$$@@$$PAGE: 139
120

chap 4 lists stacks and queues

example 41 there is a spacesaving technique that can be employed to
eliminate the additional space requirement though it will complicate the
implementation and be somewhat slower thus this is an example of a
spacetime tradeoff it is based on observing that if we store the sum of
two values then we can get either value back by subtracting the other that
is if we store a  b in variable c then b  c  a and a  c  b of course
to recover one of the values out of the stored summation the other value
must be supplied a pointer to the first node in the list along with the value
of one of its two link fields will allow access to all of the remaining nodes
of the list in order this is because the pointer to the node must be the same
as the value of the following nodes prev pointer as well as the previous
nodes next pointer it is possible to move down the list breaking apart
the summed link fields as though you were opening a zipper details for
implementing this variation are left as an exercise
the principle behind this technique is worth remembering as it has
many applications the following code fragment will swap the contents
of two variables without using a temporary variable at the cost of three
arithmetic operations
a  a  b
b  a  b  now b contains original value of a
a  a  b  now a contains original value of b

a similar effect can be had by using the exclusiveor operator this fact
is widely used in computer graphics a region of the computer screen can
be highlighted by xoring the outline of a box around it xoring the box
outline a second time restores the original contents of the screen

42

stacks

the stack is a listlike structure in which elements may be inserted or removed
from only one end while this restriction makes stacks less flexible than lists it
also makes stacks both efficient for those operations they can do and easy to implement many applications require only the limited form of insert and remove
operations that stacks provide in such cases it is more efficient to use the simpler stack data structure rather than the generic list for example the freelist of
section 412 is really a stack
despite their restrictions stacks have many uses thus a special vocabulary
for stacks has developed accountants used stacks long before the invention of the
computer they called the stack a lifo list which stands for lastin first


$$@@$$PAGE: 140
121

sec 42 stacks

 stack abtract class
template typename e class stack 
private
void operator const stack 
 protect assignment
stackconst stack 
 protect copy constructor
public
stack 
virtual stack 

 default constructor
 base destructor

 reinitialize the stack the user is responsible for
 reclaiming the storage used by the stack elements
virtual void clear  0
 push an element onto the top of the stack
 it the element being pushed onto the stack
virtual void pushconst e it  0
 remove the element at the top of the stack
 return the element at the top of the stack
virtual e pop  0
 return a copy of the top element
virtual const e topvalue const  0
 return the number of elements in the stack
virtual int length const  0

figure 417 the stack adt

out note that one implication of the lifo policy is that stacks remove elements
in reverse order of their arrival
the accessible element of the stack is called the top element elements are not
said to be inserted they are pushed onto the stack when removed an element is
said to be popped from the stack figure 417 shows a sample stack adt
as with lists there are many variations on stack implementation the two approaches presented here are arraybased and linked stacks which are analogous
to arraybased and linked lists respectively
421

arraybased stacks

figure 418 shows a complete implementation for the arraybased stack class as
with the arraybased list implementation listarray must be declared of fixed
size when the stack is created in the stack constructor size serves to indicate
this size method top acts somewhat like a current position value because the
current position is always at the top of the stack as well as indicating the number
of elements currently in the stack


$$@@$$PAGE: 141
122

chap 4 lists stacks and queues

 arraybased stack implementation
template typename e class astack public stacke 
private
int maxsize
 maximum size of stack
int top
 index for top element
e listarray
 array holding stack elements
public
astackint size defaultsize
 constructor
 maxsize  size top  0 listarray  new esize 
astack  delete  listarray 

 destructor

void clear  top  0 

 reinitialize

void pushconst e it 
 put it on stack
asserttop  maxsize stack is full
listarraytop  it

e pop 
 pop top element
asserttop  0 stack is empty
return listarraytop

const e topvalue const 
 return top element
asserttop  0 stack is empty
return listarraytop1

int length const  return top 


 return length

figure 418 arraybased stack class implementation

the arraybased stack implementation is essentially a simplified version of the
arraybased list the only important design decision to be made is which end of
the array should represent the top of the stack one choice is to make the top be
at position 0 in the array in terms of list functions all insert and remove
operations would then be on the element in position 0 this implementation is
inefficient because now every push or pop operation will require that all elements
currently in the stack be shifted one position in the array for a cost of n if there
are n elements the other choice is have the top element be at position n  1 when
there are n elements in the stack in other words as elements are pushed onto
the stack they are appended to the tail of the list method pop removes the tail
element in this case the cost for each push or pop operation is only 1
for the implementation of figure 418 top is defined to be the array index of
the first free position in the stack thus an empty stack has top set to 0 the first
available free position in the array alternatively top could have been defined to


$$@@$$PAGE: 142
sec 42 stacks

123

be the index for the top element in the stack rather than the first free position if
this had been done the empty list would initialize top as 1 methods push and
pop simply place an element into or remove an element from the array position
indicated by top because top is assumed to be at the first free position push
first inserts its value into the top position and then increments top while pop first
decrements top and then removes the top element

422

linked stacks

the linked stack implementation is quite simple the freelist of section 412 is
an example of a linked stack elements are inserted and removed only from the
head of the list a header node is not used because no specialcase code is required
for lists of zero or one elements figure 419 shows the complete linked stack
implementation the only data member is top a pointer to the first top link node
of the stack method push first modifies the next field of the newly created link
node to point to the top of the stack and then sets top to point to the new link
node method pop is also quite simple variable temp stores the top nodes value
while ltemp links to the top node as it is removed from the stack the stack is
updated by setting top to point to the next link in the stack the old top node is
then returned to free store or the freelist and the element value is returned

423

comparison of arraybased and linked stacks

all operations for the arraybased and linked stack implementations take constant
time so from a time efficiency perspective neither has a significant advantage
another basis for comparison is the total space required the analysis is similar to
that done for list implementations the arraybased stack must declare a fixedsize
array initially and some of that space is wasted whenever the stack is not full the
linked stack can shrink and grow but requires the overhead of a link field for every
element
when multiple stacks are to be implemented it is possible to take advantage of
the oneway growth of the arraybased stack this can be done by using a single
array to store two stacks one stack grows inward from each end as illustrated by
figure 420 hopefully leading to less wasted space however this only works well
when the space requirements of the two stacks are inversely correlated in other
words ideally when one stack grows the other will shrink this is particularly
effective when elements are taken from one stack and given to the other if instead
both stacks grow at the same time then the free space in the middle of the array
will be exhausted quickly


$$@@$$PAGE: 143
124

chap 4 lists stacks and queues

 linked stack implementation
template typename e class lstack public stacke 
private
linke top
 pointer to first element
int size
 number of elements
public
lstackint sz defaultsize  constructor
 top  null size  0 
lstack  clear 

 destructor

void clear 
while top  null 
linke temp  top
top  topnext
delete temp

size  0


 reinitialize
 delete link nodes

void pushconst e it   put it on stack
top  new linkeit top
size

e pop 
 remove it from stack
asserttop  null stack is empty
e it  topelement
linke ltemp  topnext
delete top
top  ltemp
size
return it

const e topvalue const   return top value
asserttop  0 stack is empty
return topelement

int length const  return size   return length

figure 419 linked stack class implementation

top1

top2

figure 420 two stacks implemented within in a single array both growing
toward the middle


$$@@$$PAGE: 144
125

sec 42 stacks

currptr 3
n

currptr  2
n
currptr 1

currptr



call fact4

n

4

currptr



call fact3

currptr 1

n 3
currptr 1

n 4

n 4

currptr

3

2
currptr  2



call fact2

currptr



call fact1

currptr  2
n

3

currptr 1

return 24
currptr 1

n

4

n

4

currptr



currptr



return 1

return 2

currptr



return 6

figure 421 implementing recursion with a stack  values indicate the address
of the program instruction to return to after completing the current function call
on each recursive function call to fact as implemented in section 25 both the
return address and the current value of n must be saved each return from fact
pops the top activation record off the stack

424

implementing recursion

perhaps the most common computer application that uses stacks is not even visible
to its users this is the implementation of subroutine calls in most programming
language runtime environments a subroutine call is normally implemented by
placing necessary information about the subroutine including the return address
parameters and local variables onto a stack this information is called an activation record further subroutine calls add to the stack each return from a
subroutine pops the top activation record off the stack figure 421 illustrates the
implementation of the recursive factorial function of section 25 from the runtime
environments point of view
consider what happens when we call fact with the value 4 we use  to
indicate the address of the program instruction where the call to fact is made
thus the stack must first store the address  and the value 4 is passed to fact


$$@@$$PAGE: 145
126

chap 4 lists stacks and queues

next a recursive call to fact is made this time with value 3 we will name the
program address from which the call is made 1  the address 1  along with the
current value for n which is 4 is saved on the stack function fact is invoked
with input parameter 3
in similar manner another recursive call is made with input parameter 2 requiring that the address from which the call is made say 2  and the current value
for n which is 3 are stored on the stack a final recursive call with input parameter 1 is made requiring that the stack store the calling address say 3  and current
value which is 2
at this point we have reached the base case for fact and so the recursion
begins to unwind each return from fact involves popping the stored value for
n from the stack along with the return address from the function call the return
value for fact is multiplied by the restored value for n and the result is returned
because an activation record must be created and placed onto the stack for
each subroutine call making subroutine calls is a relatively expensive operation
while recursion is often used to make implementation easy and clear sometimes
you might want to eliminate the overhead imposed by the recursive function calls
in some cases such as the factorial function of section 25 recursion can easily be
replaced by iteration
example 42 as a simple example of replacing recursion with a stack
consider the following nonrecursive version of the factorial function
long factint n stackint s   compute n
 to fit n in a long variable require n  12
assertn  0  n  12 input out of range
while n  1 spushn  load up the stack
long result  1
 holds final result
whileslength  0
result  result  spop
 compute
return result


here we simply push successively smaller values of n onto the stack until the base case is reached then repeatedly pop off the stored values and
multiply them into the result
an iterative form of the factorial function is both simpler and faster than the
version shown in example 42 but it is not always possible to replace recursion
with iteration recursion or some imitation of it is necessary when implementing
algorithms that require multiple branching such as in the towers of hanoi algorithm or when traversing a binary tree the mergesort and quicksort algorithms
of chapter 7 are also examples in which recursion is required fortunately it is always possible to imitate recursion with a stack let us now turn to a nonrecursive
version of the towers of hanoi function which cannot be done iteratively


$$@@$$PAGE: 146
sec 43 queues

127

example 43 the toh function shown in figure 22 makes two recursive
calls one to move n  1 rings off the bottom ring and another to move
these n  1 rings back to the goal pole we can eliminate the recursion by
using a stack to store a representation of the three operations that toh must
perform two recursive calls and a move operation to do so we must first
come up with a representation of the various operations implemented as a
class whose objects will be stored on the stack
figure 422 shows such a class we first define an enumerated type
called tohop with two values move and toh to indicate calls to the
move function and recursive calls to toh respectively class tohobj
stores five values an operation field indicating either a move or a new
toh operation the number of rings and the three poles note that the
move operation actually needs only to store information about two poles
thus there are two constructors one to store the state when imitating a
recursive call and one to store the state for a move operation
an arraybased stack is used because we know that the stack will need
to store exactly 2n1 elements the new version of toh begins by placing
on the stack a description of the initial problem for n rings the rest of
the function is simply a while loop that pops the stack and executes the
appropriate operation in the case of a toh operation for n  0 we
store on the stack representations for the three operations executed by the
recursive version however these operations must be placed on the stack
in reverse order so that they will be popped off in the correct order
recursive algorithms lend themselves to efficient implementation with a stack
when the amount of information needed to describe a subproblem is small for
example section 75 discusses a stackbased implementation for quicksort

43

queues

like the stack the queue is a listlike structure that provides restricted access to
its elements queue elements may only be inserted at the back called an enqueue
operation and removed from the front called a dequeue operation queues operate like standing in line at a movie theater ticket counter1 if nobody cheats then
newcomers go to the back of the line the person at the front of the line is the next
to be served thus queues release their elements in order of arrival accountants
have used queues since long before the existence of computers they call a queue
a fifo list which stands for firstin firstout figure 423 shows a sample
1

in britain a line of people is called a queue and getting into line to wait for service is called
queuing up


$$@@$$PAGE: 147
128

chap 4 lists stacks and queues

 operation choices domove will move a disk
 dotoh corresponds to a recursive call
enum tohop  domove dotoh 
class tohobj   an operation object
public
tohop op
 this operation type
int num
 how many disks
pole start goal tmp  define pole order
 dotoh operation constructor
tohobjint n pole s pole g pole t 
op  dotoh num  n
start  s goal  g tmp  t

 domove operation constructor
tohobjpole s pole g
 op  domove start  s goal  g 

void tohint n pole start pole goal pole tmp
stacktohobj s 
spushnew tohobjn start goal tmp  initial
tohobj t
while slength  0 
 grab next task
t  spop
if top  domove
 do a move
movetstart tgoal
else if tnum  0 
 store in reverse 3 recursive statements
int num  tnum
pole tmp  ttmp pole goal  tgoal
pole start  tstart
spushnew tohobjnum1 tmp goal start
spushnew tohobjstart goal
spushnew tohobjnum1 start tmp goal

delete t  must delete the tohobj we made


figure 422 stackbased implementation for towers of hanoi

queue adt this section presents two implementations for queues the arraybased
queue and the linked queue
431

arraybased queues

the arraybased queue is somewhat tricky to implement effectively a simple conversion of the arraybased list implementation is not efficient
assume that there are n elements in the queue by analogy to the arraybased
list implementation we could require that all elements of the queue be stored in the
first n positions of the array if we choose the rear element of the queue to be in


$$@@$$PAGE: 148
129

sec 43 queues

 abstract queue class
template typename e class queue 
private
void operator const queue 
 protect assignment
queueconst queue 
 protect copy constructor
public
queue 
 default
virtual queue   base destructor
 reinitialize the queue the user is responsible for
 reclaiming the storage used by the queue elements
virtual void clear  0
 place an element at the rear of the queue
 it the element being enqueued
virtual void enqueueconst e  0
 remove and return element at the front of the queue
 return the element at the front of the queue
virtual e dequeue  0
 return a copy of the front element
virtual const e frontvalue const  0
 return the number of elements in the queue
virtual int length const  0

figure 423 the c adt for a queue

position 0 then dequeue operations require only 1 time because the front element of the queue the one being removed is the last element in the array however
enqueue operations will require n time because the n elements currently in
the queue must each be shifted one position in the array if instead we chose the
rear element of the queue to be in position n  1 then an enqueue operation is
equivalent to an append operation on a list this requires only 1 time but
now a dequeue operation requires n time because all of the elements must
be shifted down by one position to retain the property that the remaining n  1
queue elements reside in the first n  1 positions of the array
a far more efficient implementation can be obtained by relaxing the requirement that all elements of the queue must be in the first n positions of the array
we will still require that the queue be stored be in contiguous array positions but
the contents of the queue will be permitted to drift within the array as illustrated
by figure 424 now both the enqueue and the dequeue operations can be
performed in 1 time because no other elements in the queue need be moved
this implementation raises a new problem assume that the front element of
the queue is initially at position 0 and that elements are added to successively


$$@@$$PAGE: 149
130

chap 4 lists stacks and queues

front

rear

20 5 12 17
a

rear

front
12 17 3

30 4
b

figure 424 after repeated use elements in the arraybased queue will drift to
the back of the array a the queue after the initial four numbers 20 5 12 and 17
have been inserted b the queue after elements 20 and 5 are deleted following
which 3 30 and 4 are inserted

front

front
20

5
12
17

12
17

rear

3
30
4
a

b

rear

figure 425 the circular queue with array positions increasing in the clockwise
direction a the queue after the initial four numbers 20 5 12 and 17 have been
inserted b the queue after elements 20 and 5 are deleted following which 3
30 and 4 are inserted

highernumbered positions in the array when elements are removed from the
queue the front index increases over time the entire queue will drift toward
the highernumbered positions in the array once an element is inserted into the
highestnumbered position in the array the queue has run out of space this happens despite the fact that there might be free positions at the low end of the array
where elements have previously been removed from the queue
the drifting queue problem can be solved by pretending that the array is
circular and so allow the queue to continue directly from the highestnumbered
position in the array to the lowestnumbered position this is easily implemented
through use of the modulus operator denoted by  in c  in this way positions
in the array are numbered from 0 through size1 and position size1 is defined to immediately precede position 0 which is equivalent to position size 
size figure 425 illustrates this solution


$$@@$$PAGE: 150
sec 43 queues

131

there remains one more serious though subtle problem to the arraybased
queue implementation how can we recognize when the queue is empty or full
assume that front stores the array index for the front element in the queue and
rear stores the array index for the rear element if both front and rear have the
same position then with this scheme there must be one element in the queue thus
an empty queue would be recognized by having rear be one less than front taking into account the fact that the queue is circular so position size1 is actually
considered to be one less than position 0 but what if the queue is completely full
in other words what is the situation when a queue with n array positions available
contains n elements in this case if the front element is in position 0 then the
rear element is in position size1 but this means that the value for rear is one
less than the value for front when the circular nature of the queue is taken into
account in other words the full queue is indistinguishable from the empty queue
you might think that the problem is in the assumption about front and rear
being defined to store the array indices of the front and rear elements respectively
and that some modification in this definition will allow a solution unfortunately
the problem cannot be remedied by a simple change to the definition for front
and rear because of the number of conditions or states that the queue can be in
ignoring the actual position of the first element and ignoring the actual values of
the elements stored in the queue how many different states are there there can
be no elements in the queue one element two and so on at most there can be
n elements in the queue if there are n array positions this means that there are
n  1 different states for the queue 0 through n elements are possible
if the value of front is fixed then n  1 different values for rear are needed
to distinguish among the n  1 states however there are only n possible values for
rear unless we invent a special case for say empty queues this is an example of
the pigeonhole principle defined in exercise 230 the pigeonhole principle states
that given n pigeonholes and n  1 pigeons when all of the pigeons go into the
holes we can be sure that at least one hole contains more than one pigeon in similar
manner we can be sure that two of the n  1 states are indistinguishable by the n
relative values of front and rear we must seek some other way to distinguish
full from empty queues
one obvious solution is to keep an explicit count of the number of elements in
the queue or at least a boolean variable that indicates whether the queue is empty
or not another solution is to make the array be of size n  1 and only allow
n elements to be stored which of these solutions to adopt is purely a matter of the
implementors taste in such affairs my choice is to use an array of size n  1
figure 426 shows an arraybased queue implementation listarray holds
the queue elements and as usual the queue constructor allows an optional parameter to set the maximum size of the queue the array as created is actually large
enough to hold one element more than the queue will allow so that empty queues


$$@@$$PAGE: 151
132

chap 4 lists stacks and queues

 arraybased queue implementation
template typename e class aqueue public queuee 
private
int maxsize
 maximum size of queue
int front
 index of front element
int rear
 index of rear element
e listarray
 array holding queue elements
public
aqueueint size defaultsize   constructor
 make list array one position larger for empty slot
maxsize  size1
rear  0 front  1
listarray  new emaxsize

aqueue  delete  listarray   destructor
void clear  rear  0 front  1   reinitialize
void enqueueconst e it 
 put it in queue
assertrear2  maxsize  front queue is full
rear  rear1  maxsize
 circular increment
listarrayrear  it

e dequeue 
 take element out
assertlength  0 queue is empty
e it  listarrayfront
front  front1  maxsize
 circular increment
return it

const e frontvalue const   get front value
assertlength  0 queue is empty
return listarrayfront

virtual int length const
 return length
 return rearmaxsize  front  1  maxsize 

figure 426 an arraybased queue implementation


$$@@$$PAGE: 152
sec 44 dictionaries

133

can be distinguished from full queues member maxsize is used to control the
circular motion of the queue it is the base for the modulus operator member
rear is set to the position of the current rear element while front is the position
of the current front element
in this implementation the front of the queue is defined to be toward the
lower numbered positions in the array in the counterclockwise direction in figure 425 and the rear is defined to be toward the highernumbered positions thus
enqueue increments the rear pointer modulus size and dequeue increments
the front pointer implementation of all member functions is straightforward
432

linked queues

the linked queue implementation is a straightforward adaptation of the linked list
figure 427 shows the linked queue class declaration methods front and rear
are pointers to the front and rear queue elements respectively we will use a header
link node which allows for a simpler implementation of the enqueue operation by
avoiding any special cases when the queue is empty on initialization the front
and rear pointers will point to the header node and front will always point to
the header node while rear points to the true last link node in the queue method
enqueue places the new element in a link node at the end of the linked list ie
the node that rear points to and then advances rear to point to the new link
node method dequeue removes and returns the first element of the list
433

comparison of arraybased and linked queues

all member functions for both the arraybased and linked queue implementations
require constant time the space comparison issues are the same as for the equivalent stack implementations unlike the arraybased stack implementation there is
no convenient way to store two queues in the same array unless items are always
transferred directly from one queue to the other

44

dictionaries

the most common objective of computer programs is to store and retrieve data
much of this book is about efficient ways to organize collections of data records
so that they can be stored and retrieved quickly in this section we describe a
simple interface for such a collection called a dictionary the dictionary adt
provides operations for storing records finding records and removing records from
the collection this adt gives us a standard basis for comparing various data
structures
before we can discuss the interface for a dictionary we must first define the
concepts of a key and comparable objects if we want to search for a given record


$$@@$$PAGE: 153
134

chap 4 lists stacks and queues

 linked queue implementation
template typename e class lqueue public queuee 
private
linke front
 pointer to front queue node
linke rear
 pointer to rear queue node
int size
 number of elements in queue
public
lqueueint sz defaultsize  constructor
 front  rear  new linke size  0 
lqueue  clear delete front 

 destructor

void clear 
 clear queue
whilefrontnext  null   delete each link node
rear  front
delete rear

rear  front
size  0

void enqueueconst e it   put element on rear
rearnext  new linkeit null
rear  rearnext
size

e dequeue 
 remove element from front
assertsize  0 queue is empty
e it  frontnextelement  store dequeued value
linke ltemp  frontnext  hold dequeued link
frontnext  ltempnext
 advance front
if rear  ltemp rear  front  dequeue last element
delete ltemp
 delete link
size 
return it
 return element value

const e frontvalue const   get front element
assertsize  0 queue is empty
return frontnextelement

virtual int length const  return size 

figure 427 linked queue class implementation


$$@@$$PAGE: 154
sec 44 dictionaries

135

in a database how should we describe what we are looking for a database record
could simply be a number or it could be quite complicated such as a payroll record
with many fields of varying types we do not want to describe what we are looking
for by detailing and matching the entire contents of the record if we knew everything about the record already we probably would not need to look for it instead
we typically define what record we want in terms of a key value for example if
searching for payroll records we might wish to search for the record that matches
a particular id number in this example the id number is the search key
to implement the search function we require that keys be comparable at a
minimum we must be able to take two keys and reliably determine whether they
are equal or not that is enough to enable a sequential search through a database
of records and find one that matches a given key however we typically would
like for the keys to define a total order see section 21 which means that we
can tell which of two keys is greater than the other using key types with total
orderings gives the database implementor the opportunity to organize a collection
of records in a way that makes searching more efficient an example is storing the
records in sorted order in an array which permits a binary search fortunately in
practice most fields of most records consist of simple data types with natural total
orders for example integers floats doubles and character strings all are totally
ordered ordering fields that are naturally multidimensional such as a point in two
or three dimensions present special opportunities if we wish to take advantage of
their multidimensional nature this problem is addressed in section 133
figure 428 shows the definition for a simple abstract dictionary class the
methods insert and find are the heart of the class method insert takes a
record and inserts it into the dictionary method find takes a key value and returns
some record from the dictionary whose key matches the one provided if there are
multiple records in the dictionary with that key value there is no requirement as to
which one is returned
method clear simply reinitializes the dictionary the remove method is
similar to find except that it also deletes the record returned from the dictionary
once again if there are multiple records in the dictionary that match the desired
key there is no requirement as to which one actually is removed and returned
method size returns the number of elements in the dictionary
the remaining method is removeany this is similar to remove except
that it does not take a key value instead it removes an arbitrary record from the
dictionary if one exists the purpose of this method is to allow a user the ability
to iterate over all elements in the dictionary of course the dictionary will become
empty in the process without the removeany method a dictionary user could
not get at a record of the dictionary that he didnt already know the key value for
with the removeany method the user can process all records in the dictionary as
shown in the following code fragment


$$@@$$PAGE: 155
136

chap 4 lists stacks and queues

 the dictionary abstract class
template typename key typename e
class dictionary 
private
void operator const dictionary 
dictionaryconst dictionary 
public
dictionary 
 default constructor
virtual dictionary   base destructor
 reinitialize dictionary
virtual void clear  0
 insert a record
 k the key for the record being inserted
 e the record being inserted
virtual void insertconst key k const e e  0
 remove and return a record
 k the key of the record to be removed
 return a maching record if multiple records match
 k remove an arbitrary one return null if no record
 with key k exists
virtual e removeconst key k  0
 remove and return an arbitrary record from dictionary
 return the record removed or null if none exists
virtual e removeany  0
 return a record matching k null if none exists
 if multiple records match return an arbitrary one
 k the key of the record to find
virtual e findconst key k const  0
 return the number of records in the dictionary
virtual int size  0

figure 428 the adt for a simple dictionary


$$@@$$PAGE: 156
137

sec 44 dictionaries

 a simple payroll entry with id name address fields
class payroll 
private
int id
string name
string address
public
 constructor
payrollint inid string inname string inaddr 
id  inid
name  inname
address  inaddr

payroll 

 destructor

 local data member access functions
int getid  return id 
string getname  return name 
string getaddr  return address 

figure 429 a payroll record implementation
while dictsize  0 
it  dictremoveany
dosomethingit


there are other approaches that might seem more natural for iterating though a
dictionary such as using a first and a next function but not all data structures
that we want to use to implement a dictionary are able to do first efficiently for
example a hash table implementation cannot efficiently locate the record in the
table with the smallest key value by using removeany we have a mechanism
that provides generic access
given a database storing records of a particular type we might want to search
for records in multiple ways for example we might want to store payroll records
in one dictionary that allows us to search by id and also store those same records
in a second dictionary that allows us to search by name
figure 429 shows an implementation for a payroll record class payroll has
multiple fields each of which might be used as a search key simply by varying
the type for the key and using the appropriate field in each record as the key value
we can define a dictionary whose search key is the id field another whose search
key is the name field and a third whose search key is the address field figure 430
shows an example where payroll objects are stored in two separate dictionaries
one using the id field as the key and the other using the name field as the key


$$@@$$PAGE: 157
138

chap 4 lists stacks and queues

int main 
 iddict organizes payroll records by id
ualdictint payroll iddict
 namedict organizes payroll records by name
ualdictstring payroll namedict
payroll foo1 foo2 findfoo1 findfoo2
foo1  new payroll5 joe anytown
foo2  new payroll10 john mytown
iddictinsertfoo1getid foo1
iddictinsertfoo2getid foo2
namedictinsertfoo1getname foo1
namedictinsertfoo2getname foo2
findfoo1  iddictfind5
if findfoo1  null cout  findfoo1
else cout  null 
findfoo2  namedictfindjohn
if findfoo2  null cout  findfoo2
else cout  null 

figure 430 a dictionary search example here payroll records are stored in
two dictionaries one organized by id and the other organized by name both
dictionaries are implemented with an unsorted arraybased list

the fundamental operation for a dictionary is finding a record that matches a
given key this raises the issue of how to extract the key from a record we would
like any given dictionary implementation to support arbitrary record types so we
need some mechanism for extracting keys that is sufficiently general one approach
is to require all record types to support some particular method that returns the key
value for example in java the comparable interface can be used to provide this
effect unfortunately this approach does not work when the same record type is
meant to be stored in multiple dictionaries each keyed by a different field of the
record this is typical in database applications another more general approach
is to supply a class whose job is to extract the key from the record unfortunately
this solution also does not work in all situations because there are record types for
which it is not possible to write a key extraction method2

2
one example of such a situation occurs when we have a collection of records that describe books
in a library one of the fields for such a record might be a list of subject keywords where the typical
record stores a few keywords our dictionary might be implemented as a list of records sorted by
keyword if a book contains three keywords it would appear three times on the list once for each
associated keyword however given the record there is no simple way to determine which keyword
on the keyword list triggered this appearance of the record thus we cannot write a function that
extracts the key from such a record


$$@@$$PAGE: 158
sec 44 dictionaries

139

 container for a keyvalue pair
template typename key typename e
class kvpair 
private
key k
e e
public
 constructors
kvpair 
kvpairkey kval e eval
 k  kval e  eval 
kvpairconst kvpair o  copy constructor
 k  ok e  oe 
void operator const kvpair o  assignment operator
 k  ok e  oe 
 data member access functions
key key  return k 
void setkeykey ink  k  ink 
e value  return e 

figure 431 implementation for a class representing a keyvalue pair

the fundamental issue is that the key value for a record is not an intrinsic property of the records class or of any field within the class the key for a record is
actually a property of the context in which the record is used
a truly general alternative is to explicitly store the key associated with a given
record as a separate field in the dictionary that is each entry in the dictionary
will contain both a record and its associated key such entries are known as keyvalue pairs it is typical that storing the key explicitly duplicates some field in the
record however keys tend to be much smaller than records so this additional
space overhead will not be great a simple class for representing keyvalue pairs
is shown in figure 431 the insert method of the dictionary class supports the
keyvalue pair implementation because it takes two parameters a record and its
associated key for that dictionary
now that we have defined the dictionary adt and settled on the design approach of storing keyvalue pairs for our dictionary entries we are ready to consider
ways to implement it two possibilities would be to use an arraybased or linked
list figure 432 shows an implementation for the dictionary using an unsorted
arraybased list
examining class ualdict ual stands for unsorted arraybased list we can
easily see that insert is a constanttime operation because it simply inserts the
new record at the end of the list however find and remove both require n
time in the average and worst cases because we need to do a sequential search
method remove in particular must touch every record in the list because once the


$$@@$$PAGE: 159
140

chap 4 lists stacks and queues

 dictionary implemented with an unsorted arraybased list
template typename key typename e
class ualdict  public dictionarykey e 
private
alistkvpairkeye  list
public
ualdictint sizedefaultsize
 constructor
 list  new alistkvpairkeye size 
ualdict  delete list 
 destructor
void clear  listclear 
 reinitialize
 insert an element append to list
void insertconst keyk const e e 
kvpairkeye tempk e
listappendtemp

 use sequential search to find the element to remove
e removeconst key k 
e temp  findk  find will set list position
iftemp  null listremove
return temp

e removeany   remove the last element
assertsize  0 dictionary is empty
listmovetoend
listprev
kvpairkeye e  listremove
return evalue

 find k using sequential search
e findconst key k const 
forlistmovetostart
listcurrpos  listlength listnext 
kvpairkeye temp  listgetvalue
if k  tempkey
return tempvalue

return null  k does not appear in dictionary

figure 432 a dictionary implemented with an unsorted arraybased list

int size  return list size
 return listlength 

figure 432 continued


$$@@$$PAGE: 160
141

sec 44 dictionaries

 sorted arraybased list
 inherit from alist as a protected base class
template typename key typename e
class salist protected alistkvpairkeye  
public
salistint sizedefaultsize 
alistkvpairkeye size 
salist 

 destructor

 redefine insert function to keep values sorted
void insertkvpairkeye it   insert at right
kvpairkeye curr
for movetostart currpos  length next 
curr  getvalue
ifcurrkey  itkey
break

alistkvpairkeye insertit  do alist insert

 with the exception of append all remaining methods are
 exposed from alist append is not available to salist
 class users since it has not been explicitly exposed
alistkvpairkeye clear
alistkvpairkeye remove
alistkvpairkeye movetostart
alistkvpairkeye movetoend
alistkvpairkeye prev
alistkvpairkeye next
alistkvpairkeye length
alistkvpairkeye currpos
alistkvpairkeye movetopos
alistkvpairkeye getvalue

figure 433 an implementation for a sorted arraybased list

desired record is found the remaining records must be shifted down in the list to
fill the gap method removeany removes the last record from the list so this is a
constanttime operation
as an alternative we could implement the dictionary using a linked list the
implementation would be quite similar to that shown in figure 432 and the cost
of the functions should be the same asymptotically
another alternative would be to implement the dictionary with a sorted list the
advantage of this approach would be that we might be able to speed up the find
operation by using a binary search to do so first we must define a variation on the
list adt to support sorted listsan implementation for the arraybased sorted
list is shown in figure 433 a sorted list is somewhat different from an unsorted
list in that it cannot permit the user to control where elements get inserted thus


$$@@$$PAGE: 161
142

chap 4 lists stacks and queues

the insert method must be quite different in a sorted list than in an unsorted list
likewise the user cannot be permitted to append elements onto the list for these
reasons a sorted list cannot be implemented with straightforward inheritance from
the list adt
class salist sal stands for sorted arraybased list does inherit from
class alist however it does so using class alist as a protected base class this
means that salist has available for its use any member functions of alist but
those member functions are not necessarily available to the user of salist however many of the alist member functions are useful to the sallist user thus
most of the alist member functions are passed along directly to the salist user
without change for example the line
alistkvpairkeye remove

provides salists clients with access to the remove method of alist however the original insert method from class alist is replaced and the append
method of alist is kept hidden
the dictionary adt can easily be implemented from class salist as shown
in figure 434 method insert for the dictionary simply calls the insert
method of the sorted list method find uses a generalization of the binary search
function originally shown in section 35 the cost for find in a sorted list is
log n for a list of length n this is a great improvement over the cost of find
in an unsorted list unfortunately the cost of insert changes from constant time
in the unsorted list to n time in the sorted list whether the sorted list implementation for the dictionary adt is more or less efficient than the unsorted list
implementation depends on the relative number of insert and find operations
to be performed if many more find operations than insert operations are used
then it might be worth using a sorted list to implement the dictionary in both cases
remove requires n time in the worst and average cases even if we used binary search to cut down on the time to find the record prior to removal we would
still need to shift down the remaining records in the list to fill the gap left by the
remove operation
given two keys we have not properly addressed the issue of how to compare
them one possibility would be to simply use the basic   and  operators
built into c  this is the approach taken by our implementations for dictionaries
shown infigures 432 and 434 if the key type is int for example this will work
fine however if the key is a pointer to a string or any other type of object then
this will not give the desired result when we compare two strings we probably
want to know which comes first in alphabetical order but what we will get from
the standard comparison operators is simply which object appears first in memory
unfortunately the code will compile fine but the answers probably will not be fine
in a language like c that supports operator overloading we could require
that the user of the dictionary overload the   and  operators for the given


$$@@$$PAGE: 162
sec 44 dictionaries

143

 dictionary implemented with a sorted arraybased list
template typename key typename e
class saldict  public dictionarykey e 
private
salistkeye list
public
saldictint sizedefaultsize
 constructor
 list  new salistkeyesize 
saldict  delete list 
 destructor
void clear  listclear 
 reinitialize
 insert an element keep elements sorted
void insertconst keyk const e e 
kvpairkeye tempk e
listinserttemp

 use sequential search to find the element to remove
e removeconst key k 
e temp  findk
if temp  null listremove
return temp

e removeany   remove the last element
assertsize  0 dictionary is empty
listmovetoend
listprev
kvpairkeye e  listremove
return evalue

 find k using binary search
e findconst key k const 
int l  1
int r  listlength
while l1  r   stop when l and r meet
int i  lr2  check middle of remaining subarray
listmovetoposi
kvpairkeye temp  listgetvalue
if k  tempkey r  i
 in left
if k  tempkey return tempvalue  found it
if k  tempkey l  i
 in right

return null  k does not appear in dictionary

figure 434 dictionary implementation using a sorted arraybased list


$$@@$$PAGE: 163
144

chap 4 lists stacks and queues

int size  return list size
 return listlength 

figure 434 continued

key type this requirement then becomes an obligation on the user of the dictionary
class unfortunately this obligation is hidden within the code of the dictionary and
possibly in the users manual rather than exposed in the dictionarys interface as
a result some users of the dictionary might neglect to implement the overloading
with unexpected results again the compiler will not catch this problem
the most general solution is to have users supply their own definition for comparing keys the concept of a class that does comparison called a comparator is
quite important by making these operations be template parameters the requirement to supply the comparator class becomes part of the interface this design is an
example of the strategy design pattern because the strategies for comparing and
getting keys from records are provided by the client in some cases it makes sense
for the comparator class to extract the key from the record type as an alternative to
storing keyvalue pairs
here is an example of the required class for comparing two integers
class intintcompare   comparator class
public
static bool ltint x int y  return x
static bool eqint x int y  return x
static bool gtint x int y  return x


for integer keys
 y 
 y 
 y 

class intintcompare provides methods for determining if two int variables
are equal eq or if the first is less than the second lt or greater than the second
gt
here is a class for comparing two cstyle character strings it makes use of the
standard library function strcmp to do the actual comparison
class cccompare   compare two character strings
public
static bool ltchar x char y
 return strcmpx y  0 
static bool eqchar x char y
 return strcmpx y  0 
static bool gtchar x char y
 return strcmpx y  0 


we will usea comparator in section 55 to implement comparison in heaps and
in chapter 7 to implement comparison in sorting algorithms


$$@@$$PAGE: 164
145

sec 45 further reading

45

further reading

for more discussion on choice of functions used to define the list adt see the
work of the reusable software research group from ohio state their definition
for the list adt can be found in swh93 more information about designing
such classes can be found in sw94

46

exercises

41 assume a list has the following configuration
h  2 23 15 5 9 i
write a series of c statements using the list adt of figure 41 to delete
the element with value 15
42 show the list configuration resulting from each series of list operations using
the list adt of figure 41 assume that lists l1 and l2 are empty at the
beginning of each series show where the current position is in the list
a l1append10
l1append20
l1append15

b l2append10
l2append20
l2append15
l2movetostart
l2insert39
l2next
l2insert12

43 write a series of c statements that uses the list adt of figure 41 to
create a list capable of holding twenty elements and which actually stores the
list with the following configuration
h 2 23  15 5 9 i
44 using the list adt of figure 41 write a function to interchange the current
element and the one following it
45 in the linked list implementation presented in section 412 the current position is implemented using a pointer to the element ahead of the logical
current node the more natural approach might seem to be to have curr
point directly to the node containing the current element however if this
was done then the pointer of the node preceding the current one cannot be


$$@@$$PAGE: 165
146

chap 4 lists stacks and queues

updated properly because there is no access to this node from curr an
alternative is to add a new node after the current element copy the value of
the current element to this new node and then insert the new value into the
old current node
a what happens if curr is at the end of the list already is there still a
way to make this work is the resulting code simpler or more complex
than the implementation of section 412
b will deletion always work in constant time if curr points directly to
the current node in particular can you make several deletions in a
row
46 add to the llist class implementation a member function to reverse the
order of the elements on the list your algorithm should run in n time for
a list of n elements
47 write a function to merge two linked lists the input lists have their elements
in sorted order from lowest to highest the output list should also be sorted
from lowest to highest your algorithm should run in linear time on the length
of the output list
48 a circular linked list is one in which the next field for the last link node
of the list points to the first link node of the list this can be useful when
you wish to have a relative positioning for elements but no concept of an
absolute first or last position
a modify the code of figure 48 to implement circular singly linked lists
b modify the code of figure 414 to implement circular doubly linked
lists
49 section 413 states the space required by the arraybased list implementation is n but can be greater explain why this is so
410 section 413 presents an equation for determining the breakeven point for
the space requirements of two implementations of lists the variables are d
e p  and n what are the dimensional units for each variable show that
both sides of the equation balance in terms of their dimensional units
411 use the space equation of section 413 to determine the breakeven point for
an arraybased list and linked list implementation for lists when the sizes for
the data field a pointer and the arraybased lists array are as specified state
when the linked list needs less space than the array
a the data field is eight bytes a pointer is four bytes and the array holds
twenty elements
b the data field is two bytes a pointer is four bytes and the array holds
thirty elements
c the data field is one byte a pointer is four bytes and the array holds
thirty elements


$$@@$$PAGE: 166
147

sec 46 exercises

412

413
414

415

416
417

d the data field is 32 bytes a pointer is four bytes and the array holds
forty elements
determine the size of an int variable a double variable and a pointer on
your computer the c operator sizeof might be useful here if you do
not already know the answer
a calculate the breakeven point as a function of n beyond which the
arraybased list is more space efficient than the linked list for lists
whose elements are of type int
b calculate the breakeven point as a function of n beyond which the
arraybased list is more space efficient than the linked list for lists
whose elements are of type double
modify the code of figure 418 to implement two stacks sharing the same
array as shown in figure 420
modify the arraybased queue definition of figure 426 to use a separate
boolean member to keep track of whether the queue is empty rather than
require that one array position remain empty
a palindrome is a string that reads the same forwards as backwards using
only a fixed number of stacks and queues the stack and queue adt functions and a fixed number of int and char variables write an algorithm to
determine if a string is a palindrome assume that the string is read from
standard input one character at a time the algorithm should output true or
false as appropriate
reimplement function fibr from exercise 211 using a stack to replace
the recursive call as described in section 424
write a recursive algorithm to compute the value of the recurrence relation
tn  tdn2e  tbn2c  n

t1  1

then rewrite your algorithm to simulate the recursive calls with a stack
418 let q be a nonempty queue and let s be an empty stack using only the
stack and queue adt functions and a single element variable x write an
algorithm to reverse the order of the elements in q
419 a common problem for compilers and text editors is to determine if the
parentheses or other brackets in a string are balanced and properly nested
for example the string  contains properly nested pairs of parentheses but the string  does not and the string  does not contain
properly matching parentheses
a give an algorithm that returns true if a string contains properly nested
and balanced parentheses and false otherwise use a stack to keep
track of the number of left parentheses seen so far hint at no time
while scanning a legal string from left to right will you have encountered more right parentheses than left parentheses


$$@@$$PAGE: 167
148

chap 4 lists stacks and queues

b give an algorithm that returns the position in the string of the first offending parenthesis if the string is not properly nested and balanced
that is if an excess right parenthesis is found return its position if
there are too many left parentheses return the position of the first excess left parenthesis return 1 if the string is properly balanced and
nested use a stack to keep track of the number and positions of left
parentheses seen so far
420 imagine that you are designing an application where you need to perform
the operations insert delete maximum and delete minimum for
this application the cost of inserting is not important because it can be done
offline prior to startup of the timecritical section but the performance of
the two deletion operations are critical repeated deletions of either kind
must work as fast as possible suggest a data structure that can support this
application and justify your suggestion what is the time complexity for
each of the three key operations
421 write a function that reverses the order of an array of n items

47

projects

41 a deque pronounced deck is like a queue except that items may be added
and removed from both the front and the rear write either an arraybased or
linked implementation for the deque
42 one solution to the problem of running out of space for an arraybased list
implementation is to replace the array with a larger array whenever the original array overflows a good rule that leads to an implementation that is both
space and time efficient is to double the current size of the array when there
is an overflow reimplement the arraybased list class of figure 42 to
support this arraydoubling rule
43 use singly linked lists to implement integers of unlimited size each node of
the list should store one digit of the integer you should implement addition
subtraction multiplication and exponentiation operations limit exponents
to be positive integers what is the asymptotic running time for each of your
operations expressed in terms of the number of digits for the two operands
of each function
44 implement doubly linked lists by storing the sum of the next and prev
pointers in a single pointer variable as described in example 41
45 implement a city database using unordered lists each database record contains the name of the city a string of arbitrary length and the coordinates
of the city expressed as integer x and y coordinates your database should
allow records to be inserted deleted by name or coordinate and searched
by name or coordinate another operation that should be supported is to


$$@@$$PAGE: 168
149

sec 47 projects

top  10
a b c

3

h e l

l o

5

0

3

4

7

9 10

1

2

5

6

8

figure 435 an arraybased stack storing variablelength strings each position
stores either one character or the length of the string immediately to the left of it
in the stack

46

47

48

49
410

print all records within a given distance of a specified point implement the
database using an arraybased list implementation and then a linked list implementation collect running time statistics for each operation in both implementations what are your conclusions about the relative advantages and
disadvantages of the two implementations would storing records on the
list in alphabetical order by city name speed any of the operations would
keeping the list in alphabetical order slow any of the operations
modify the code of figure 418 to support storing variablelength strings of
at most 255 characters the stack array should have type char a string is
represented by a series of characters one character per stack element with
the length of the string stored in the stack element immediately above the
string itself as illustrated by figure 435 the push operation would store an
element requiring i storage units in the i positions beginning with the current
value of top and store the size in the position i storage units above top
the value of top would then be reset above the newly inserted element the
pop operation need only look at the size value stored in position top  1 and
then pop off the appropriate number of units you may store the string on the
stack in reverse order if you prefer provided that when it is popped from the
stack it is returned in its proper order
define an adt for a bag see section 21 and create an arraybased implementation for bags be sure that your bag adt does not rely in any way
on knowing or controlling the position of an element then implement the
dictionary adt of figure 428 using your bag implementation
implement the dictionary adt of figure 428 using an unsorted linked list as
defined by class llist in figure 48 make the implementation as efficient
as you can given the restriction that your implementation must use the unsorted linked list and its access operations to implement the dictionary state
the asymptotic time requirements for each function member of the dictionary
adt under your implementation
implement the dictionary adt of figure 428 based on stacks your implementation should declare and use two stacks
implement the dictionary adt of figure 428 based on queues your implementation should declare and use two queues


$$@@$$PAGE: 169

$$@@$$PAGE: 170
5
binary trees

the list representations of chapter 4 have a fundamental limitation either search
or insert can be made efficient but not both at the same time tree structures
permit both efficient access and update to large collections of data binary trees in
particular are widely used and relatively easy to implement but binary trees are
useful for many things besides searching just a few examples of applications that
trees can speed up include prioritizing jobs describing mathematical expressions
and the syntactic elements of computer programs or organizing the information
needed to drive data compression algorithms
this chapter begins by presenting definitions and some key properties of binary trees section 52 discusses how to process all nodes of the binary tree in an
organized manner section 53 presents various methods for implementing binary
trees and their nodes sections 54 through 56 present three examples of binary
trees used in specific applications the binary search tree bst for implementing
dictionaries heaps for implementing priority queues and huffman coding trees for
text compression the bst heap and huffman coding tree each have distinctive
structural features that affect their implementation and use

51

definitions and properties

a binary tree is made up of a finite set of elements called nodes this set either
is empty or consists of a node called the root together with two binary trees called
the left and right subtrees which are disjoint from each other and from the root
disjoint means that they have no nodes in common the roots of these subtrees
are children of the root there is an edge from a node to each of its children and
a node is said to be the parent of its children
if n1  n2   nk is a sequence of nodes in the tree such that ni is the parent of
ni1 for 1  i  k then this sequence is called a path from n1 to nk  the length
of the path is k  1 if there is a path from node r to node m then r is an ancestor
of m and m is a descendant of r thus all nodes in the tree are descendants of the
151


$$@@$$PAGE: 171
152

chap 5 binary trees

a
b

c
d

e
g

f
h

i

figure 51 a binary tree node a is the root nodes b and c are as children
nodes b and d together form a subtree node b has two children its left child
is the empty tree and its right child is d nodes a c and e are ancestors of g
nodes d e and f make up level 2 of the tree node a is at level 0 the edges
from a to c to e to g form a path of length 3 nodes d g h and i are leaves
nodes a b c e and f are internal nodes the depth of i is 3 the height of this
tree is 4

root of the tree while the root is the ancestor of all nodes the depth of a node m
in the tree is the length of the path from the root of the tree to m the height of a
tree is one more than the depth of the deepest node in the tree all nodes of depth d
are at level d in the tree the root is the only node at level 0 and its depth is 0 a
leaf node is any node that has two empty children an internal node is any node
that has at least one nonempty child
figure 51 illustrates the various terms used to identify parts of a binary tree
figure 52 illustrates an important point regarding the structure of binary trees
because all binary tree nodes have two children one or both of which might be
empty the two binary trees of figure 52 are not the same
two restricted forms of binary tree are sufficiently important to warrant special
names each node in a full binary tree is either 1 an internal node with exactly
two nonempty children or 2 a leaf a complete binary tree has a restricted shape
obtained by starting at the root and filling the tree by levels from left to right in the
complete binary tree of height d all levels except possibly level d1 are completely
full the bottom level has its nodes filled in from the left side
figure 53 illustrates the differences between full and complete binary trees1
there is no particular relationship between these two tree shapes that is the tree of
figure 53a is full but not complete while the tree of figure 53b is complete but
1

while these definitions for full and complete binary tree are the ones most commonly used they
are not universal because the common meaning of the words full and complete are quite similar
there is little that you can do to distinguish between them other than to memorize the definitions here
is a memory aid that you might find useful complete is a wider word than full and complete
binary trees tend to be wider than full binary trees because each level of a complete binary tree is as
wide as possible


$$@@$$PAGE: 172
153

sec 51 definitions and properties

a

a
b

b
a

b

a

a
empty

b

empty

c

b
d

figure 52 two different binary trees a a binary tree whose root has a nonempty left child b a binary tree whose root has a nonempty right child c the
binary tree of a with the missing right child made explicit d the binary tree
of b with the missing left child made explicit

a

b

figure 53 examples of full and complete binary trees a this tree is full but
not complete b this tree is complete but not full

not full the heap data structure section 55 is an example of a complete binary
tree the huffman coding tree section 56 is an example of a full binary tree
511

the full binary tree theorem

some binary tree implementations store data only at the leaf nodes using the internal nodes to provide structure to the tree more generally binary tree implementations might require some amount of space for internal nodes and a different amount
for leaf nodes thus to analyze the space required by such implementations it is
useful to know the minimum and maximum fraction of the nodes that are leaves in
a tree containing n internal nodes
unfortunately this fraction is not fixed a binary tree of n internal nodes might
have only one leaf this occurs when the internal nodes are arranged in a chain
ending in a single leaf as shown in figure 54 in this case the number of leaves
is low because each internal node has only one nonempty child to find an upper
bound on the number of leaves for a tree of n internal nodes first note that the upper


$$@@$$PAGE: 173
154

chap 5 binary trees

any number of
internal nodes

figure 54 a tree containing many internal nodes and a single leaf

bound will occur when each internal node has two nonempty children that is
when the tree is full however this observation does not tell what shape of tree will
yield the highest percentage of nonempty leaves it turns out not to matter because
all full binary trees with n internal nodes have the same number of leaves this fact
allows us to compute the space requirements for a full binary tree implementation
whose leaves require a different amount of space from its internal nodes
theorem 51 full binary tree theorem the number of leaves in a nonempty
full binary tree is one more than the number of internal nodes
proof the proof is by mathematical induction on n the number of internal nodes
this is an example of an induction proof where we reduce from an arbitrary instance of size n to an instance of size n  1 that meets the induction hypothesis
 base cases the nonempty tree with zero internal nodes has one leaf node
a full binary tree with one internal node has two leaf nodes thus the base
cases for n  0 and n  1 conform to the theorem
 induction hypothesis assume that any full binary tree t containing n  1
internal nodes has n leaves
 induction step given tree t with n internal nodes select an internal node i
whose children are both leaf nodes remove both of is children making
i a leaf node call the new tree t0  t0 has n  1 internal nodes from
the induction hypothesis t0 has n leaves now restore is two children we
once again have tree t with n internal nodes how many leaves does t have
because t0 has n leaves adding the two children yields n2 however node
i counted as one of the leaves in t0 and has now become an internal node
thus tree t has n  1 leaf nodes and n internal nodes
by mathematical induction the theorem holds for all values of n  0

2

when analyzing the space requirements for a binary tree implementation it is
useful to know how many empty subtrees a tree contains a simple extension of
the full binary tree theorem tells us exactly how many empty subtrees there are
in any binary tree whether full or not here are two approaches to proving the
following theorem and each suggests a useful way of thinking about binary trees


$$@@$$PAGE: 174
sec 52 binary tree traversals

155

theorem 52 the number of empty subtrees in a nonempty binary tree is one
more than the number of nodes in the tree
proof 1 take an arbitrary binary tree t and replace every empty subtree with a
leaf node call the new tree t0  all nodes originally in t will be internal nodes in
t0 because even the leaf nodes of t have children in t0  t0 is a full binary tree
because every internal node of t now must have two children in t0  and each leaf
node in t must have two children in t0 the leaves just added the full binary tree
theorem tells us that the number of leaves in a full binary tree is one more than the
number of internal nodes thus the number of new leaves that were added to create
t0 is one more than the number of nodes in t each leaf node in t0 corresponds to
an empty subtree in t thus the number of empty subtrees in t is one more than
the number of nodes in t
2
proof 2 by definition every node in binary tree t has two children for a total of
2n children in a tree of n nodes every node except the root node has one parent
for a total of n  1 nodes with parents in other words there are n  1 nonempty
children because the total number of children is 2n the remaining n  1 children
must be empty
2
512

a binary tree node adt

just as a linked list is comprised of a collection of link objects a tree is comprised
of a collection of node objects figure 55 shows an adt for binary tree nodes
called binnode this class will be used by some of the binary tree structures
presented later class binnode is a template with parameter e which is the type
for the data record stored in the node member functions are provided that set or
return the element value set or return a pointer to the left child set or return a
pointer to the right child or indicate whether the node is a leaf

52

binary tree traversals

often we wish to process a binary tree by visiting each of its nodes each time
performing a specific action such as printing the contents of the node any process
for visiting all of the nodes in some order is called a traversal any traversal that
lists every node in the tree exactly once is called an enumeration of the trees
nodes some applications do not require that the nodes be visited in any particular
order as long as each node is visited precisely once for other applications nodes
must be visited in an order that preserves some relationship for example we might
wish to make sure that we visit any given node before we visit its children this is
called a preorder traversal


$$@@$$PAGE: 175
156

chap 5 binary trees

 binary tree node abstract class
template typename e class binnode 
public
virtual binnode   base destructor
 return the nodes value
virtual e element  0
 set the nodes value
virtual void setelementconst e  0
 return the nodes left child
virtual binnode left const  0
 set the nodes left child
virtual void setleftbinnode  0
 return the nodes right child
virtual binnode right const  0
 set the nodes right child
virtual void setrightbinnode  0
 return true if the node is a leaf false otherwise
virtual bool isleaf  0

figure 55 a binary tree node adt

example 51 the preorder enumeration for the tree of figure 51 is
abdcegfhi
the first node printed is the root then all nodes of the left subtree are
printed in preorder before any node of the right subtree
alternatively we might wish to visit each node only after we visit its children
and their subtrees for example this would be necessary if we wish to return
all nodes in the tree to free store we would like to delete the children of a node
before deleting the node itself but to do that requires that the childrens children
be deleted first and so on this is called a postorder traversal
example 52 the postorder enumeration for the tree of figure 51 is
dbgehifca
an inorder traversal first visits the left child including its entire subtree then
visits the node and finally visits the right child including its entire subtree the


$$@@$$PAGE: 176
157

sec 52 binary tree traversals

binary search tree of section 54 makes use of this traversal to print all nodes in
ascending order of value
example 53 the inorder enumeration for the tree of figure 51 is
bdagechfi
a traversal routine is naturally written as a recursive function its input parameter is a pointer to a node which we will call root because each node can be
viewed as the root of a some subtree the initial call to the traversal function passes
in a pointer to the root node of the tree the traversal function visits root and
its children if any in the desired order for example a preorder traversal specifies that root be visited before its children this can easily be implemented as
follows
template typename e
void preorderbinnodee root 
if root  null return  empty subtree do nothing
visitroot
 perform desired action
preorderrootleft
preorderrootright


function preorder first checks that the tree is not empty if it is then the traversal
is done and preorder simply returns otherwise preorder makes a call to
visit which processes the root node ie prints the value or performs whatever
computation as required by the application function preorder is then called
recursively on the left subtree which will visit all nodes in that subtree finally
preorder is called on the right subtree visiting all nodes in the right subtree
postorder and inorder traversals are similar they simply change the order in which
the node and its children are visited as appropriate
an important decision in the implementation of any recursive function on trees
is when to check for an empty subtree function preorder first checks to see if
the value for root is null if not it will recursively call itself on the left and right
children of root in other words preorder makes no attempt to avoid calling
itself on an empty child some programmers use an alternate design in which the
left and right pointers of the current node are checked so that the recursive call is
made only on nonempty children such a design typically looks as follows
template typename e
void preorder2binnodee root 
visitroot  perform whatever action is desired
if rootleft  null preorder2rootleft
if rootright  null preorder2rootright



$$@@$$PAGE: 177
158

chap 5 binary trees

at first it might appear that preorder2 is more efficient than preorder
because it makes only half as many recursive calls why on the other hand
preorder2 must access the left and right child pointers twice as often the net
result is little or no performance improvement
in reality the design of preorder2 is inferior to that of preorder for two
reasons first while it is not apparent in this simple example for more complex
traversals it can become awkward to place the check for the null pointer in the
calling code even here we had to write two tests for null rather than the one
needed by preorder the more important concern with preorder2 is that it
tends to be error prone while preorder2 insures that no recursive calls will
be made on empty subtrees it will fail if the initial call passes in a null pointer
this would occur if the original tree is empty to avoid the bug either preorder2
needs an additional test for a null pointer at the beginning making the subsequent
tests redundant after all or the caller of preorder2 has a hidden obligation to
pass in a nonempty tree which is unreliable design the net result is that many
programmers forget to test for the possibility that the empty tree is being traversed
by using the first design which explicitly supports processing of empty subtrees
the problem is avoided
another issue to consider when designing a traversal is how to define the visitor
function that is to be executed on every node one approach is simply to write a
new version of the traversal for each such visitor function as needed the disadvantage to this is that whatever function does the traversal must have access to the
binnode class it is probably better design to permit only the tree class to have
access to the binnode class
another approach is for the tree class to supply a generic traversal function
which takes the visitor either as a template parameter or as a function parameter
this is known as the visitor design pattern a major constraint on this approach is
that the signature for all visitor functions that is their return type and parameters
must be fixed in advance thus the designer of the generic traversal function must
be able to adequately judge what parameters and return type will likely be needed
by potential visitor functions
handling information flow between parts of a program can be a significant
design challenge especially when dealing with recursive functions such as tree
traversals in general we can run into trouble either with passing in the correct
information needed by the function to do its work or with returning information
to the recursive functions caller we will see many examples throughout the book
that illustrate methods for passing information in and out of recursive functions as
they traverse a tree structure here are a few simple examples
first we consider the simple case where a computation requires that we communicate information back up the tree to the end user


$$@@$$PAGE: 178
159

sec 52 binary tree traversals

20
50
40

75

20 to 40

figure 56 to be a binary search tree the left child of the node with value 40
must have a value between 20 and 40

example 54 we wish to count the number of nodes in a binary tree the
key insight is that the total count for any nonempty subtree is one for the
root plus the counts for the left and right subtrees where do left and right
subtree counts come from calls to function count on the subtrees will
compute this for us thus we can implement count as follows
template typename e
int countbinnodee root 
if root  null return 0  nothing to count
return 1  countrootleft
 countrootright


another problem that occurs when recursively processing data collections is
controlling which members of the collection will be visited for example some
tree traversals might in fact visit only some tree nodes while avoiding processing
of others exercise 520 must solve exactly this problem in the context of a binary
search tree it must visit only those children of a given node that might possibly
fall within a given range of values fortunately it requires only a simple local
calculation to determine which children to visit
a more difficult situation is illustrated by the following problem given an
arbitrary binary tree we wish to determine if for every node a are all nodes in as
left subtree less than the value of a and are all nodes in as right subtree greater
than the value of a this happens to be the definition for a binary search tree
described in section 54 unfortunately to make this decision we need to know
some context that is not available just by looking at the nodes parent or children
as shown by figure 56 it is not enough to verify that as left child has a value
less than that of a and that as right child has a greater value nor is it enough to
verify that a has a value consistent with that of its parent in fact we need to know
information about what range of values is legal for a given node that information
might come from any of the nodes ancestors thus relevant range information
must be passed down the tree we can implement this function as follows


$$@@$$PAGE: 179
160

chap 5 binary trees

template typename key typename e
bool checkbstbstnodekeye root key low key high 
if root  null return true  empty subtree
key rootkey  rootkey
if rootkey  low  rootkey  high
return false  out of range
if checkbstkeyerootleft low rootkey
return false  left side failed
return checkbstkeyerootright rootkey high


53

binary tree node implementations

in this section we examine ways to implement binary tree nodes we begin with
some options for pointerbased binary tree node implementations then comes a
discussion on techniques for determining the space requirements for a given implementation the section concludes with an introduction to the arraybased implementation for complete binary trees
531

pointerbased node implementations

by definition all binary tree nodes have two children though one or both children
can be empty binary tree nodes typically contain a value field with the type of
the field depending on the application the most common node implementation
includes a value field and pointers to the two children
figure 57 shows a simple implementation for the binnode abstract class
which we will name bstnode class bstnode includes a data member of type
e which is the second template parameter for the element type to support search
structures such as the binary search tree an additional field is included with corresponding access methods to store a key value whose purpose is explained in
section 44 its type is determined by the first template parameter named key
every bstnode object also has two pointers one to its left child and another to
its right child overloaded new and delete operators could be added to support
a freelist as described in section 412figure 58 illustrates the bstnode implementation
some programmers find it convenient to add a pointer to the nodes parent
allowing easy upward movement in the tree using a parent pointer is somewhat
analogous to adding a link to the previous node in a doubly linked list in practice
the parent pointer is almost always unnecessary and adds to the space overhead for
the tree implementation it is not just a problem that parent pointers take space
more importantly many uses of the parent pointer are driven by improper understanding of recursion and so indicate poor programming if you are inclined toward
using a parent pointer consider if there is a more efficient implementation possible


$$@@$$PAGE: 180
161

sec 53 binary tree node implementations

 simple binary tree node implementation
template typename key typename e
class bstnode  public binnodee 
private
key k
 the nodes key
e it
 the nodes value
bstnode lc
 pointer to left child
bstnode rc
 pointer to right child
public
 two constructors  with and without initial values
bstnode  lc  rc  null 
bstnodekey k e e bstnode l null bstnode r null
 k  k it  e lc  l rc  r 
bstnode 
 destructor
 functions to set and return the value and key
e element  return it 
void setelementconst e e  it  e 
key key  return k 
void setkeyconst key k  k  k 
 functions to set and return the children
inline bstnode left const  return lc 
void setleftbinnodee b  lc  bstnodeb 
inline bstnode right const  return rc 
void setrightbinnodee b  rc  bstnodeb 
 return true if it is a leaf false otherwise
bool isleaf  return lc  null  rc  null 

figure 57 a binary tree node class implementation

a

b

c

d

e

g

f

h

i

figure 58 illustration of a typical pointerbased binary tree implementation
where each node stores two child pointers and a value


$$@@$$PAGE: 181
162

chap 5 binary trees


c





4

x

a


2

x

figure 59 an expression tree for 4x2x  a  c

an important decision in the design of a pointerbased node implementation
is whether the same class definition will be used for leaves and internal nodes
using the same class for both will simplify the implementation but might be an
inefficient use of space some applications require data values only for the leaves
other applications require one type of value for the leaves and another for the internal nodes examples include the binary trie of section 131 the pr quadtree of
section 133 the huffman coding tree of section 56 and the expression tree illustrated by figure 59 by definition only internal nodes have nonempty children
if we use the same node implementation for both internal and leaf nodes then both
must store the child pointers but it seems wasteful to store child pointers in the
leaf nodes thus there are many reasons why it can save space to have separate
implementations for internal and leaf nodes
as an example of a tree that stores different information at the leaf and internal nodes consider the expression tree illustrated by figure 59 the expression
tree represents an algebraic expression composed of binary operators such as addition subtraction multiplication and division internal nodes store operators
while the leaves store operands the tree of figure 59 represents the expression
4x2x  a  c the storage requirements for a leaf in an expression tree are quite
different from those of an internal node internal nodes store one of a small set of
operators so internal nodes could store a small code identifying the operator such
as a single byte for the operators character symbol in contrast leaves store variable names or numbers which is considerably larger in order to handle the wider
range of possible values at the same time leaf nodes need not store child pointers
c allows us to differentiate leaf from internal nodes through the use of class
inheritance a base class provides a general definition for an object and a subclass
modifies a base class to add more detail a base class can be declared for binary tree
nodes in general with subclasses defined for the internal and leaf nodes the base
class of figure 510 is named varbinnode it includes a virtual member function


$$@@$$PAGE: 182
sec 53 binary tree node implementations

163

named isleaf which indicates the node type subclasses for the internal and leaf
node types each implement isleaf internal nodes store child pointers of the base
class type they do not distinguish their childrens actual subclass whenever a node
is examined its version of isleaf indicates the nodes subclass
figure 510 includes two subclasses derived from class varbinnode named
leafnode and intlnode class intlnode can access its children through
pointers of type varbinnode function traverse illustrates the use of these
classes when traverse calls method isleaf c s runtime environment
determines which subclass this particular instance of rt happens to be and calls that
subclasss version of isleaf method isleaf then provides the actual node type
to its caller the other member functions for the derived subclasses are accessed by
typecasting the base class pointer as appropriate as shown in function traverse
there is another approach that we can take to represent separate leaf and internal nodes also using a virtual base class and separate node classes for the two types
this is to implement nodes using the composite design pattern this approach is
noticeably different from the one of figure 510 in that the node classes themselves
implement the functionality of traverse figure 511 shows the implementation here base class varbinnode declares a member function traverse that
each subclass must implement each subclass then implements its own appropriate
behavior for its role in a traversal the whole traversal process is called by invoking
traverse on the root node which in turn invokes traverse on its children
when comparing the implementations of figures 510 and 511 each has advantages and disadvantages the first does not require that the node classes know
about the traverse function with this approach it is easy to add new methods
to the tree class that do other traversals or other operations on nodes of the tree
however we see that traverse in figure 510 does need to be familiar with each
node subclass adding a new node subclass would therefore require modifications
to the traverse function in contrast the approach of figure 511 requires that
any new operation on the tree that requires a traversal also be implemented in the
node subclasses on the other hand the approach of figure 511 avoids the need for
the traverse function to know anything about the distinct abilities of the node
subclasses those subclasses handle the responsibility of performing a traversal on
themselves a secondary benefit is that there is no need for traverse to explicitly enumerate all of the different node subclasses directing appropriate action for
each with only two node classes this is a minor point but if there were many such
subclasses this could become a bigger problem a disadvantage is that the traversal
operation must not be called on a null pointer because there is no object to catch
the call this problem could be avoided by using a flyweight see section 131 to
implement empty nodes
typically the version of figure 510 would be preferred in this example if
traverse is a member function of the tree class and if the node subclasses are


$$@@$$PAGE: 183
164

chap 5 binary trees

 node implementation with simple inheritance
class varbinnode 
 node abstract base class
public
virtual varbinnode 
virtual bool isleaf  0
 subclasses must implement


class leafnode  public varbinnode   leaf node
private
operand var
 operand value
public
leafnodeconst operand val  var  val   constructor
bool isleaf  return true 
 version for leafnode
operand value  return var 
 return node value


class intlnode  public varbinnode   internal node
private
varbinnode left
 left child
varbinnode right
 right child
operator opx
 operator value
public
intlnodeconst operator op varbinnode l varbinnode r
 opx  op left  l right  r   constructor
bool isleaf  return false 
 version for intlnode
varbinnode leftchild  return left 
 left child
varbinnode rightchild  return right   right child
operator value  return opx 
 value


void traversevarbinnode root 
 preorder traversal
if root  null return
 nothing to visit
if rootisleaf
 do leaf node
cout  leaf   leafnode rootvalue  endl
else 
 do internal node
cout  internal 
 intlnode rootvalue  endl
traverseintlnode rootleftchild
traverseintlnode rootrightchild


figure 510 an implementation for separate internal and leaf node representations using c class inheritance and virtual functions


$$@@$$PAGE: 184
165

sec 53 binary tree node implementations

 node implementation with the composite design pattern
class varbinnode 
 node abstract base class
public
virtual varbinnode 
 generic destructor
virtual bool isleaf  0
virtual void traverse  0


class leafnode  public varbinnode   leaf node
private
operand var
 operand value
public
leafnodeconst operand val  var
bool isleaf  return true 
operand value  return var 
void traverse  cout  leaf 


 val   constructor
 isleaf for leafnode
 return node value
 value  endl 

class intlnode  public varbinnode   internal node
private
varbinnode lc
 left child
varbinnode rc
 right child
operator opx
 operator value
public
intlnodeconst operator op varbinnode l varbinnode r
 opx  op lc  l rc  r 
 constructor
bool isleaf  return false 
varbinnode left  return lc 
varbinnode right  return rc 
operator value  return opx 






isleaf for intlnode
left child
right child
value

void traverse   traversal behavior for internal nodes
cout  internal   value  endl
if left  null lefttraverse
if right  null righttraverse



 do a preorder traversal
void traversevarbinnode root 
if root  null roottraverse

figure 511 a second implementation for separate internal and leaf node representations using c class inheritance and virtual functions using the composite
design pattern here the functionality of traverse is embedded into the node
subclasses


$$@@$$PAGE: 185
166

chap 5 binary trees

hidden from users of that tree class on the other hand if the nodes are objects
that have meaning to users of the tree separate from their existence as nodes in the
tree then the version of figure 511 might be preferred because hiding the internal
behavior of the nodes becomes more important
another advantage of the composite design is that implementing each node
types functionality might be easier this is because you can focus solely on the
information passing and other behavior needed by this node type to do its job this
breaks down the complexity that many programmers feel overwhelmed by when
dealing with complex information flows related to recursive processing
532

space requirements

this section presents techniques for calculating the amount of overhead required by
a binary tree implementation recall that overhead is the amount of space necessary
to maintain the data structure in other words it is any space not used to store
data records the amount of overhead depends on several factors including which
nodes store data values all nodes or just the leaves whether the leaves store child
pointers and whether the tree is a full binary tree
in a simple pointerbased implementation for the binary tree such as that of
figure 57 every node has two pointers to its children even when the children are
null this implementation requires total space amounting to n2p  d for a
tree of n nodes here p stands for the amount of space required by a pointer and
d stands for the amount of space required by a data value the total overhead space
will be 2p n for the entire tree thus the overhead fraction will be 2p2p  d
the actual value for this expression depends on the relative size of pointers versus
data fields if we arbitrarily assume that p  d then a full tree has about two
thirds of its total space taken up in overhead worse yet theorem 52 tells us that
about half of the pointers are wasted null values that serve only to indicate tree
structure but which do not provide access to new data
a common implementation is not to store any actual data in a node but rather a
pointer to the data record in this case each node will typically store three pointers
all of which are overhead resulting in an overhead fraction of 3p3p  d
if only leaves store data values then the fraction of total space devoted to overhead depends on whether the tree is full if the tree is not full then conceivably
there might only be one leaf node at the end of a series of internal nodes thus
the overhead can be an arbitrarily high percentage for nonfull binary trees the
overhead fraction drops as the tree becomes closer to full being lowest when the
tree is truly full in this case about one half of the nodes are internal
great savings can be had by eliminating the pointers from leaf nodes in full
binary trees again assume the tree stores a pointer to the data field because about
half of the nodes are leaves and half internal nodes and because only internal nodes


$$@@$$PAGE: 186
167

sec 53 binary tree node implementations

now have child pointers the overhead fraction in this case will be approximately
n
2 2p 
n
2 2p 

 dn



p

p d

if p  d the overhead drops to about one half of the total space however if only
leaf nodes store useful information the overhead fraction for this implementation is
actually three quarters of the total space because half of the data space is unused
if a full binary tree needs to store data only at the leaf nodes a better implementation would have the internal nodes store two pointers and no data field while
the leaf nodes store only a pointer to the data field this implementation requires
n
n
2 2p  2 pd units of space if p  d then the overhead is 3p3p d  34
it might seem counterintuitive that the overhead ratio has gone up while the total
amount of space has gone down the reason is because we have changed our definition of data to refer only to what is stored in the leaf nodes so while the overhead
fraction is higher it is from a total storage requirement that is lower
there is one serious flaw with this analysis when using separate implementations for internal and leaf nodes there must be a way to distinguish between
the node types when separate node types are implemented via c subclasses
the runtime environment stores information with each object allowing it to determine for example the correct subclass to use when the isleaf virtual function is
called thus each node requires additional space only one bit is truly necessary
to distinguish the two possibilities in rare applications where space is a critical
resource implementors can often find a spare bit within the nodes value field in
which to store the node type indicator an alternative is to use a spare bit within
a node pointer to indicate node type for example this is often possible when the
compiler requires that structures and objects start on word boundaries leaving the
last bit of a pointer value always zero thus this bit can be used to store the nodetype flag and is reset to zero before the pointer is dereferenced another alternative
when the leaf value field is smaller than a pointer is to replace the pointer to a leaf
with that leafs value when space is limited such techniques can make the difference between success and failure in any other situation such bit packing tricks
should be avoided because they are difficult to debug and understand at best and
are often machine dependent at worst2

2

in the early to mid 1980s i worked on a geographic information system that stored spatial data
in quadtrees see section 133 at the time space was a critical resource so we used a bitpacking
approach where we stored the nodetype flag as the last bit in the parent nodes pointer this worked
perfectly on various 32bit workstations unfortunately in those days ibm pccompatibles used
16bit pointers we never did figure out how to port our code to the 16bit machine


$$@@$$PAGE: 187
168

chap 5 binary trees

533

array implementation for complete binary trees

the previous section points out that a large fraction of the space in a typical binary
tree node implementation is devoted to structural overhead not to storing data
this section presents a simple compact implementation for complete binary trees
recall that complete binary trees have all levels except the bottom filled out completely and the bottom level has all of its nodes filled in from left to right thus
a complete binary tree of n nodes has only one possible shape you might think
that a complete binary tree is such an unusual occurrence that there is no reason
to develop a special implementation for it however the complete binary tree has
practical uses the most important being the heap data structure discussed in section 55 heaps are often used to implement priority queues section 55 and for
external sorting algorithms section 852
we begin by assigning numbers to the node positions in the complete binary
tree level by level from left to right as shown in figure 512a an array can
store the trees data values efficiently placing each data value in the array position
corresponding to that nodes position within the tree figure 512b lists the array
indices for the children parent and siblings of each node in figure 512a from
figure 512b you should see a pattern regarding the positions of a nodes relatives
within the array simple formulas can be derived for calculating the array index for
each relative of a node r from rs index no explicit pointers are necessary to
reach a nodes left or right child this means there is no overhead to the array
implementation if the array is selected to be of size n for a tree of n nodes
the formulae for calculating the array indices of the various relatives of a node
are as follows the total number of nodes in the tree is n the index of the node in
question is r which must fall in the range 0 to n  1






54

parentr  br  12c if r 6 0
left childr  2r  1 if 2r  1  n
right childr  2r  2 if 2r  2  n
left siblingr  r  1 if r is even
right siblingr  r  1 if r is odd and r  1  n

binary search trees

section 44 presented the dictionary adt along with dictionary implementations
based on sorted and unsorted lists when implementing the dictionary with an
unsorted list inserting a new record into the dictionary can be performed quickly by
putting it at the end of the list however searching an unsorted list for a particular
record requires n time in the average case for a large database this is probably
much too slow alternatively the records can be stored in a sorted list if the list
is implemented using a linked list then no speedup to the search operation will


$$@@$$PAGE: 188
169

sec 54 binary search trees

0

1

2

3

4

7

8

9

5

10

6

11

a

position
parent
left child
right child
left sibling
right sibling

0

1
2



1
0
3
4

2

2
0
5
6
1


3
1
7
8

4

4
1
9
10
3


5
2
11


6

6
2


5


7
3



8

8
3


7


9
4



10

10
4


9


11
5





b

figure 512 a complete binary tree and its array implementation a the complete binary tree with twelve nodes each node has been labeled with its position
in the tree b the positions for the relatives of each node a dash indicates that
the relative does not exist

result from storing the records in sorted order on the other hand if we use a sorted
arraybased list to implement the dictionary then binary search can be used to find
a record in only log n time however insertion will now require n time on
average because once the proper location for the new record in the sorted list has
been found many records might be shifted to make room for the new record
is there some way to organize a collection of records so that inserting records
and searching for records can both be done quickly this section presents the
binary search tree bst which allows an improved solution to this problem
a bst is a binary tree that conforms to the following condition known as
the binary search tree property all nodes stored in the left subtree of a node
whose key value is k have key values less than k all nodes stored in the right
subtree of a node whose key value is k have key values greater than or equal to k
figure 513 shows two bsts for a collection of values one consequence of the
binary search tree property is that if the bst nodes are printed using an inorder
traversal see section 52 the resulting enumeration will be in sorted order from
lowest to highest
figure 514 shows a class declaration for the bst that implements the dictionary adt the public member functions include those required by the dictionary


$$@@$$PAGE: 189
170

chap 5 binary trees

120
37

42

24
7

42
40

32

7
42
120

2

2

42
32

24

37
40

a

b

figure 513 two binary search trees for a collection of values tree a results
if values are inserted in the order 37 24 42 7 2 40 42 32 120 tree b results
if the same values are inserted in the order 120 42 42 7 2 32 37 24 40

adt along with a constructor and destructor recall from the discussion in section 44 that there are various ways to deal with keys and comparing records three
approaches being keyvalue pairs a special comparison method and passing in a
comparator function our bst implementation will handle comparison by explicitly storing a key separate from the data value at each node of the tree
to find a record with key value k in a bst begin at the root if the root stores
a record with key value k then the search is over if not then we must search
deeper in the tree what makes the bst efficient during search is that we need
search only one of the nodes two subtrees if k is less than the root nodes key
value we search only the left subtree if k is greater than the root nodes key
value we search only the right subtree this process continues until a record with
key value k is found or we reach a leaf node if we reach a leaf node without
encountering k then no record exists in the bst whose key value is k
example 55 consider searching for the node with key value 32 in the
tree of figure 513a because 32 is less than the root value of 37 the
search proceeds to the left subtree because 32 is greater than 24 we search
in 24s right subtree at this point the node containing 32 is found if
the search value were 35 the same path would be followed to the node
containing 32 because this node has no children we know that 35 is not
in the bst
notice that in figure 514 public member function find calls private member
function findhelp method find takes the search key as an explicit parameter
and its bst as an implicit parameter and returns the record that matches the key


$$@@$$PAGE: 190
171

sec 54 binary search trees

 binary search tree implementation for the dictionary adt
template typename key typename e
class bst  public dictionarykeye 
private
bstnodekeye root
 root of the bst
int nodecount
 number of nodes in the bst
 private helper functions
void clearhelpbstnodekey e
bstnodekeye inserthelpbstnodekey e
const key const e
bstnodekeye deleteminbstnodekey e
bstnodekeye getminbstnodekey e
bstnodekeye removehelpbstnodekey e const key
e findhelpbstnodekey e const key const
void printhelpbstnodekey e int const
public
bst  root  null nodecount  0 
bst  clearhelproot 

 constructor
 destructor

void clear
 reinitialize tree
 clearhelproot root  null nodecount  0 
 insert a record into the tree
 k key value of the record
 e the record to insert
void insertconst key k const e e 
root  inserthelproot k e
nodecount

 remove a record from the tree
 k key value of record to remove
 return the record removed or null if there is none
e removeconst key k 
e temp  findhelproot k
 first find it
if temp  null 
root  removehelproot k
nodecount

return temp

figure 514 the binary search tree implementation


$$@@$$PAGE: 191
172

chap 5 binary trees

 remove and return the root node from the dictionary
 return the record removed null if tree is empty
e removeany   delete min value
if root  null 
e temp  rootelement
root  removehelproot rootkey
nodecount
return temp

else return null

 return record with key value k null if none exist
 k the key value to find 
 return some record matching k
 return true if such exists false otherwise if
 multiple records match k return an arbitrary one
e findconst key k const  return findhelproot k 
 return the number of records in the dictionary
int size  return nodecount 
void print const   print the contents of the bst
if root  null cout  the bst is emptyn
else printhelproot 0


figure 514 continued

however the find operation is most easily implemented as a recursive function
whose parameters are the root of a subtree and the search key member findhelp
has the desired form for this recursive subroutine and is implemented as follows
template typename key typename e
e bstkey efindhelpbstnodekey e root
const key k const 
if root  null return null
 empty tree
if k  rootkey
return findhelprootleft k
 check left
else if k  rootkey
return findhelprootright k  check right
else return rootelement  found it


once the desired record is found it is passed through return values up the chain of
recursive calls if a suitable record is not found null is returned
inserting a record with key value k requires that we first find where that record
would have been if it were in the tree this takes us to either a leaf node or to an


$$@@$$PAGE: 192
173

sec 54 binary search trees

37
24
7
2

42
40

32
35

42
120

figure 515 an example of bst insertion a record with value 35 is inserted
into the bst of figure 513a the node with value 32 becomes the parent of the
new node containing 35

internal node with no child in the appropriate direction3 call this node r 0  we then
add a new node containing the new record as a child of r 0  figure 515 illustrates
this operation the value 35 is added as the right child of the node with value 32
here is the implementation for inserthelp
template typename key typename e
bstnodekey e bstkey einserthelp
bstnodekey e root const key k const e it 
if root  null  empty tree create node
return new bstnodekey ek it null null
if k  rootkey
rootsetleftinserthelprootleft k it
else rootsetrightinserthelprootright k it
return root
 return tree with node inserted


you should pay careful attention to the implementation for inserthelp
note that inserthelp returns a pointer to a bstnode what is being returned
is a subtree identical to the old subtree except that it has been modified to contain
the new record being inserted each node along a path from the root to the parent
of the new node added to the tree will have its appropriate child pointer assigned
to it except for the last node in the path none of these nodes will actually change
their childs pointer value in that sense many of the assignments seem redundant
however the cost of these additional assignments is worth paying to keep the insertion process simple the alternative is to check if a given assignment is necessary
which is probably more expensive than the assignment
the shape of a bst depends on the order in which elements are inserted a new
element is added to the bst as a new leaf node potentially increasing the depth of
the tree figure 513 illustrates two bsts for a collection of values it is possible
3
this assumes that no node has a key value equal to the one being inserted if we find a node that
duplicates the key value to be inserted we have two options if the application does not allow nodes
with equal keys then this insertion should be treated as an error or ignored if duplicate keys are
allowed our convention will be to insert the duplicate in the right subtree


$$@@$$PAGE: 193
174

chap 5 binary trees

for the bst containing n nodes to be a chain of nodes with height n this would
happen if for example all elements were inserted in sorted order in general it is
preferable for a bst to be as shallow as possible this keeps the average cost of a
bst operation low
removing a node from a bst is a bit trickier than inserting a node but it is not
complicated if all of the possible cases are considered individually before tackling
the general node removal process let us first discuss how to remove from a given
subtree the node with the smallest key value this routine will be used later by the
general node removal function to remove the node with the minimum key value
from a subtree first find that node by continuously moving down the left link until
there is no further left link to follow call this node s to remove s simply have
the parent of s change its pointer to point to the right child of s we know that s
has no left child because if s did have a left child s would not be the node with
minimum key value thus changing the pointer as described will maintain a bst
with s removed the code for this method named deletemin is as follows
template typename key typename e
bstnodekey e bstkey e
deleteminbstnodekey e rt 
if rtleft  null  found min
return rtright
else 
 continue left
rtsetleftdeleteminrtleft
return rt



example 56 figure 516 illustrates the deletemin process beginning
at the root node with value 10 deletemin follows the left link until there
is no further left link in this case reaching the node with value 5 the node
with value 10 is changed to point to the right child of the node containing
the minimum value this is indicated in figure 516 by a dashed line
a pointer to the node containing the minimumvalued element is stored in parameter s the return value of the deletemin method is the subtree of the current node with the minimumvalued node in the subtree removed as with method
inserthelp each node on the path back to the root has its left child pointer
reassigned to the subtree resulting from its call to the deletemin method
a useful companion method is getmin which returns a pointer to the node
containing the minimum value in the subtree


$$@@$$PAGE: 194
175

sec 54 binary search trees

subroot
10
5

20
9

5
figure 516 an example of deleting the node with minimum value in this tree
the node with minimum value 5 is the left child of the root thus the roots
left pointer is changed to point to 5s right child
template typename key typename e
bstnodekey e bstkey e
getminbstnodekey e rt 
if rtleft  null
return rt
else return getminrtleft


removing a node with given key value r from the bst requires that we first
find r and then remove it from the tree so the first part of the remove operation
is a search to find r once r is found there are several possibilities if r has no
children then rs parent has its pointer set to null if r has one child then rs
parent has its pointer set to rs child similar to deletemin the problem comes
if r has two children one simple approach though expensive is to set rs parent
to point to one of rs subtrees and then reinsert the remaining subtrees nodes one
at a time a better alternative is to find a value in one of the subtrees that can
replace the value in r
thus the question becomes which value can substitute for the one being removed it cannot be any arbitrary value because we must preserve the bst property without making major changes to the structure of the tree which value is
most like the one being removed the answer is the least key value greater than
or equal to the one being removed or else the greatest key value less than the one
being removed if either of these values replace the one being removed then the
bst property is maintained
example 57 assume that we wish to remove the value 37 from the bst
of figure 513a instead of removing the root node we remove the node
with the least value in the right subtree using the deletemin operation
this value can then replace the value in the root in this example we first
remove the node with value 40 because it contains the least value in the


$$@@$$PAGE: 195
176

chap 5 binary trees

37 40
24
7
2

42
32

40

42
120

figure 517 an example of removing the value 37 from the bst the node
containing this value has two children we replace value 37 with the least value
from the nodes right subtree in this case 40

right subtree we then substitute 40 as the new value for the root node
figure 517 illustrates this process
when duplicate node values do not appear in the tree it makes no difference
whether the replacement is the greatest value from the left subtree or the least value
from the right subtree if duplicates are stored then we must select the replacement
from the right subtree to see why call the greatest value in the left subtree g
if multiple nodes in the left subtree have value g selecting g as the replacement
value for the root of the subtree will result in a tree with equal values to the left of
the node now containing g precisely this situation occurs if we replace value 120
with the greatest value in the left subtree of figure 513b selecting the least value
from the right subtree does not have a similar problem because it does not violate
the binary search tree property if equal values appear in the right subtree
from the above we see that if we want to remove the record stored in a node
with two children then we simply call deletemin on the nodes right subtree
and substitute the record returned for the record being removed figure 518 shows
an implementation for removehelp
the cost for findhelp and inserthelp is the depth of the node found or
inserted the cost for removehelp is the depth of the node being removed or
in the case when this node has two children the depth of the node with smallest
value in its right subtree thus in the worst case the cost for any one of these
operations is the depth of the deepest node in the tree this is why it is desirable to
keep bsts balanced that is with least possible height if a binary tree is balanced
then the height for a tree of n nodes is approximately log n however if the tree
is completely unbalanced for example in the shape of a linked list then the height
for a tree with n nodes can be as great as n thus a balanced bst will in the
average case have operations costing log n while a badly unbalanced bst can
have operations in the worst case costing n consider the situation where we
construct a bst of n nodes by inserting records one at a time if we are fortunate
to have them arrive in an order that results in a balanced tree a random order is


$$@@$$PAGE: 196
sec 54 binary search trees

177

 remove a node with key value k
 return the tree with the node removed
template typename key typename e
bstnodekey e bstkey e
removehelpbstnodekey e rt const key k 
if rt  null return null
 k is not in tree
else if k  rtkey
rtsetleftremovehelprtleft k
else if k  rtkey
rtsetrightremovehelprtright k
else 
 found remove it
bstnodekey e temp  rt
if rtleft  null 
 only a right child
rt  rtright
 so point to right
delete temp

else if rtright  null   only a left child
rt  rtleft
 so point to left
delete temp

else 
 both children are nonempty
bstnodekey e temp  getminrtright
rtsetelementtempelement
rtsetkeytempkey
rtsetrightdeleteminrtright
delete temp


return rt

figure 518 implementation for the bst removehelp method

likely to be good enough for this purpose then each insertion will cost on average
log n for a total cost of n log n however if the records are inserted in
order of increasing value then the resulting
tree will be a chain of height n the
p
cost of insertion in this case will be ni1 i  n2 
traversing a bst costs n regardless of the shape of the tree each node is
visited exactly once and each child pointer is followed exactly once
below are two example traversals the first is member clearhelp which
returns the nodes of the bst to the freelist because the children of a node must be
freed before the node itself this is a postorder traversal
template typename key typename e
void bstkey e
clearhelpbstnodekey e root 
if root  null return
clearhelprootleft
clearhelprootright
delete root



$$@@$$PAGE: 197
178

chap 5 binary trees

the next example is printhelp which performs an inorder traversal on the
bst to print the node values in ascending order note that printhelp indents
each line to indicate the depth of the corresponding node in the tree thus we pass
in the current level of the tree in level and increment this value each time that
we make a recursive call
template typename key typename e
void bstkey e
printhelpbstnodekey e root int level const 
if root  null return
 empty tree
printhelprootleft level1
 do left subtree
for int i0 ilevel i
 indent to level
cout   
cout  rootkey  n
 print node value
printhelprootright level1  do right subtree


while the bst is simple to implement and efficient when the tree is balanced
the possibility of its being unbalanced is a serious liability there are techniques
for organizing a bst to guarantee good performance two examples are the avl
tree and the splay tree of section 132 other search trees are guaranteed to remain
balanced such as the 23 tree of section 104

55

heaps and priority queues

there are many situations both in real life and in computing applications where
we wish to choose the next most important from a collection of people tasks
or objects for example doctors in a hospital emergency room often choose to
see next the most critical patient rather than the one who arrived first when
scheduling programs for execution in a multitasking operating system at any given
moment there might be several programs usually called jobs ready to run the
next job selected is the one with the highest priority priority is indicated by a
particular value associated with the job and might change while the job remains in
the wait list
when a collection of objects is organized by importance or priority we call
this a priority queue a normal queue data structure will not implement a priority queue efficiently because search for the element with highest priority will take
n time a list whether sorted or not will also require n time for either insertion or removal a bst that organizes records by priority could be used with the
total of n inserts and n remove operations requiring n log n time in the average
case however there is always the possibility that the bst will become unbalanced leading to bad performance instead we would like to find a data structure
that is guaranteed to have good performance for this special application


$$@@$$PAGE: 198
sec 55 heaps and priority queues

179

this section presents the heap4 data structure a heap is defined by two properties first it is a complete binary tree so heaps are nearly always implemented
using the array representation for complete binary trees presented in section 533
second the values stored in a heap are partially ordered this means that there is
a relationship between the value stored at any node and the values of its children
there are two variants of the heap depending on the definition of this relationship
a maxheap has the property that every node stores a value that is greater than
or equal to the value of either of its children because the root has a value greater
than or equal to its children which in turn have values greater than or equal to their
children the root stores the maximum of all values in the tree
a minheap has the property that every node stores a value that is less than
or equal to that of its children because the root has a value less than or equal to
its children which in turn have values less than or equal to their children the root
stores the minimum of all values in the tree
note that there is no necessary relationship between the value of a node and that
of its sibling in either the minheap or the maxheap for example it is possible that
the values for all nodes in the left subtree of the root are greater than the values for
every node of the right subtree we can contrast bsts and heaps by the strength of
their ordering relationships a bst defines a total order on its nodes in that given
the positions for any two nodes in the tree the one to the left equivalently the
one appearing earlier in an inorder traversal has a smaller key value than the one
to the right in contrast a heap implements a partial order given their positions
we can determine the relative order for the key values of two nodes in the heap only
if one is a descendant of the other
minheaps and maxheaps both have their uses for example the heapsort
of section 76 uses the maxheap while the replacement selection algorithm of
section 852 uses a minheap the examples in the rest of this section will use a
maxheap
be careful not to confuse the logical representation of a heap with its physical
implementation by means of the arraybased complete binary tree the two are not
synonymous because the logical view of the heap is actually a tree structure while
the typical physical implementation uses an array
figure 519 shows an implementation for heapsthe class is a template with two
parameters e defines the type for the data elements stored in the heap while comp
is the comparison class for comparing two elements this class can implement either a minheap or a maxheap by changing the definition for comp comp defines
method prior a binary boolean function that returns true if the first parameter
should come before the second in the heap
this class definition makes two concessions to the fact that an arraybased implementation is used first heap nodes are indicated by their logical position within
4

the term heap is also sometimes used to refer to a memory pool see section 123


$$@@$$PAGE: 199
180

chap 5 binary trees

 heap class
template typename e typename comp class heap 
private
e heap
 pointer to the heap array
int maxsize
 maximum size of the heap
int n
 number of elements now in the heap
 helper function to put element in its correct place
void siftdownint pos 
while isleafpos   stop if pos is a leaf
int j  leftchildpos int rc  rightchildpos
if rc  n  comppriorheaprc heapj
j  rc
 set j to greater childs value
if comppriorheappos heapj return  done
swapheap pos j
pos  j
 move down


public
heape h int num int max
 constructor
 heap  h n  num maxsize  max buildheap 
int size const
 return current heap size
 return n 
bool isleafint pos const  true if pos is a leaf
 return pos  n2  pos  n 
int leftchildint pos const
 return 2pos  1 
 return leftchild position
int rightchildint pos const
 return 2pos  2 
 return rightchild position
int parentint pos const  return parent position
 return pos12 
void buildheap
 heapify contents of heap
 for int in21 i0 i siftdowni 
 insert it into the heap
void insertconst e it 
assertn  maxsize heap is full
int curr  n
heapcurr  it
 start at end of heap
 now sift up until currs parent  curr
while curr0 
comppriorheapcurr heapparentcurr 
swapheap curr parentcurr
curr  parentcurr


figure 519 an implementation for the heap


$$@@$$PAGE: 200
sec 55 heaps and priority queues

181

 remove first value
e removefirst 
assert n  0 heap is empty
swapheap 0 n
 swap first with last value
if n  0 siftdown0  siftdown new root val
return heapn
 return deleted value

 remove and return element at specified position
e removeint pos 
assertpos  0  pos  n bad position
if pos  n1 n  last element no work to do
else

swapheap pos n
 swap with last value
while pos  0 
comppriorheappos heapparentpos 
swapheap pos parentpos  push up large key
pos  parentpos

if n  0 siftdownpos
 push down small key

return heapn


figure 519 continued

the heap rather than by a pointer to the node in practice the logical heap position
corresponds to the identically numbered physical position in the array second the
constructor takes as input a pointer to the array to be used this approach provides
the greatest flexibility for using the heap because all data values can be loaded into
the array directly by the client the advantage of this comes during the heap construction phase as explained below the constructor also takes an integer parameter indicating the initial size of the heap based on the number of elements initially
loaded into the array and a second integer parameter indicating the maximum size
allowed for the heap the size of the array
method heapsize returns the current size of the heap hisleafpos
returns true if position pos is a leaf in heap h and false otherwise members
leftchild rightchild and parent return the position actually the array
index for the left child right child and parent of the position passed respectively
one way to build a heap is to insert the elements one at a time method insert
will insert a new element v into the heap you might expect the heap insertion process to be similar to the insert function for a bst starting at the root and working
down through the heap however this approach is not likely to work because the
heap must maintain the shape of a complete binary tree equivalently if the heap
takes up the first n positions of its array prior to the call to insert it must take


$$@@$$PAGE: 201
182

chap 5 binary trees

up the first n  1 positions after to accomplish this insert first places v at position n of the array of course v is unlikely to be in the correct position to move
v to the right place it is compared to its parents value if the value of v is less
than or equal to the value of its parent then it is in the correct place and the insert
routine is finished if the value of v is greater than that of its parent then the two
elements swap positions from here the process of comparing v to its current
parent continues until v reaches its correct position
since the heap is a complete binary tree its height is guaranteed to be the
minimum possible in particular a heap containing n nodes will have a height of
log n intuitively we can see that this must be true because each level that we
add will slightly more than double the number of nodes in the tree the ith level has
2i nodes and the sum of the first i levels is 2i1  1 starting at 1 we can double
only log n times to reach a value of n to be precise the height of a heap with n
nodes is dlogn  1e
each call to insert takes log n time in the worst case because the value
being inserted can move at most the distance from the bottom of the tree to the top
of the tree thus to insert n values into the heap if we insert them one at a time
will take n log n time in the worst case
if all n values are available at the beginning of the building process we can
build the heap faster than just inserting the values into the heap one by one consider figure 520a which shows one series of exchanges that could be used to
build the heap all exchanges are between a node and one of its children the heap
is formed as a result of this exchange process the array for the righthand tree of
figure 520a would appear as follows
7

4

6

1

2

3

5

figure 520b shows an alternate series of exchanges that also forms a heap
but much more efficiently the equivalent array representation would be
7

5

6

4

2

1

3

from this example it is clear that the heap for any given set of numbers is not
unique and we see that some rearrangements of the input values require fewer exchanges than others to build the heap so how do we pick the best rearrangement
one good algorithm stems from induction suppose that the left and right subtrees of the root are already heaps and r is the name of the element at the root
this situation is illustrated by figure 521 in this case there are two possibilities
1 r has a value greater than or equal to its two children in this case construction
is complete 2 r has a value less than one or both of its children in this case
r should be exchanged with the child that has greater value the result will be a
heap except that r might still be less than one or both of its new children in
this case we simply continue the process of pushing down r until it reaches a


$$@@$$PAGE: 202
183

sec 55 heaps and priority queues

1

7

2
4

3
5

6

4
7

1

6
2

3

5

a

1

7

2

4

3

5

6

5

7

4

6

2

1

3

b

figure 520 two series of exchanges to build a maxheap a this heap is built
by a series of nine exchanges in the order 42 41 21 52 54 63
65 75 76 b this heap is built by a series of four exchanges in the order
52 73 71 61

r

h1

h2

figure 521 final stage in the heapbuilding algorithm both subtrees of node r
are heaps all that remains is to push r down to its proper level in the heap

level where it is greater than its children or is a leaf node this process is implemented by the private method siftdown the siftdown operation is illustrated by
figure 522
this approach assumes that the subtrees are already heaps suggesting that a
complete algorithm can be obtained by visiting the nodes in some order such that
the children of a node are visited before the node itself one simple way to do this
is simply to work from the high index of the array to the low index actually the
build process need not visit the leaf nodes they can never move down because they
are already at the bottom so the building algorithm can start in the middle of the
array with the first internal node the exchanges shown in figure 520b result
from this process method buildheap implements the building algorithm


$$@@$$PAGE: 203
184

chap 5 binary trees

1

7

5

4

7

2

6

7

5

3

4

a

1

2

6

5

3

b

4

6

2

1

3

c

figure 522 the siftdown operation the subtrees of the root are assumed to
be heaps a the partially completed heap b values 1 and 7 are swapped
c values 1 and 6 are swapped to form the final heap

what is the cost of buildheap clearly it is the sum of the costs for the calls
to siftdown each siftdown operation can cost at most the number of levels it
takes for the node being sifted to reach the bottom of the tree in any complete tree
approximately half of the nodes are leaves and so cannot be moved downward at
all one quarter of the nodes are one level above the leaves and so their elements
can move down at most one level at each step up the tree we get half the number of
nodes as were at the previous level and an additional height of one the maximum
sum of total distances that elements can go is therefore
log
xn
i1

log n
n
n x i1
i  1 i 

2
2
2i1
i1

from equation 29 we know that this summation has a closedform solution of
approximately 2 so this algorithm takes n time in the worst case this is far
better than building the heap one element at a time which would cost n log n
in the worst case it is also faster than the n log n averagecase time and n2 
worstcase time required to build the bst
removing the maximum root value from a heap containing n elements requires that we maintain the complete binary tree shape and that the remaining
n  1 node values conform to the heap property we can maintain the proper shape
by moving the element in the last position in the heap the current last element in
the array to the root position we now consider the heap to be one element smaller
unfortunately the new root value is probably not the maximum value in the new
heap this problem is easily solved by using siftdown to reorder the heap because the heap is log n levels deep the cost of deleting the maximum element is
log n in the average and worst cases
the heap is a natural implementation for the priority queue discussed at the
beginning of this section jobs can be added to the heap using their priority value
as the ordering key when needed method removemax can be called whenever a
new job is to be executed


$$@@$$PAGE: 204
sec 56 huffman coding trees

185

some applications of priority queues require the ability to change the priority of
an object already stored in the queue this might require that the objects position
in the heap representation be updated unfortunately a maxheap is not efficient
when searching for an arbitrary value it is only good for finding the maximum
value however if we already know the index for an object within the heap it is
a simple matter to update its priority including changing its position to maintain
the heap property or remove it the remove method takes as input the position
of the node to be removed from the heap a typical implementation for priority
queues requiring updating of priorities will need to use an auxiliary data structure
that supports efficient search for objects such as a bst records in the auxiliary
data structure will store the objects heap index so that the object can be deleted
from the heap and reinserted with its new priority see project 55 sections 1141
and 1151 present applications for a priority queue with priority updating

56

huffman coding trees

the spacetime tradeoff principle from section 39 states that one can often gain
an improvement in space requirements in exchange for a penalty in running time
there are many situations where this is a desirable tradeoff a typical example is
storing files on disk if the files are not actively used the owner might wish to
compress them to save space later they can be uncompressed for use which costs
some time but only once
we often represent a set of items in a computer program by assigning a unique
code to each item for example the standard ascii coding scheme assigns a
unique eightbit value to each character it takes a certain minimum number of
bits to provide unique codes for each character for example it takes dlog 128e or
seven bits to provide the 128 unique codes needed to represent the 128 symbols of
the ascii character set5
the requirement for dlog ne bits to represent n unique code values assumes that
all codes will be the same length as are ascii codes this is called a fixedlength
coding scheme if all characters were used equally often then a fixedlength coding
scheme is the most space efficient method however you are probably aware that
not all characters are used equally often in many applications for example the
various letters in an english language document have greatly different frequencies
of use
figure 523 shows the relative frequencies of the letters of the alphabet from
this table we can see that the letter e appears about 60 times more often than the
letter z in normal ascii the words deed and muck require the same
5

the ascii standard is eight bits not seven even though there are only 128 characters represented the eighth bit is used either to check for transmission errors or to support extended ascii
codes with an additional 128 characters


$$@@$$PAGE: 205
186

chap 5 binary trees

letter
a
b
c
d
e
f
g
h
i
j
k
l
m

frequency
77
17
32
42
120
24
17
50
76
4
7
42
24

letter
n
o
p
q
r
s
t
u
v
w
x
y
z

frequency
67
67
20
5
59
67
85
37
12
22
4
22
2

figure 523 relative frequencies for the 26 letters of the alphabet as they appear in a selected set of english documents frequency represents the expected
frequency of occurrence per 1000 letters ignoring case

amount of space four bytes it would seem that words such as deed which
are composed of relatively common letters should be storable in less space than
words such as muck which are composed of relatively uncommon letters
if some characters are used more frequently than others is it possible to take
advantage of this fact and somehow assign them shorter codes the price could
be that other characters require longer codes but this might be worthwhile if such
characters appear rarely enough this concept is at the heart of file compression
techniques in common use today the next section presents one such approach to
assigning variablelength codes called huffman coding while it is not commonly
used in its simplest form for file compression there are better methods huffman
coding gives the flavor of such coding schemes one motivation for studying huffman coding is because it provides our first opportunity to see a type of tree structure
referred to as a search trie
561

building huffman coding trees

huffman coding assigns codes to characters such that the length of the code depends on the relative frequency or weight of the corresponding character thus it
is a variablelength code if the estimated frequencies for letters match the actual
frequency found in an encoded message then the length of that message will typically be less than if a fixedlength code had been used the huffman code for each
letter is derived from a full binary tree called the huffman coding tree or simply
the huffman tree each leaf of the huffman tree corresponds to a letter and we
define the weight of the leaf node to be the weight frequency of its associated


$$@@$$PAGE: 206
187

sec 56 huffman coding trees

letter
frequency

c
32

d
42

e
120

k
7

l
42

m
24

u
37

z
2

figure 524 the relative frequencies for eight selected letters

letter the goal is to build a tree with the minimum external path weight define
the weighted path length of a leaf to be its weight times its depth the binary tree
with minimum external path weight is the one with the minimum sum of weighted
path lengths for the given set of leaves a letter with high weight should have low
depth so that it will count the least against the total path length as a result another
letter might be pushed deeper in the tree if it has less weight
the process of building the huffman tree for n letters is quite simple first create a collection of n initial huffman trees each of which is a single leaf node containing one of the letters put the n partial trees onto a priority queue organized by
weight frequency next remove the first two trees the ones with lowest weight
from the priority queue join these two trees together to create a new tree whose
root has the two trees as children and whose weight is the sum of the weights of the
two trees put this new tree back into the priority queue this process is repeated
until all of the partial huffman trees have been combined into one
example 58 figure 525 illustrates part of the huffman tree construction
process for the eight letters of figure 524 ranking d and l arbitrarily by
alphabetical order the letters are ordered by frequency as
letter
frequency

z
2

k
7

m
24

c
32

u
37

d
42

l
42

e
120

because the first two letters on the list are z and k they are selected to
be the first trees joined together6 they become the children of a root node
with weight 9 thus a tree whose root has weight 9 is placed back on the
list where it takes up the first position the next step is to take values 9
and 24 off the list corresponding to the partial tree with two leaf nodes
built in the last step and the partial tree storing the letter m respectively
and join them together the resulting root node has weight 33 and so this
tree is placed back into the list its priority will be between the trees with
values 32 for letter c and 37 for letter u this process continues until a
tree whose root has weight 306 is built this tree is shown in figure 526

6

for clarity the examples for building huffman trees show a sorted list to keep the letters ordered
by frequency but a real implementation would use a heap to implement the priority queue for
efficiency


$$@@$$PAGE: 207
188

chap 5 binary trees

step 1

2
z

7
k
9

step 2
2
z

24
m

32
c

37
u

42
d

42
l

120
e

24
m

32
c

37
u

42
d

42
l

120
e

37
u

42
d

42
l

120
e

7
k

32
c

33

step 3

24
m

9

37
u

2
z

7
k

42
d

42
l

33

32
c

step 4

24
m

9
2
z
42
l

step 5

120
e

65

7
k

65

79

32
c

37
u

33

42
d

24
m

9
2
z

120
e

7
k

figure 525 the first five steps of the building process for a sample huffman
tree


$$@@$$PAGE: 208
189

sec 56 huffman coding trees

0

306

1

120
e

0

186

1

79
0
37
u

0

1
42
d

107

42
l

1
0

65

1
33

32
c

0

1

9
0
2
z

1

24
m

7
k

figure 526 a huffman tree for the letters of figure 524

figure 527 shows an implementation for huffman tree nodes this implementation is similar to the varbinnode implementation of figure 510 there is an
abstract base class named huffnode and two subclasses named leafnode
and intlnode this implementation reflects the fact that leaf and internal nodes
contain distinctly different information
figure 528 shows the implementation for the huffman tree figure 529 shows
the c code for the treebuilding process
huffman tree building is an example of a greedy algorithm at each step the
algorithm makes a greedy decision to merge the two subtrees with least weight
this makes the algorithm simple but does it give the desired result this section concludes with a proof that the huffman tree indeed gives the most efficient
arrangement for the set of letters the proof requires the following lemma
lemma 51 for any huffman tree built by function buildhuff containing at
least two letters the two letters with least frequency are stored in siblings nodes
whose depth is at least as deep as any other leaf nodes in the tree
proof call the two letters with least frequency l1 and l2  they must be siblings
because buildhuff selects them in the first step of the construction process
assume that l1 and l2 are not the deepest nodes in the tree in this case the huffman
tree must either look as shown in figure 530 or in some sense be symmetrical
to this for this situation to occur the parent of l1 and l2  labeled v must have
greater weight than the node labeled x otherwise function buildhuff would
have selected node v in place of node x as the child of node u however this is
impossible because l1 and l2 are the letters with least frequency
2


$$@@$$PAGE: 209
190

chap 5 binary trees

 huffman tree node abstract base class
template typename e class huffnode 
public
virtual huffnode 
 base destructor
virtual int weight  0
 return frequency
virtual bool isleaf  0
 determine type

template typename e
 leaf node subclass
class leafnode  public huffnodee 
private
e it
 value
int wgt
 weight
public
leafnodeconst e val int freq
 constructor
 it  val wgt  freq 
int weight  return wgt 
e val  return it 
bool isleaf  return true 

template typename e
 internal node subclass
class intlnode  public huffnodee 
private
huffnodee lc
 left child
huffnodee rc
 right child
int wgt
 subtree weight
public
intlnodehuffnodee l huffnodee r
 wgt  lweight  rweight lc  l rc  r 
int weight  return wgt 
bool isleaf  return false 
huffnodee left const  return lc 
void setlefthuffnodee b
 lc  huffnodeeb 
huffnodee right const  return rc 
void setrighthuffnodee b
 rc  huffnodeeb 

figure 527 implementation for huffman tree nodes internal nodes and leaf
nodes are represented by separate classes each derived from an abstract base class


$$@@$$PAGE: 210
sec 56 huffman coding trees

191

 hufftree is a template of two parameters the element
 type being coded and a comparator for two such elements
template typename e
class hufftree 
private
huffnodee root
 tree root
public
hufftreee val int freq  leaf constructor
 root  new leafnodeeval freq 
 internal node constructor
hufftreehufftreee l hufftreee r
 root  new intlnodeelroot rroot 
hufftree 
 destructor
huffnodee root  return root 
 get root
int weight  return rootweight   root weight

figure 528 class declarations for the huffman tree

 build a huffman tree from a collection of frequencies
template typename e hufftreee
buildhuffhufftreee treearray int count 
heaphufftreeemintreecomp forest 
new heaphufftreee mintreecomptreearray
count count
hufftreechar temp1 temp2 temp3  null
while forestsize  1 
temp1  forestremovefirst
 pull first two trees
temp2  forestremovefirst

off the list
temp3  new hufftreeetemp1 temp2
forestinserttemp3  put the new tree back on list
delete temp1
 must delete the remnants
delete temp2

of the trees we created

return temp3

figure 529 implementation for the huffman tree construction function
buildhuff takes as input fl the minheap of partial huffman trees which
initially are single leaf nodes as shown in step 1 of figure 525 the body of
function buildtree consists mainly of a for loop on each iteration of the
for loop the first two partial trees are taken off the heap and placed in variables
temp1 and temp2 a tree is created temp3 such that the left and right subtrees
are temp1 and temp2 respectively finally temp3 is returned to fl


$$@@$$PAGE: 211
192

chap 5 binary trees

v
l1

u
l2

x

figure 530 an impossible huffman tree showing the situation where the two
nodes with least weight l1 and l2  are not the deepest nodes in the tree triangles
represent subtrees

theorem 53 function buildhuff builds the huffman tree with the minimum
external path weight for the given set of letters
proof the proof is by induction on n the number of letters
 base case for n  2 the huffman tree must have the minimum external
path weight because there are only two possible trees each with identical
weighted path lengths for the two leaves
 induction hypothesis assume that any tree created by buildhuff that
contains n  1 leaves has minimum external path length
 induction step given a huffman tree t built by buildhuff with n
leaves n  2 suppose that w1  w2      wn where w1 to wn are
the weights of the letters call v the parent of the letters with frequencies w1
and w2  from the lemma we know that the leaf nodes containing the letters
with frequencies w1 and w2 are as deep as any nodes in t if any other leaf
nodes in the tree were deeper we could reduce their weighted path length by
swapping them with w1 or w2  but the lemma tells us that no such deeper
nodes exist call t0 the huffman tree that is identical to t except that node
v is replaced with a leaf node v 0 whose weight is w1  w2  by the induction
hypothesis t0 has minimum external path length returning the children to
v 0 restores tree t which must also have minimum external path length
thus by mathematical induction function buildhuff creates the huffman
tree with minimum external path length
2
562

assigning and using huffman codes

once the huffman tree has been constructed it is an easy matter to assign codes
to individual letters beginning at the root we assign either a 0 or a 1 to each
edge in the tree 0 is assigned to edges connecting a node with its left child and
1 to edges connecting a node with its right child this process is illustrated by


$$@@$$PAGE: 212
193

sec 56 huffman coding trees

letter
c
d
e
k
l
m
u
z

freq
32
42
120
7
42
24
37
2

code
1110
101
0
111101
110
11111
100
111100

bits
4
3
1
6
3
5
3
6

figure 531 the huffman codes for the letters of figure 524

figure 526 the huffman code for a letter is simply a binary number determined
by the path from the root to the leaf corresponding to that letter thus the code
for e is 0 because the path from the root to the leaf node for e takes a single left
branch the code for k is 111101 because the path to the node for k takes four
right branches then a left and finally one last right figure 531 lists the codes for
all eight letters
given codes for the letters it is a simple matter to use these codes to encode a
text message we simply replace each letter in the string with its binary code a
lookup table can be used for this purpose
example 59 using the code generated by our example huffman tree
the word deed is represented by the bit string 10100101 and the word
muck is represented by the bit string 111111001110111101
decoding the message is done by looking at the bits in the coded string from
left to right until a letter is decoded this can be done by using the huffman tree in
a reverse process from that used to generate the codes decoding a bit string begins
at the root of the tree we take branches depending on the bit value  left for 0
and right for 1  until reaching a leaf node this leaf contains the first character
in the message we then process the next bit in the code restarting at the root to
begin the next character
example 510 to decode the bit string 1011001110111101 we begin
at the root of the tree and take a right branch for the first bit which is 1
because the next bit is a 0 we take a left branch we then take another
right branch for the third bit 1 arriving at the leaf node corresponding
to the letter d thus the first letter of the coded word is d we then begin
again at the root of the tree to process the fourth bit which is a 1 taking
a right branch then two left branches for the next two bits which are 0
we reach the leaf node corresponding to the letter u thus the second letter


$$@@$$PAGE: 213
194

chap 5 binary trees

is u in similar manner we complete the decoding process to find that the
last two letters are c and k spelling the word duck
a set of codes is said to meet the prefix property if no code in the set is the
prefix of another the prefix property guarantees that there will be no ambiguity in
how a bit string is decoded in other words once we reach the last bit of a code
during the decoding process we know which letter it is the code for huffman codes
certainly have the prefix property because any prefix for a code would correspond to
an internal node while all codes correspond to leaf nodes for example the code
for m is 11111 taking five right branches in the huffman tree of figure 526
brings us to the leaf node containing m we can be sure that no letter can have code
111 because this corresponds to an internal node of the tree and the treebuilding
process places letters only at the leaf nodes
how efficient is huffman coding in theory it is an optimal coding method
whenever the true frequencies are known and the frequency of a letter is independent of the context of that letter in the message in practice the frequencies of
letters in an english text document do change depending on context for example
while e is the most commonly used letter of the alphabet in english documents
t is more common as the first letter of a word this is why most commercial compression utilities do not use huffman coding as their primary coding method but
instead use techniques that take advantage of the context for the letters
another factor that affects the compression efficiency of huffman coding is the
relative frequencies of the letters some frequency patterns will save no space as
compared to fixedlength codes others can result in great compression in general
huffman coding does better when there is large variation in the frequencies of
letters in the particular case of the frequencies shown in figure 531 we can
determine the expected savings from huffman coding if the actual frequencies of a
coded message match the expected frequencies
example 511 because the sum of the frequencies in figure 531 is 306
and e has frequency 120 we expect it to appear 120 times in a message
containing 306 letters an actual message might or might not meet this
expectation letters d l and u have code lengths of three and together
are expected to appear 121 times in 306 letters letter c has a code length of
four and is expected to appear 32 times in 306 letters letter m has a code
length of five and is expected to appear 24 times in 306 letters finally
letters k and z have code lengths of six and together are expected to appear
only 9 times in 306 letters the average expected cost per character is
simply the sum of the cost for each character ci  times the probability of
its occurring pi  or
c1 p1  c2 p2      cn pn 


$$@@$$PAGE: 214
195

sec 56 huffman coding trees

this can be reorganized as
c1 f1  c2 f2      cn fn
ft
where fi is the relative frequency of letter i and ft is the total for all letter
frequencies for this set of frequencies the expected cost per letter is
1120312143252469306  785306  257
a fixedlength code for these eight characters would require log 8  3 bits
per letter as opposed to about 257 bits per letter for huffman coding thus
huffman coding is expected to save about 14 for this set of letters
huffman coding for all ascii symbols should do better than this the letters of
figure 531 are atypical in that there are too many common letters compared to the
number of rare letters huffman coding for all 26 letters would yield an expected
cost of 429 bits per letter the equivalent fixedlength code would require about
five bits this is somewhat unfair to fixedlength coding because there is actually
room for 32 codes in five bits but only 26 letters more generally huffman coding
of a typical text file will save around 40 over ascii coding if we charge ascii
coding at eight bits per character huffman coding for a binary file such as a
compiled executable would have a very different set of distribution frequencies and
so would have a different space savings most commercial compression programs
use two or three coding schemes to adjust to different types of files
in the preceding example deed was coded in 8 bits a saving of 33 over
the twelve bits required from a fixedlength coding however muck requires
18 bits more space than required by the corresponding fixedlength coding the
problem is that muck is composed of letters that are not expected to occur
often if the message does not match the expected frequencies of the letters than
the length of the encoding will not be as expected either
563

search in huffman trees

when we decode a character using the huffman coding tree we follow a path
through the tree dictated by the bits in the code string each 0 bit indicates a left
branch while each 1 bit indicates a right branch now look at figure 526 and
consider this structure in terms of searching for a given letter whose key value is
its huffman code we see that all letters with codes beginning with 0 are stored
in the left branch while all letters with codes beginning with 1 are stored in the
right branch contrast this with storing records in a bst there all records with
key value less than the root value are stored in the left branch while all records
with key values greater than the root are stored in the right branch


$$@@$$PAGE: 215
196

chap 5 binary trees

if we view all records stored in either of these structures as appearing at some
point on a number line representing the key space we can see that the splitting
behavior of these two structures is very different the bst splits the space based
on the key values as they are encountered when going down the tree but the splits
in the key space are predetermined for the huffman tree search tree structures
whose splitting points in the key space are predetermined are given the special
name trie to distinguish them from the type of search tree like the bst whose
splitting points are determined by the data tries are discussed in more detail in
chapter 13

57

further reading

see shaffer and brown sb93 for an example of a tree implementation where an
internal node pointer field stores the value of its child instead of a pointer to its
child when the child is a leaf node
many techniques exist for maintaining reasonably balanced bsts in the face of
an unfriendly series of insert and delete operations one example is the avl tree of
adelsonvelskii and landis which is discussed by knuth knu98 the avl tree
see section 132 is actually a bst whose insert and delete routines reorganize the
tree structure so as to guarantee that the subtrees rooted by the children of any node
will differ in height by at most one another example is the splay tree st85 also
discussed in section 132
see bentleys programming pearl thanks heaps ben85 ben88 for a good
discussion on the heap data structure and its uses
the proof of section 561 that the huffman coding tree has minimum external
path weight is from knuth knu97 for more information on data compression
techniques see managing gigabytes by witten moffat and bell wmb99 and
codes and cryptography by dominic welsh wel88 tables 523 and 524 are
derived from welsh wel88

58

exercises

51 section 511 claims that a full binary tree has the highest number of leaf
nodes among all trees with n internal nodes prove that this is true
52 define the degree of a node as the number of its nonempty children prove
by induction that the number of degree 2 nodes in any binary tree is one less
than the number of leaves
53 define the internal path length for a tree as the sum of the depths of all
internal nodes while the external path length is the sum of the depths of all
leaf nodes in the tree prove by induction that if tree t is a full binary tree
with n internal nodes i is ts internal path length and e is ts external path
length then e  i  2n for n  0


$$@@$$PAGE: 216
sec 58 exercises

197

54 explain why function preorder2 from section 52 makes half as many
recursive calls as function preorder explain why it makes twice as many
accesses to left and right children
55 a modify the preorder traversal of section 52 to perform an inorder
traversal of a binary tree
b modify the preorder traversal of section 52 to perform a postorder
traversal of a binary tree
56 write a recursive function named search that takes as input the pointer to
the root of a binary tree not a bst and a value k and returns true if
value k appears in the tree and false otherwise
57 write an algorithm that takes as input the pointer to the root of a binary
tree and prints the node values of the tree in level order level order first
prints the root then all nodes of level 1 then all nodes of level 2 and so
on hint preorder traversals make use of a stack through recursive calls
consider making use of another data structure to help implement the levelorder traversal
58 write a recursive function that returns the height of a binary tree
59 write a recursive function that returns a count of the number of leaf nodes in
a binary tree
510 assume that a given binary tree stores integer values in its nodes write a
recursive function that sums the values of all nodes in the tree
511 assume that a given binary tree stores integer values in its nodes write a
recursive function that traverses a binary tree and prints the value of every
node whos grandparent has a value that is a multiple of five
512 write a recursive function that traverses a binary tree and prints the value of
every node which has at least four greatgrandchildren
513 compute the overhead fraction for each of the following full binary tree implementations
a all nodes store data two child pointers and a parent pointer the data
field requires four bytes and each pointer requires four bytes
b all nodes store data and two child pointers the data field requires
sixteen bytes and each pointer requires four bytes
c all nodes store data and a parent pointer and internal nodes store two
child pointers the data field requires eight bytes and each pointer requires four bytes
d only leaf nodes store data internal nodes store two child pointers the
data field requires eight bytes and each pointer requires four bytes
514 why is the bst property defined so that nodes with values equal to the value
of the root appear only in the right subtree rather than allow equalvalued
nodes to appear in either subtree


$$@@$$PAGE: 217
198
515

516
517
518
519

520

521

522

523
524
525

chap 5 binary trees

a show the bst that results from inserting the values 15 20 25 18 16
5 and 7 in that order
b show the enumerations for the tree of a that result from doing a preorder traversal an inorder traversal and a postorder traversal
draw the bst that results from adding the value 5 to the bst shown in
figure 513a
draw the bst that results from deleting the value 7 from the bst of figure 513b
write a function that prints out the node values for a bst in sorted order
from highest to lowest
write a recursive function named smallcount that given the pointer to
the root of a bst and a key k returns the number of nodes having key
values less than or equal to k function smallcount should visit as few
nodes in the bst as possible
write a recursive function named printrange that given the pointer to
the root of a bst a low key value and a high key value prints in sorted
order all records whose key values fall between the two given keys function
printrange should visit as few nodes in the bst as possible
write a recursive function named checkbst that given the pointer to the
root of a binary tree will return true if the tree is a bst and false if it is
not
describe a simple modification to the bst that will allow it to easily support
finding the kth smallest value in log n average case time then write
a pseudocode function for finding the kth smallest value in your modified
bst
what are the minimum and maximum number of elements in a heap of
height h
where in a maxheap might the smallest element reside
show the maxheap that results from running buildheap on the following
values stored in an array
10

526

5

12

3

2

1

8

7

9

4

a show the heap that results from deleting the maximum value from the
maxheap of figure 520b
b show the heap that results from deleting the element with value 5 from
the maxheap of figure 520b
527 revise the heap definition of figure 519 to implement a minheap the
member function removemax should be replaced by a new function called
removemin
528 build the huffman coding tree and determine the codes for the following set
of letters and weights


$$@@$$PAGE: 218
199

sec 58 exercises

letter
frequency

a
2

b
3

c
5

d
7

e
11

f
13

g
17

h
19

i
23

j
31

k
37

l
41

what is the expected length in bits of a message containing n characters for
this frequency distribution
529 what will the huffman coding tree look like for a set of sixteen characters all
with equal weight what is the average code length for a letter in this case
how does this differ from the smallest possible fixed length code for sixteen
characters
530 a set of characters with varying weights is assigned huffman codes if one
of the characters is assigned code 001 then
a describe all codes that cannot have been assigned
b describe all codes that must have been assigned
531 assume that a sample alphabet has the following weights
letter
frequency

q
2

z
3

f
10

m
10

t
10

s
15

o
20

e
30

a for this alphabet what is the worstcase number of bits required by the
huffman code for a string of n letters what strings have the worstcase performance
b for this alphabet what is the bestcase number of bits required by the
huffman code for a string of n letters what strings have the bestcase performance
c what is the average number of bits required by a character using the
huffman code for this alphabet
532 you must keep track of some data your options are
1 a linkedlist maintained in sorted order
2 a linkedlist of unsorted records
3 a binary search tree
4 an arraybased list maintained in sorted order
5 an arraybased list of unsorted records
for each of the following scenarios which of these choices would be best
explain your answer
a the records are guaranteed to arrive already sorted from lowest to highest ie whenever a record is inserted its key value will always be
greater than that of the last record inserted a total of 1000 inserts will
be interspersed with 1000 searches
b the records arrive with values having a uniform random distribution
so the bst is likely to be well balanced 1000000 insertions are
performed followed by 10 searches


$$@@$$PAGE: 219
200

chap 5 binary trees

c the records arrive with values having a uniform random distribution so
the bst is likely to be well balanced 1000 insertions are interspersed
with 1000 searches
d the records arrive with values having a uniform random distribution so
the bst is likely to be well balanced 1000 insertions are performed
followed by 1000000 searches

59

projects

51 reimplement the composite design for the binary tree node class of figure 511 using a flyweight in place of null pointers to empty nodes
52 one way to deal with the problem of null pointers in binary trees is to
use that space for some other purpose one example is the threaded binary
tree extending the node implementation of figure 57 the threaded binary
tree stores with each node two additional bit fields that indicate if the child
pointers lc and rc are regular pointers to child nodes or threads if lc
is not a pointer to a nonempty child ie if it would be null in a regular
binary tree then it instead stores a pointer to the inorder predecessor of that
node the inorder predecessor is the node that would be printed immediately
before the current node in an inorder traversal if rc is not a pointer to a
child then it instead stores a pointer to the nodes inorder successor the
inorder successor is the node that would be printed immediately after the
current node in an inorder traversal the main advantage of threaded binary
trees is that operations such as inorder traversal can be implemented without
using recursion or a stack
reimplement the bst as a threaded binary tree and include a nonrecursive
version of the preorder traversal
53 implement a city database using a bst to store the database records each
database record contains the name of the city a string of arbitrary length
and the coordinates of the city expressed as integer x and ycoordinates
the bst should be organized by city name your database should allow
records to be inserted deleted by name or coordinate and searched by name
or coordinate another operation that should be supported is to print all
records within a given distance of a specified point collect runningtime
statistics for each operation which operations can be implemented reasonably efficiently ie in log n time in the average case using a bst can
the database system be made more efficient by using one or more additional
bsts to organize the records by location
54 create a binary tree adt that includes generic traversal methods that take a
visitor as described in section 52 write functions count and bstcheck
of section 52 as visitors to be used with the generic traversal method


$$@@$$PAGE: 220
sec 59 projects

201

55 implement a priority queue class based on the maxheap class implementation of figure 519 the following methods should be supported for manipulating the priority queue
void enqueueint objectid int priority
int dequeue
void changeweightint objectid int newpriority

method enqueue inserts a new object into the priority queue with id number objectid and priority priority method dequeue removes the
object with highest priority from the priority queue and returns its object id
method changeweight changes the priority of the object with id number
objectid to be newpriority the type for e should be a class that
stores the object id and the priority for that object you will need a mechanism for finding the position of the desired object within the heap use an
array storing the object with objectid i in position i be sure in your
testing to keep the objectids within the array bounds you must also
modify the heap implementation to store the objects position in the auxiliary array so that updates to objects in the heap can be updated as well in the
array
56 the huffman coding tree function buildhuff of figure 529 manipulates
a sorted list this could result in a n2  algorithm because placing an intermediate huffman tree on the list could take n time revise this algorithm
to use a priority queue based on a minheap instead of a list
57 complete the implementation of the huffman coding tree building on the
code presented in section 56 include a function to compute and store in a
table the codes for each letter and functions to encode and decode messages
this project can be further extended to support file compression to do so
requires adding two steps 1 read through the input file to generate actual
frequencies for all letters in the file and 2 store a representation for the
huffman tree at the beginning of the encoded output file to be used by the
decoding function if you have trouble with devising such a representation
see section 65


$$@@$$PAGE: 221

$$@@$$PAGE: 222
6
nonbinary trees

many organizations are hierarchical in nature such as the military and most businesses consider a company with a president and some number of vice presidents
who report to the president each vice president has some number of direct subordinates and so on if we wanted to model this company with a data structure it
would be natural to think of the president in the root node of a tree the vice presidents at level 1 and their subordinates at lower levels in the tree as we go down the
organizational hierarchy
because the number of vice presidents is likely to be more than two this companys organization cannot easily be represented by a binary tree we need instead
to use a tree whose nodes have an arbitrary number of children unfortunately
when we permit trees to have nodes with an arbitrary number of children they become much harder to implement than binary trees we consider such trees in this
chapter to distinguish them from binary trees we use the term general tree
section 61 presents general tree terminology section 62 presents a simple
representation for solving the important problem of processing equivalence classes
several pointerbased implementations for general trees are covered in section 63
aside from general trees and binary trees there are also uses for trees whose internal nodes have a fixed number k of children where k is something other than
two such trees are known as kary trees section 64 generalizes the properties
of binary trees to kary trees sequential representations useful for applications
such as storing trees on disk are covered in section 65

61

general tree definitions and terminology

a tree t is a finite set of one or more nodes such that there is one designated node
r called the root of t if the set t  r is not empty these nodes are partitioned
into n  0 disjoint subsets t0  t1   tn1  each of which is a tree and whose
roots r1  r2   rn  respectively are children of r the subsets ti 0  i  n are
said to be subtrees of t these subtrees are ordered in that ti is said to come before
203


$$@@$$PAGE: 223
204

chap 6 nonbinary trees

root

parent of v

v
c1

c2

r

ancestors of v

p

s1 s2
c3

siblings of v
subtree rooted at v

children of v
figure 61 notation for general trees node p is the parent of nodes v s1
and s2 thus v s1 and s2 are children of p nodes r and p are ancestors of v
nodes v s1 and s2 are called siblings the oval surrounds the subtree having v
as its root

tj if i  j by convention the subtrees are arranged from left to right with subtree
t0 called the leftmost child of r a nodes out degree is the number of children for
that node a forest is a collection of one or more trees figure 61 presents further
tree notation generalized from the notation for binary trees presented in chapter 5
each node in a tree has precisely one parent except for the root which has no
parent from this observation it immediately follows that a tree with n nodes must
have n  1 edges because each node aside from the root has one edge connecting
that node to its parent

611

an adt for general tree nodes

before discussing general tree implementations we should first make precise what
operations such implementations must support any implementation must be able
to initialize a tree given a tree we need access to the root of that tree there
must be some way to access the children of a node in the case of the adt for
binary tree nodes this was done by providing member functions that give explicit
access to the left and right child pointers unfortunately because we do not know
in advance how many children a given node will have in the general tree we cannot
give explicit functions to access each child an alternative must be found that works
for an unknown number of children


$$@@$$PAGE: 224
205

sec 61 general tree definitions and terminology

 general tree node adt
template typename e class gtnode
public
e value

bool isleaf
gtnode parent
gtnode leftmostchild
gtnode rightsibling
void setvaluee

void insertfirstgtnodee 
void insertnextgtnodee 
void removefirst
void removenext



return nodes value
 true if node is a leaf
 return parent
 return first child
 return right sibling
set nodes value
insert first child
insert next sibling
 remove first child
 remove right sibling

 general tree adt
template typename e class gentree 
public
void clear
 send all nodes to free store
gtnodee root
 return the root of the tree
 combine two subtrees
void newroote gtnodee gtnodee
void print
 print a tree

figure 62 definitions for the general tree and general tree node

one choice would be to provide a function that takes as its parameter the index
for the desired child that combined with a function that returns the number of
children for a given node would support the ability to access any node or process
all children of a node unfortunately this view of access tends to bias the choice for
node implementations in favor of an arraybased approach because these functions
favor random access to a list of children in practice an implementation based on
a linked list is often preferred
an alternative is to provide access to the first or leftmost child of a node and
to provide access to the next or right sibling of a node figure 62 shows class
declarations for general trees and their nodes based on these two access functions
the children of a node can be traversed like a list trying to find the next sibling of
the rightmost sibling would return null
612

general tree traversals

in section 52 three tree traversals were presented for binary trees preorder postorder and inorder for general trees preorder and postorder traversals are defined
with meanings similar to their binary tree counterparts preorder traversal of a general tree first visits the root of the tree then performs a preorder traversal of each
subtree from left to right a postorder traversal of a general tree performs a postorder traversal of the roots subtrees from left to right then visits the root inorder


$$@@$$PAGE: 225
206

chap 6 nonbinary trees

r

a

c

d

b

e

f

figure 63 an example of a general tree

traversal does not have a natural definition for the general tree because there is no
particular number of children for an internal node an arbitrary definition  such
as visit the leftmost subtree in inorder then the root then visit the remaining subtrees in inorder  can be invented however inorder traversals are generally not
useful with general trees
example 61 a preorder traversal of the tree in figure 63 visits the nodes
in order racdebf 
a postorder traversal of this tree visits the nodes in order cdeaf br
to perform a preorder traversal it is necessary to visit each of the children for
a given node say r from left to right this is accomplished by starting at rs
leftmost child call it t from t we can move to ts right sibling and then to that
nodes right sibling and so on
using the adt of figure 62 here is a c implementation to print the nodes
of a general tree in preorder note the for loop at the end which processes the
list of children by beginning with the leftmost child then repeatedly moving to the
next child until calling next returns null
 print using a preorder traversal
void printhelpgtnodee root 
if rootisleaf cout  leaf 
else cout  internal 
cout  rootvalue  n
 now process the children of root
for gtnodee temp  rootleftmostchild
temp  null temp  temprightsibling
printhelptemp



$$@@$$PAGE: 226
sec 62 the parent pointer implementation

62

207

the parent pointer implementation

perhaps the simplest general tree implementation is to store for each node only a
pointer to that nodes parent we will call this the parent pointer implementation
clearly this implementation is not general purpose because it is inadequate for
such important operations as finding the leftmost child or the right sibling for a
node thus it may seem to be a poor idea to implement a general tree in this
way however the parent pointer implementation stores precisely the information
required to answer the following useful question given two nodes are they in
the same tree to answer the question we need only follow the series of parent
pointers from each node to its respective root if both nodes reach the same root
then they must be in the same tree if the roots are different then the two nodes are
not in the same tree the process of finding the ultimate root for a given node we
will call find
the parent pointer representation is most often used to maintain a collection of
disjoint sets two disjoint sets share no members in common their intersection is
empty a collection of disjoint sets partitions some objects such that every object
is in exactly one of the disjoint sets there are two basic operations that we wish to
support
1 determine if two objects are in the same set and
2 merge two sets together
because two merged sets are united the merging operation is called union and
the whole process of determining if two objects are in the same set and then merging
the sets goes by the name unionfind
to implement unionfind we represent each disjoint set with a separate
general tree two objects are in the same disjoint set if they are in the same tree
every node of the tree except for the root has precisely one parent thus each
node requires the same space to represent it the collection of objects is typically
stored in an array where each element of the array corresponds to one object and
each element stores the objects value the objects also correspond to nodes in
the various disjoint trees one tree for each disjoint set so we also store the parent
value with each object in the array those nodes that are the roots of their respective
trees store an appropriate indicator note that this representation means that a single
array is being used to implement a collection of trees this makes it easy to merge
trees together with union operations
figure 64 shows the parent pointer implementation for the general tree called
parptrtree this class is greatly simplified from the declarations of figure 62
because we need only a subset of the general tree operations instead of implementing a separate node class parptrtree simply stores an array where each array
element corresponds to a node of the tree each position i of the array stores the
value for node i and the array position for the parent of node i class parptrtree


$$@@$$PAGE: 227
208

chap 6 nonbinary trees

 general tree representation for unionfind
class parptrtree 
private
int array
 node array
int size
 size of node array
int findint const
 find root
public
parptrtreeint
 constructor
parptrtree  delete  array   destructor
void unionint int
 merge equivalences
bool differint int
 true if not in same tree

int parptrtreefindint curr const   find root
while arraycurr  root curr  arraycurr
return curr  at root

figure 64 general tree implementation using parent pointers for the union
find algorithm

is given two new methods differ and union method differ checks if two
objects are in different sets and method union merges two sets together a private
method find is used to find the ultimate root for an object
an application using the unionfind operations should store a set of n objects where each object is assigned a unique index in the range 0 to n  1 the
indices refer to the corresponding parent pointers in the array class parptrtree
creates and initializes the unionfind array and methods differ and union
take array indices as inputs
figure 65 illustrates the parent pointer implementation note that the nodes
can appear in any order within the array and the array can store up to n separate
trees for example figure 65 shows two trees stored in the same array thus
a single array can store a collection of items distributed among an arbitrary and
changing number of disjoint subsets
consider the problem of assigning the members of a set to disjoint subsets
called equivalence classes recall from section 21 that an equivalence relation is
reflexive symmetric and transitive thus if objects a and b are equivalent and
objects b and c are equivalent we must be able to recognize that objects a and c
are also equivalent
there are many practical uses for disjoint sets and representing equivalences
for example consider figure 66 which shows a graph of ten nodes labeled a
through j notice that for nodes a through i there is some series of edges that
connects any pair of the nodes but node j is disconnected from the rest of the
nodes such a graph might be used to represent connections such as wires between components on a circuit board or roads between cities we can consider
two nodes of the graph to be equivalent if there is a path between them thus


$$@@$$PAGE: 228
209

sec 62 the parent pointer implementation

r

a

c
parents index

w

b

d

e

x

y

z

f

0

0

1

1

1

2

7

7

7

r

a

b

c

d

e

f

w x

y

z

node index 0

1

2

3

4

5

6

7

9 10

label

8

figure 65 the parent pointer array implementation each node corresponds
to a position in the node array which stores its value and a pointer to its parent
the parent pointers are represented by the position in the array of the parent the
root of any tree stores root represented graphically by a slash in the parents
index box this figure shows two trees stored in the same parent pointer array
one rooted at r and the other rooted at w

i

a

b

d

f

c

h

e

g

j

figure 66 a graph with two connected components

nodes a h and e would be equivalent in figure 66 but j is not equivalent to any
other a subset of equivalent connected edges in a graph is called a connected
component the goal is to quickly classify the objects into disjoint sets that correspond to the connected components another application for unionfind occurs
in kruskals algorithm for computing the minimal cost spanning tree for a graph
section 1152
the input to the unionfind algorithm is typically a series of equivalence
pairs in the case of the connected components example the equivalence pairs
would simply be the set of edges in the graph an equivalence pair might say that
object c is equivalent to object a if so c and a are placed in the same subset if
a later equivalence relates a and b then by implication c is also equivalent to b
thus an equivalence pair may cause two subsets to merge each of which contains
several objects


$$@@$$PAGE: 229
210

chap 6 nonbinary trees

equivalence classes can be managed efficiently with the unionfind algorithm initially each object is at the root of its own tree an equivalence pair is
processed by checking to see if both objects of the pair are in the same tree using method differ if they are in the same tree then no change need be made
because the objects are already in the same equivalence class otherwise the two
equivalence classes should be merged by the union method
example 62 as an example of solving the equivalence class problem
consider the graph of figure 66 initially we assume that each node of the
graph is in a distinct equivalence class this is represented by storing each
as the root of its own tree figure 67a shows this initial configuration
using the parent pointer array representation now consider what happens
when equivalence relationship a b is processed the root of the tree
containing a is a and the root of the tree containing b is b to make them
equivalent one of these two roots is set to be the parent of the other in
this case it is irrelevant which points to which so we arbitrarily select the
first in alphabetical order to be the root this is represented in the parent
pointer array by setting the parent field of b the node in array position 1
of the array to store a pointer to a equivalence pairs c h g f and
d e are processed in similar fashion when processing the equivalence
pair i f because i and f are both their own roots i is set to point to f
note that this also makes g equivalent to i the result of processing these
five equivalences is shown in figure 67b
the parent pointer representation places no limit on the number of nodes that
can share a parent to make equivalence processing as efficient as possible the
distance from each node to the root of its respective tree should be as small as
possible thus we would like to keep the height of the trees small when merging
two equivalence classes together ideally each tree would have all nodes pointing
directly to the root achieving this goal all the time would require too much additional processing to be worth the effort so we must settle for getting as close as
possible
a lowcost approach to reducing the height is to be smart about how two trees
are joined together one simple technique called the weighted union rule joins
the tree with fewer nodes to the tree with more nodes by making the smaller trees
root point to the root of the bigger tree this will limit the total depth of the tree to
olog n because the depth of nodes only in the smaller tree will now increase by
one and the depth of the deepest node in the combined tree can only be at most one
deeper than the deepest node before the trees were combined the total number
of nodes in the combined tree is therefore at least twice the number in the smaller


$$@@$$PAGE: 230
211

sec 62 the parent pointer implementation

a

b

c

d

e

f

g

h

i

j

0

1

2

3

4

5

6

7

8

9

a

b

c

d

e

f

g

h

i

j

a

c

f

j

d

b

h

g

i

e

f

j

i

d

a

0

3

5

2

5

a

b

c

d

e

f

g

h

i

j

0

1

2

3

4

5

6

7

8

9
b

0

0

5

3

a

b

c

d

e

0

1

2

3

4

a

5

2

5

f

g

h

i

j

5

6

7

8

9

b

c

g

e

h
c

5

0

0

5

3

a

b

c

d

e

0

1

2

3

4

j

f

5

2

5

f

g

h

i

j

5

6

7

8

9

a

b

g

i

c

h
d

figure 67 an example of equivalence processing a initial configuration for
the ten nodes of the graph in figure 66 the nodes are placed into ten independent
equivalence classes b the result of processing five edges a b c h g f
d e and i f c the result of processing two more edges h a and e g
d the result of processing edge h e

d

e


$$@@$$PAGE: 231
212

chap 6 nonbinary trees

subtree thus the depth of any node can be increased at most log n times when n
equivalences are processed
example 63 when processing equivalence pair i f in figure 67b
f is the root of a tree with two nodes while i is the root of a tree with only
one node thus i is set to point to f rather than the other way around
figure 67c shows the result of processing two more equivalence pairs
h a and e g for the first pair the root for h is c while the root
for a is itself both trees contain two nodes so it is an arbitrary decision
as to which node is set to be the root for the combined tree in the case
of equivalence pair e g the root of e is d while the root of g is f
because f is the root of the larger tree node d is set to point to f
not all equivalences will combine two trees if equivalence f g is processed
when the representation is in the state shown in figure 67c no change will be
made because f is already the root for g
the weighted union rule helps to minimize the depth of the tree but we can do
better than this path compression is a method that tends to create extremely shallow trees path compression takes place while finding the root for a given node x
call this root r path compression resets the parent of every node on the path from
x to r to point directly to r this can be implemented by first finding r a second
pass is then made along the path from x to r assigning the parent field of each
node encountered to r alternatively a recursive algorithm can be implemented as
follows this version of find not only returns the root of the current node but
also makes all ancestors of the current node point to the root
 find with path compression
int parptrtreefindint curr const 
if arraycurr  root return curr  at root
arraycurr  findarraycurr
return arraycurr


example 64 figure 67d shows the result of processing equivalence
pair h e on the the representation shown in figure 67c using the standard weighted union rule without path compression figure 68 illustrates
the path compression process for the same equivalence pair after locating
the root for node h we can perform path compression to make h point
directly to root object a likewise e is set to point directly to its root f
finally object a is set to point to root object f
note that path compression takes place during the find operation not
during the union operation in figure 68 this means that nodes b c and
h have node a remain as their parent rather than changing their parent to


$$@@$$PAGE: 232
213

sec 63 general tree implementations

5

0

0

5

5

a

b

c

d

e

0

1

2

3

4

f

5

0

5

f

g

h

i

j

5

6

7

8

9

b

a

g

c

h

i

j

d

e

figure 68 an example of path compression showing the result of processing
equivalence pair h e on the representation of figure 67c

be f while we might prefer to have these nodes point to f to accomplish
this would require that additional information from the find operation be
passed back to the union operation this would not be practical
path compression keeps the cost of each find operation very close to constant
to be more precise about what is meant by very close to constant the cost of path
compression for n find operations on n nodes when combined with the weighted
union rule for joining sets is approximately1 n log n the notation log n
means the number of times that the log of n must be taken before n  1 for
example log 65536 is 4 because log 65536  16 log 16  4 log 4  2 and
finally log 2  1 thus log n grows very slowly so the cost for a series of n find
operations is very close to n
note that this does not mean that the tree resulting from processing n equivalence pairs necessarily has depth log n one can devise a series of equivalence
operations that yields log n depth for the resulting tree however many of the
equivalences in such a series will look only at the roots of the trees being merged
requiring little processing time the total amount of processing time required for
n operations will be n log n yielding nearly constant time for each equivalence operation this is an example of amortized analysis discussed further in
section 143

63

general tree implementations

we now tackle the problem of devising an implementation for general trees that
allows efficient processing for all member functions of the adts shown in figure 62 this section presents several approaches to implementing general trees
each implementation yields advantages and disadvantages in the amount of space
required to store a node and the relative ease with which key operations can be
performed general tree implementations should place no restriction on how many
1

to be more precise this cost has been found to grow in time proportional to the inverse of
ackermanns function see section 66


$$@@$$PAGE: 233
214

chap 6 nonbinary trees

index val par
r

a

c

d

0 r
b

e

f

1 a

0

2 c

1

3 b

0

4 d

1

5 f

3

6 e

1

1

3

2

4

6

5

7

figure 69 the list of children implementation for general trees the column of numbers to the left of the node array labels the array indices the column
labeled val stores node values the column labeled par stores indices or
pointers to the parents the last column stores pointers to the linked list of children for each internal node each element of the linked list stores a pointer to one
of the nodes children shown as the array index of the target node

children a node may have in some applications once a node is created the number
of children never changes in such cases a fixed amount of space can be allocated
for the node when it is created based on the number of children for the node matters become more complicated if children can be added to or deleted from a node
requiring that the nodes space allocation be adjusted accordingly
631

list of children

our first attempt to create a general tree implementation is called the list of children implementation for general trees it simply stores with each internal node a
linked list of its children this is illustrated by figure 69
the list of children implementation stores the tree nodes in an array each
node contains a value a pointer or index to its parent and a pointer to a linked list
of the nodes children stored in order from left to right each linked list element
contains a pointer to one child thus the leftmost child of a node can be found
directly because it is the first element in the linked list however to find the right
sibling for a node is more difficult consider the case of a node m and its parent p
to find ms right sibling we must move down the child list of p until the linked list
element storing the pointer to m has been found going one step further takes us to
the linked list element that stores a pointer to ms right sibling thus in the worst
case to find ms right sibling requires that all children of ms parent be searched


$$@@$$PAGE: 234
215

sec 63 general tree implementations

left val parright
1 r
r

r

a

c

d

x

b

e

f

3 a

0

2

6 b

0

c

1

4

d

1

5

e

1

f

2

8 r
x

7

figure 610 the leftchildrightsibling implementation

combining trees using this representation is difficult if each tree is stored in a
separate node array if the nodes of both trees are stored in a single node array then
adding tree t as a subtree of node r is done by simply adding the root of t to rs
list of children
632

the leftchildrightsibling implementation

with the list of children implementation it is difficult to access a nodes right
sibling figure 610 presents an improvement here each node stores its value
and pointers to its parent leftmost child and right sibling thus each of the basic
adt operations can be implemented by reading a value directly from the node
if two trees are stored within the same node array then adding one as the subtree
of the other simply requires setting three pointers combining trees in this way
is illustrated by figure 611 this implementation is more space efficient than the
list of children implementation and each node requires a fixed amount of space
in the node array
633

dynamic node implementations

the two general tree implementations just described use an array to store the collection of nodes in contrast our standard implementation for binary trees stores
each node as a separate dynamic object containing its value and pointers to its two
children unfortunately nodes of a general tree can have any number of children
and this number may change during the life of the node a general tree node implementation must support these properties one solution is simply to limit the number
of children permitted for any node and allocate pointers for exactly that number of


$$@@$$PAGE: 235
216

chap 6 nonbinary trees

left val par right
r

r

a

c

d

1 r

7

8

3 a

0

2

6 b

0

c

1

4

d

1

5

e

1

f

2

x

b

e

f

0

r
x

7

figure 611 combining two trees that use the leftchildrightsibling implementation the subtree rooted at r in figure 610 now becomes the first child
of r0  three pointers are adjusted in the node array the leftchild field of r0 now
points to node r while the rightsibling field for r points to node x the parent
field of node r points to node r0 

children there are two major objections to this first it places an undesirable
limit on the number of children which makes certain trees unrepresentable by this
implementation second this might be extremely wasteful of space because most
nodes will have far fewer children and thus leave some pointer positions empty
the alternative is to allocate variable space for each node there are two basic
approaches one is to allocate an array of child pointers as part of the node in
essence each node stores an arraybased list of child pointers figure 612 illustrates the concept this approach assumes that the number of children is known
when the node is created which is true for some applications but not for others
it also works best if the number of children does not change if the number of
children does change especially if it increases then some special recovery mechanism must be provided to support a change in the size of the child pointer array
one possibility is to allocate a new node of the correct size from free store and return the old copy of the node to free store for later reuse this works especially well
in a language with builtin garbage collection such as java for example assume
that a node m initially has two children and that space for two child pointers is allocated when m is created if a third child is added to m space for a new node with
three child pointers can be allocated the contents of m is copied over to the new
space and the old space is then returned to free store as an alternative to relying
on the systems garbage collector a memory manager for variable size storage units
can be implemented as described in section 123 another possibility is to use a
collection of free lists one for each array size as described in section 412 note


$$@@$$PAGE: 236
217

sec 63 general tree implementations

val size
r

2

a

3

r
a

c

b

d

e

f

c

b

0

d

0

a

e

1

0

f

0

b

figure 612 a dynamic general tree representation with fixedsize arrays for the
child pointers a the general tree b the tree representation for each node
the first field stores the node value while the second field stores the size of the
child pointer array

r

a

r
a

c

d

b

b

e
a

f

c

d

e

f

b

figure 613 a dynamic general tree representation with linked lists of child
pointers a the general tree b the tree representation

in figure 612 that the current number of children for each node is stored explicitly
in a size field the child pointers are stored in an array with size elements
another approach that is more flexible but which requires more space is to
store a linked list of child pointers with each node as illustrated by figure 613
this implementation is essentially the same as the list of children implementation
of section 631 but with dynamically allocated nodes rather than storing the nodes
in an array


$$@@$$PAGE: 237
218

chap 6 nonbinary trees

root

a

b

figure 614 converting from a forest of general trees to a single binary tree
each node stores pointers to its left child and right sibling the tree roots are
assumed to be siblings for the purpose of converting

634

dynamic leftchildrightsibling implementation

the leftchildrightsibling implementation of section 632 stores a fixed number
of pointers with each node this can be readily adapted to a dynamic implementation in essence we substitute a binary tree for a general tree each node of the
leftchildrightsibling implementation points to two children in a new binary
tree structure the left child of this new structure is the nodes first child in the
general tree the right child is the nodes right sibling we can easily extend this
conversion to a forest of general trees because the roots of the trees can be considered siblings converting from a forest of general trees to a single binary tree
is illustrated by figure 614 here we simply include links from each node to its
right sibling and remove links to all children except the leftmost child figure 615
shows how this might look in an implementation with two pointers at each node
compared with the implementation illustrated by figure 613 which requires overhead of three pointersnode the implementation of figure 615 only requires two
pointers per node the representation of figure 615 is likely to be easier to implement space efficient and more flexible than the other implementations presented
in this section

64

k ary trees

kary trees are trees whose internal nodes all have exactly k children thus
a full binary tree is a 2ary tree the pr quadtree discussed in section 133 is an
example of a 4ary tree because kary tree nodes have a fixed number of children
unlike general trees they are relatively easy to implement in general kary trees
bear many similarities to binary trees and similar implementations can be used for
kary tree nodes note that as k becomes large the potential number of null
pointers grows and the difference between the required sizes for internal nodes
and leaf nodes increases thus as k becomes larger the need to choose separate
implementations for the internal and leaf nodes becomes more pressing


$$@@$$PAGE: 238
219

sec 65 sequential tree implementations

r
a
r
a

c

d

c
b

e

b
d

f

a

f
e
b

figure 615 a general tree converted to the dynamic leftchildrightsibling
representation compared to the representation of figure 613 this representation
requires less space

a

b

figure 616 full and complete 3ary trees a this tree is full but not complete
b this tree is complete but not full

full and complete kary trees are analogous to full and complete binary trees
respectively figure 616 shows full and complete kary trees for k  3 in
practice most applications of kary trees limit them to be either full or complete
many of the properties of binary trees extend to kary trees equivalent theorems to those in section 511 regarding the number of null pointers in a kary
tree and the relationship between the number of leaves and the number of internal
nodes in a kary tree can be derived we can also store a complete kary tree in
an array using simple formulas to compute a nodes relations in a manner similar
to that used in section 533

65

sequential tree implementations

next we consider a fundamentally different approach to implementing trees the
goal is to store a series of node values with the minimum information needed to
reconstruct the tree structure this approach known as a sequential tree implementation has the advantage of saving space because no pointers are stored it has


$$@@$$PAGE: 239
220

chap 6 nonbinary trees

the disadvantage that accessing any node in the tree requires sequentially processing all nodes that appear before it in the node list in other words node access must
start at the beginning of the node list processing nodes sequentially in whatever
order they are stored until the desired node is reached thus one primary virtue
of the other implementations discussed in this section is lost efficient access typically log n time to arbitrary nodes in the tree sequential tree implementations
are ideal for archiving trees on disk for later use because they save space and the
tree structure can be reconstructed as needed for later processing
sequential tree implementations can be used to serialize a tree structure serialization is the process of storing an object as a series of bytes typically so that the
data structure can be transmitted between computers this capability is important
when using data structures in a distributed processing environment
a sequential tree implementation typically stores the node values as they would
be enumerated by a preorder traversal along with sufficient information to describe
the trees shape if the tree has restricted form for example if it is a full binary tree
then less information about structure typically needs to be stored a general tree
because it has the most flexible shape tends to require the most additional shape
information there are many possible sequential tree implementation schemes we
will begin by describing methods appropriate to binary trees then generalize to an
implementation appropriate to a general tree structure
because every node of a binary tree is either a leaf or has two possibly empty
children we can take advantage of this fact to implicitly represent the trees structure the most straightforward sequential tree implementation lists every node
value as it would be enumerated by a preorder traversal unfortunately the node
values alone do not provide enough information to recover the shape of the tree in
particular as we read the series of node values we do not know when a leaf node
has been reached however we can treat all nonempty nodes as internal nodes
with two possibly empty children only null values will be interpreted as leaf
nodes and these can be listed explicitly such an augmented node list provides
enough information to recover the tree structure
example 65 for the binary tree of figure 617 the corresponding sequential representation would be as follows assuming that  stands for
null
abdcegfhi

61

to reconstruct the tree structure from this node list we begin by setting
node a to be the root as left child will be node b node bs left child is
a null pointer so node d must be bs right child node d has two null
children so node c must be the right child of node a


$$@@$$PAGE: 240
221

sec 65 sequential tree implementations

a
b

c
d

e
g

f
h

i

figure 617 sample binary tree for sequential tree implementation examples

to illustrate the difficulty involved in using the sequential tree representation
for processing consider searching for the right child of the root node we must first
move sequentially through the node list of the left subtree only at this point do
we reach the value of the roots right child clearly the sequential representation
is space efficient but not time efficient for descending through the tree along some
arbitrary path
assume that each node value takes a constant amount of space an example
would be if the node value is a positive integer and null is indicated by the value
zero from the full binary tree theorem of section 511 we know that the size
of the node list will be about twice the number of nodes ie the overhead fraction
is 12 the extra space is required by the null pointers we should be able to
store the node list more compactly however any sequential implementation must
recognize when a leaf node has been reached that is a leaf node indicates the end
of a subtree one way to do this is to explicitly list with each node whether it is
an internal node or a leaf if a node x is an internal node then we know that its
two children which may be subtrees immediately follow x in the node list if x
is a leaf node then the next node in the list is the right child of some ancestor
of x not the right child of x in particular the next node will be the child of xs
most recent ancestor that has not yet seen its right child however this assumes
that each internal node does in fact have two children in other words that the
tree is full empty children must be indicated in the node list explicitly assume
that internal nodes are marked with a prime 0  and that leaf nodes show no mark
empty children of internal nodes are indicated by  but the empty children of
leaf nodes are not represented at all note that a full binary tree stores no null
values with this implementation and so requires less overhead
example 66 we can represent the tree of figure 617 as follows
a0 b0 dc0 e0 gf0 hi

62


$$@@$$PAGE: 241
222

chap 6 nonbinary trees

note that slashes are needed for the empty children because this is not a full
binary tree
storing n extra bits can be a considerable savings over storing n null values
in example 66 each node is shown with a mark if it is internal or no mark if it
is a leaf this requires that each node value has space to store the mark bit this
might be true if for example the node value were stored as a 4byte integer but
the range of the values sored was small enough so that not all bits are used an
example would be if all node values must be positive then the highorder sign
bit of the integer value could be used as the mark bit
another approach is to store a separate bit vector to represent the status of each
node in this case each node of the tree corresponds to one bit in the bit vector a
value of 1 could indicate an internal node and 0 could indicate a leaf node
example 67 the bit vector for the tree if figure 617 including positions
for the null children of nodes b and e would be
11001100100

63

storing general trees by means of a sequential implementation requires that
more explicit structural information be included with the node list not only must
the general tree implementation indicate whether a node is leaf or internal it must
also indicate how many children the node has alternatively the implementation
can indicate when a nodes child list has come to an end the next example dispenses with marks for internal or leaf nodes instead it includes a special mark we
will use the  symbol to indicate the end of a child list all leaf nodes are followed by a  symbol because they have no children a leaf node that is also the
last child for its parent would indicate this by two or more successive  symbols
example 68 for the general tree of figure 63 we get the sequential
representation
racdebf

64

note that f is followed by three  marks because it is a leaf the last node
of bs rightmost subtree and the last node of rs rightmost subtree
note that this representation for serializing general trees cannot be used for binary trees this is because a binary tree is not merely a restricted form of general
tree with at most two children every binary tree node has a left and a right child
though either or both might be empty for example the representation of example 68 cannot let us distinguish whether node d in figure 617 is the left or right
child of node b


$$@@$$PAGE: 242
sec 66 further reading

66

223

further reading

the expression log n cited in section 62 is closely related to the inverse of ackermanns function for more information about ackermanns function and the cost
of path compression for unionfind see robert e tarjans paper on the efficiency of a good but not linear set merging algorithm tar75 the article data
structures and algorithms for disjoint set union problems by galil and italiano
gi91 covers many aspects of the equivalence class problem
foundations of multidimensional and metric data structures by hanan samet
sam06 treats various implementations of tree structures in detail within the context of kary trees samet covers sequential implementations as well as the linked
and array implementations such as those described in this chapter and chapter 5
while these books are ostensibly concerned with spatial data structures many of
the concepts treated are relevant to anyone who must implement tree structures

67

exercises

61 write an algorithm to determine if two general trees are identical make the
algorithm as efficient as you can analyze your algorithms running time
62 write an algorithm to determine if two binary trees are identical when the
ordering of the subtrees for a node is ignored for example if a tree has root
node with value r left child with value a and right child with value b this
would be considered identical to another tree with root node value r left
child value b and right child value a make the algorithm as efficient as you
can analyze your algorithms running time how much harder would it be
to make this algorithm work on a general tree
63 write a postorder traversal function for general trees similar to the preorder
traversal function named preorder given in section 612
64 write a function that takes as input a general tree and returns the number of
nodes in that tree write your function to use the gentree and gtnode
adts of figure 62
65 describe how to implement the weighted union rule efficiently in particular
describe what information must be stored with each node and how this information is updated when two trees are merged modify the implementation of
figure 64 to support the weighted union rule
66 a potential alternative to the weighted union rule for combining two trees is
the height union rule the height union rule requires that the root of the tree
with greater height become the root of the union explain why the height
union rule can lead to worse average time behavior than the weighted union
rule
67 using the weighted union rule and path compression show the array for
the parent pointer implementation that results from the following series of


$$@@$$PAGE: 243
224

chap 6 nonbinary trees

equivalences on a set of objects indexed by the values 0 through 15 initially
each element in the set should be in a separate equivalence class when two
trees to be merged are the same size make the root with greater index value
be the child of the root with lesser index value
0 2 1 2 3 4 3 1 3 5 9 11 12 14 3 9 4 14 6 7 8 10 8 7
7 0 10 15 10 13
68 using the weighted union rule and path compression show the array for
the parent pointer implementation that results from the following series of
equivalences on a set of objects indexed by the values 0 through 15 initially
each element in the set should be in a separate equivalence class when two
trees to be merged are the same size make the root with greater index value
be the child of the root with lesser index value

69

610

611

612

613

614
615

2 3 4 5 6 5 3 5 1 0 7 8 1 8 3 8 9 10 11 14 11 10
12 13 11 13 14 1
devise a series of equivalence statements for a collection of sixteen items
that yields a tree of height 5 when both the weighted union rule and path
compression are used what is the total number of parent pointers followed
to perform this series
one alternative to path compression that gives similar performance gains
is called path halving in path halving when the path is traversed from
the node to the root we make the grandparent of every other node i on the
path the new parent of i write a version of find that implements path
halving your find operation should work as you move up the tree rather
than require the two passes needed by path compression
analyze the fraction of overhead required by the list of children implementation the leftchildrightsibling implementation and the two linked
implementations of section 633 how do these implementations compare
in space efficiency
using the general tree adt of figure 62 write a function that takes as input
the root of a general tree and returns a binary tree generated by the conversion
process illustrated by figure 614
use mathematical induction to prove that the number of leaves in a nonempty full kary tree is k  1n  1 where n is the number of internal
nodes
derive the formulas for computing the relatives of a nonempty complete
kary tree node stored in the complete tree representation of section 533
find the overhead fraction for a full kary tree implementation with space
requirements as follows
a all nodes store data k child pointers and a parent pointer the data
field requires four bytes and each pointer requires four bytes


$$@@$$PAGE: 244
225

sec 67 exercises

c

a

f

b

g

e

h

d

i

figure 618 a sample tree for exercise 616

b all nodes store data and k child pointers the data field requires sixteen bytes and each pointer requires four bytes
c all nodes store data and a parent pointer and internal nodes store k
child pointers the data field requires eight bytes and each pointer requires four bytes
d only leaf nodes store data only internal nodes store k child pointers
the data field requires four bytes and each pointer requires two bytes
616

a write out the sequential representation for figure 618 using the coding
illustrated by example 65
b write out the sequential representation for figure 618 using the coding
illustrated by example 66
617 draw the binary tree representing the following sequential representation for
binary trees illustrated by example 65
abdecf
618 draw the binary tree representing the following sequential representation for
binary trees illustrated by example 66
a0 b0 c0 d0 ge
show the bit vector for leaf and internal nodes as illustrated by example 67
for this tree
619 draw the general tree represented by the following sequential representation
for general trees illustrated by example 68
xpcqrvm
620

a write a function to decode the sequential representation for binary trees
illustrated by example 65 the input should be the sequential representation and the output should be a pointer to the root of the resulting
binary tree


$$@@$$PAGE: 245
226

chap 6 nonbinary trees

b write a function to decode the sequential representation for full binary
trees illustrated by example 66 the input should be the sequential
representation and the output should be a pointer to the root of the resulting binary tree
c write a function to decode the sequential representation for general
trees illustrated by example 68 the input should be the sequential
representation and the output should be a pointer to the root of the resulting general tree
621 devise a sequential representation for huffman coding trees suitable for use
as part of a file compression utility see project 57

68

projects

61 write classes that implement the general tree class declarations of figure 62
using the dynamic leftchildrightsibling representation described in section 634
62 write classes that implement the general tree class declarations of figure 62
using the linked general tree implementation with child pointer arrays of figure 612 your implementation should support only fixedsize nodes that
do not change their number of children once they are created then reimplement these classes with the linked list of children representation of
figure 613 how do the two implementations compare in space and time
efficiency and ease of implementation
63 write classes that implement the general tree class declarations of figure 62
using the linked general tree implementation with child pointer arrays of figure 612 your implementation must be able to support changes in the number of children for a node when created a node should be allocated with
only enough space to store its initial set of children whenever a new child is
added to a node such that the array overflows allocate a new array from free
store that can store twice as many children
64 implement a bst file archiver your program should take a bst created in
main memory using the implementation of figure 514 and write it out to
disk using one of the sequential representations of section 65 it should also
be able to read in disk files using your sequential representation and create
the equivalent main memory representation
65 use the unionfind algorithm to implement a solution to the following
problem given a set of points represented by their xycoordinates assign
the points to clusters any two points are defined to be in the same cluster if
they are within a specified distance d of each other for the purpose of this
problem clustering is an equivalence relationship in other words points a
b and c are defined to be in the same cluster if the distance between a and b


$$@@$$PAGE: 246
sec 68 projects

227

is less than d and the distance between a and c is also less than d even if the
distance between b and c is greater than d to solve the problem compute
the distance between each pair of points using the equivalence processing
algorithm to merge clusters whenever two points are within the specified
distance what is the asymptotic complexity of this algorithm where is the
bottleneck in processing
66 in this project you will run some empirical tests to determine if some variations on path compression in the unionfind algorithm will lead to improved performance you should compare the following five implementations
a standard unionfind with path compression and weighted union
b path compression and weighted union except that path compression is
done after the union instead of during the find operation that is
make all nodes along the paths traversed in both trees point directly to
the root of the larger tree
c weighted union and path halving as described in exercise 610
d weighted union and a simplified form of path compression at the end
of every find operation make the node point to its trees root but
dont change the pointers for other nodes along the path
e weighted union and a simplified form of path compression both nodes
in the equivalence will be set to point directly to the root of the larger
tree after the union operation for example consider processing the
equivalence a b where a0 is the root of a and b0 is the root of b
assume the tree with root a0 is bigger than the tree with root b0  at the
end of the unionfind operation nodes a b and b0 will all point
directly to a0 


$$@@$$PAGE: 247

$$@@$$PAGE: 248
part iii
sorting and searching

229


$$@@$$PAGE: 249

$$@@$$PAGE: 250
7
internal sorting

we sort many things in our everyday lives a handful of cards when playing bridge
bills and other piles of paper jars of spices and so on and we have many intuitive
strategies that we can use to do the sorting depending on how many objects we
have to sort and how hard they are to move around sorting is also one of the most
frequently performed computing tasks we might sort the records in a database
so that we can search the collection efficiently we might sort the records by zip
code so that we can print and mail them more cheaply we might use sorting as an
intrinsic part of an algorithm to solve some other problem such as when computing
the minimumcost spanning tree see section 115
because sorting is so important naturally it has been studied intensively and
many algorithms have been devised some of these algorithms are straightforward
adaptations of schemes we use in everyday life others are totally alien to how humans do things having been invented to sort thousands or even millions of records
stored on the computer after years of study there are still unsolved problems
related to sorting new algorithms are still being developed and refined for specialpurpose applications
while introducing this central problem in computer science this chapter has
a secondary purpose of illustrating issues in algorithm design and analysis for
example this collection of sorting algorithms shows multiple approaches to using divideandconquer in particular there are multiple ways to do the dividing
mergesort divides a list in half quicksort divides a list into big values and small
values and radix sort divides the problem by working on one digit of the key at
a time sorting algorithms can also illustrate a wide variety of analysis techniques
well find that it is possible for an algorithm to have an average case whose growth
rate is significantly smaller than its worse case quicksort well see how it is
possible to speed up sorting algorithms both shellsort and quicksort by taking
advantage of the best case behavior of another algorithm insertion sort well see
several examples of how we can tune an algorithm for better performance well
see that special case behavior by some algorithms makes them a good solution for
231


$$@@$$PAGE: 251
232

chap 7 internal sorting

special niche applications heapsort sorting provides an example of a significant
technique for analyzing the lower bound for a problem sorting will also be used
to motivate the introduction to file processing presented in chapter 8
the present chapter covers several standard algorithms appropriate for sorting
a collection of records that fit in the computers main memory it begins with a discussion of three simple but relatively slow algorithms requiring n2  time in the
average and worst cases several algorithms with considerably better performance
are then presented some with n log n worstcase running time the final sorting method presented requires only n worstcase time under special conditions
the chapter concludes with a proof that sorting in general requires n log n time
in the worst case

71

sorting terminology and notation

except where noted otherwise input to the sorting algorithms presented in this
chapter is a collection of records stored in an array records are compared to one
another by means of a comparator class as introduced in section 44 to simplify
the discussion we will assume that each record has a key field whose value is extracted from the record by the comparator the key method of the comparator class
is prior which returns true when its first argument should appear prior to its second argument in the sorted list we also assume that for every record type there is
a swap function that can interchange the contents of two records in the arraysee
the appendix
given a set of records r1  r2   rn with key values k1  k2   kn  the sorting
problem is to arrange the records into any order s such that records rs1  rs2   rsn
have keys obeying the property ks1  ks2    ksn  in other words the sorting
problem is to arrange a set of records so that the values of their key fields are in
nondecreasing order
as defined the sorting problem allows input with two or more records that have
the same key value certain applications require that input not contain duplicate
key values the sorting algorithms presented in this chapter and in chapter 8 can
handle duplicate key values unless noted otherwise
when duplicate key values are allowed there might be an implicit ordering
to the duplicates typically based on their order of occurrence within the input it
might be desirable to maintain this initial ordering among duplicates a sorting
algorithm is said to be stable if it does not change the relative ordering of records
with identical key values many but not all of the sorting algorithms presented in
this chapter are stable or can be made stable with minor changes
when comparing two sorting algorithms the most straightforward approach
would seem to be simply program both and measure their running times an example of such timings is presented in figure 720 however such a comparison


$$@@$$PAGE: 252
sec 72 three

n2  sorting algorithms

233

can be misleading because the running time for many sorting algorithms depends
on specifics of the input values in particular the number of records the size of
the keys and the records the allowable range of the key values and the amount by
which the input records are out of order can all greatly affect the relative running
times for sorting algorithms
when analyzing sorting algorithms it is traditional to measure the number of
comparisons made between keys this measure is usually closely related to the
running time for the algorithm and has the advantage of being machine and datatype independent however in some cases records might be so large that their
physical movement might take a significant fraction of the total running time if so
it might be appropriate to measure the number of swap operations performed by the
algorithm in most applications we can assume that all records and keys are of fixed
length and that a single comparison or a single swap operation requires a constant
amount of time regardless of which keys are involved some special situations
change the rules for comparing sorting algorithms for example an application
with records or keys having widely varying length such as sorting a sequence of
variable length strings will benefit from a specialpurpose sorting technique some
applications require that a small number of records be sorted but that the sort be
performed frequently an example would be an application that repeatedly sorts
groups of five numbers in such cases the constants in the runtime equations that
are usually ignored in an asymptotic analysis now become crucial finally some
situations require that a sorting algorithm use as little memory as possible we will
note which sorting algorithms require significant extra memory beyond the input
array

72

three n2  sorting algorithms

this section presents three simple sorting algorithms while easy to understand
and implement we will soon see that they are unacceptably slow when there are
many records to sort nonetheless there are situations where one of these simple
algorithms is the best tool for the job
721

insertion sort

imagine that you have a stack of phone bills from the past two years and that you
wish to organize them by date a fairly natural way to do this might be to look at
the first two bills and put them in order then take the third bill and put it into the
right order with respect to the first two and so on as you take each bill you would
add it to the sorted pile that you have already made this naturally intuitive process
is the inspiration for our first sorting algorithm called insertion sort insertion
sort iterates through a list of records each record is inserted in turn at the correct
position within a sorted list composed of those records already processed the


$$@@$$PAGE: 253
234

chap 7 internal sorting

42
20
17
13
28
14
23
15

i1

2

3

4

5

6

7

20
42
17
13
28
14
23
15

17
20
42
13
28
14
23
15

13
17
20
42
28
14
23
15

13
17
20
28
42
14
23
15

13
14
17
20
28
42
23
15

13
14
17
20
23
28
42
15

13
14
15
17
20
23
28
42

figure 71 an illustration of insertion sort each column shows the array after
the iteration with the indicated value of i in the outer for loop values above
the line in each column have been sorted arrows indicate the upward motions of
records through the array

following is a c implementation the input is an array of n records stored in
array a
template typename e typename comp
void inssorte a int n   insertion sort
for int i1 in i
 insert ith record
for int ji j0  compprioraj aj1 j
swapa j j1


consider the case where inssort is processing the ith record which has key
value x the record is moved upward in the array as long as x is less than the
key value immediately above it as soon as a key value less than or equal to x is
encountered inssort is done with that record because all records above it in the
array must have smaller keys figure 71 illustrates how insertion sort works
the body of inssort is made up of two nested for loops the outer for
loop is executed n  1 times the inner for loop is harder to analyze because
the number of times it executes depends on how many keys in positions 1 to i  1
have a value less than that of the key in position i in the worst case each record
must make its way to the top of the array this would occur if the keys are initially
arranged from highest to lowest in the reverse of sorted order in this case the
number of comparisons will be one the first time through the for loop two the
second time and so on thus the total number of comparisons will be
n
x

i  n2 2  n2 

i2

in contrast consider the bestcase cost this occurs when the keys begin in
sorted order from lowest to highest in this case every pass through the inner
for loop will fail immediately and no values will be moved the total number


$$@@$$PAGE: 254
sec 72 three

n2  sorting algorithms

235

of comparisons will be n  1 which is the number of times the outer for loop
executes thus the cost for insertion sort in the best case is n
while the best case is significantly faster than the worst case the worst case
is usually a more reliable indication of the typical running time however there
are situations where we can expect the input to be in sorted or nearly sorted order
one example is when an already sorted list is slightly disordered by a small number
of additions to the list restoring sorted order using insertion sort might be a good
idea if we know that the disordering is slight examples of algorithms that take advantage of insertion sorts nearbestcase running time are the shellsort algorithm
of section 73 and the quicksort algorithm of section 75
what is the averagecase cost of insertion sort when record i is processed
the number of times through the inner for loop depends on how far out of order
the record is in particular the inner for loop is executed once for each key greater
than the key of record i that appears in array positions 0 through i1 for example
in the leftmost column of figure 71 the value 15 is preceded by five values greater
than 15 each such occurrence is called an inversion the number of inversions
ie the number of values greater than a given value that occur prior to it in the
array will determine the number of comparisons and swaps that must take place
we need to determine what the average number of inversions will be for the record
in position i we expect on average that half of the keys in the first i  1 array
positions will have a value greater than that of the key at position i thus the
average case should be about half the cost of the worst case or around n2 4 which
is still n2  so the average case is no better than the worst case in asymptotic
complexity
counting comparisons or swaps yields similar results each time through the
inner for loop yields both a comparison and a swap except the last ie the
comparison that fails the inner for loops test which has no swap thus the
number of swaps for the entire sort operation is n  1 less than the number of
comparisons this is 0 in the best case and n2  in the average and worst cases
722

bubble sort

our next sorting algorithm is called bubble sort bubble sort is often taught to
novice programmers in introductory computer science courses this is unfortunate
because bubble sort has no redeeming features whatsoever it is a relatively slow
sort it is no easier to understand than insertion sort it does not correspond to any
intuitive counterpart in everyday use and it has a poor bestcase running time
however bubble sort can serve as the inspiration for a better sorting algorithm that
will be presented in section 723
bubble sort consists of a simple double for loop the first iteration of the
inner for loop moves through the record array from bottom to top comparing
adjacent keys if the lowerindexed keys value is greater than its higherindexed


$$@@$$PAGE: 255
236

chap 7 internal sorting

42
20
17
13
28
14
23
15

i0

1

2

3

4

5

6

13
42
20
17
14
28
15
23

13
14
42
20
17
15
28
23

13
14
15
42
20
17
23
28

13
14
15
17
42
20
23
28

13
14
15
17
20
42
23
28

13
14
15
17
20
23
42
28

13
14
15
17
20
23
28
42

figure 72 an illustration of bubble sort each column shows the array after
the iteration with the indicated value of i in the outer for loop values above the
line in each column have been sorted arrows indicate the swaps that take place
during a given iteration

neighbor then the two values are swapped once the smallest value is encountered
this process will cause it to bubble up to the top of the array the second pass
through the array repeats this process however because we know that the smallest
value reached the top of the array on the first pass there is no need to compare
the top two elements on the second pass likewise each succeeding pass through
the array compares adjacent elements looking at one less value than the preceding
pass figure 72 illustrates bubble sort a c implementation is as follows
template typename e typename comp
void bubsorte a int n   bubble sort
for int i0 in1 i
 bubble up ith record
for int jn1 ji j
if compprioraj aj1
swapa j j1


determining bubble sorts number of comparisons is easy regardless of the
arrangement of the values in the array the number of comparisons made by the
inner for loop is always i leading to a total cost of
n
x

i  n2 2  n2 

i1

bubble sorts running time is roughly the same in the best average and worst
cases
the number of swaps required depends on how often a value is less than the
one immediately preceding it in the array we can expect this to occur for about
half the comparisons in the average case leading to n2  for the expected number
of swaps the actual number of swaps performed by bubble sort will be identical
to that performed by insertion sort


$$@@$$PAGE: 256
sec 72 three

n2  sorting algorithms

42
20
17
13
28
14
23
15

237

i0

1

2

3

4

5

6

13
20
17
42
28
14
23
15

13
14
17
42
28
20
23
15

13
14
15
42
28
20
23
17

13
14
15
17
28
20
23
42

13
14
15
17
20
28
23
42

13
14
15
17
20
23
28
42

13
14
15
17
20
23
28
42

figure 73 an example of selection sort each column shows the array after the
iteration with the indicated value of i in the outer for loop numbers above the
line in each column have been sorted and are in their final positions

723

selection sort

consider again the problem of sorting a pile of phone bills for the past year another intuitive approach might be to look through the pile until you find the bill for
january and pull that out then look through the remaining pile until you find the
bill for february and add that behind january proceed through the evershrinking
pile of bills to select the next one in order until you are done this is the inspiration
for our last n2  sort called selection sort the ith pass of selection sort selects the ith smallest key in the array placing that record into position i in other
words selection sort first finds the smallest key in an unsorted list then the second
smallest and so on its unique feature is that there are few record swaps to find
the next smallest key value requires searching through the entire unsorted portion
of the array but only one swap is required to put the record in place thus the total
number of swaps required will be n  1 we get the last record in place for free
figure 73 illustrates selection sort below is a c implementation
template typename e typename comp
void selsorte a int n   selection sort
for int i0 in1 i 
 select ith record
int lowindex  i
 remember its index
for int jn1 ji j
 find the least value
if compprioraj alowindex
lowindex  j
 put it in place
swapa i lowindex



selection sort as written here is essentially a bubble sort except that rather
than repeatedly swapping adjacent values to get the next smallest record into place
we instead remember the position of the element to be selected and do one swap
at the end thus the number of comparisons is still n2  but the number of
swaps is much less than that required by bubble sort selection sort is particularly


$$@@$$PAGE: 257
238

chap 7 internal sorting

key  42

key  42

key  5

key  5

key  23

key  23

key  10

key  10

a

b

figure 74 an example of swapping pointers to records a a series of four
records the record with key value 42 comes before the record with key value 5
b the four records after the top two pointers have been swapped now the record
with key value 5 comes before the record with key value 42

advantageous when the cost to do a swap is high for example when the elements
are long strings or other large records selection sort is more efficient than bubble
sort by a constant factor in most other situations as well
there is another approach to keeping the cost of swapping records low that
can be used by any sorting algorithm even when the records are large this is
to have each element of the array store a pointer to a record rather than store the
record itself in this implementation a swap operation need only exchange the
pointer values the records themselves do not move this technique is illustrated
by figure 74 additional space is needed to store the pointers but the return is a
faster swap operation
724

the cost of exchange sorting

figure 75 summarizes the cost of insertion bubble and selection sort in terms of
their required number of comparisons and swaps1 in the best average and worst
cases the running time for each of these sorts is n2  in the average and worst
cases
the remaining sorting algorithms presented in this chapter are significantly better than these three under typical conditions but before continuing on it is instructive to investigate what makes these three sorts so slow the crucial bottleneck
is that only adjacent records are compared thus comparisons and moves in all
but selection sort are by single steps swapping adjacent records is called an exchange thus these sorts are sometimes referred to as exchange sorts the cost
of any exchange sort can be at best the total number of steps that the records in the
1

there is a slight anomaly with selection sort the supposed advantage for selection sort is its
low number of swaps required yet selection sorts bestcase number of swaps is worse than that for
insertion sort or bubble sort this is because the implementation given for selection sort does not
avoid a swap in the case where record i is already in position i one could put in a test to avoid
swapping in this situation but it usually takes more time to do the tests than would be saved by
avoiding such swaps


$$@@$$PAGE: 258
239

sec 73 shellsort

insertion

bubble

selection

comparisons
best case
average case
worst case

n
n2 
n2 

n2 
n2 
n2 

n2 
n2 
n2 

swaps
best case
average case
worst case

0
n2 
n2 

0
n2 
n2 

n
n
n

figure 75 a comparison of the asymptotic complexities for three simple sorting
algorithms

array must move to reach their correct location ie the number of inversions for
each record
what is the average number of inversions consider a list l containing n values define lr to be l in reverse l has nn12 distinct pairs of values each of
which could potentially be an inversion each such pair must either be an inversion
in l or in lr  thus the total number of inversions in l and lr together is exactly
nn  12 for an average of nn  14 per list we therefore know with certainty
that any sorting algorithm which limits comparisons to adjacent items will cost at
least nn  14  n2  in the average case

73

shellsort

the next sorting algorithm that we consider is called shellsort named after its
inventor dl shell it is also sometimes called the diminishing increment sort
unlike insertion and selection sort there is no real life intuitive equivalent to shellsort unlike the exchange sorts shellsort makes comparisons and swaps between
nonadjacent elements shellsort also exploits the bestcase performance of insertion sort shellsorts strategy is to make the list mostly sorted so that a final
insertion sort can finish the job when properly implemented shellsort will give
substantially better performance than n2  in the worst case
shellsort uses a process that forms the basis for many of the sorts presented
in the following sections break the list into sublists sort them then recombine
the sublists shellsort breaks the array of elements into virtual sublists each
sublist is sorted using an insertion sort another group of sublists is then chosen
and sorted and so on
during each iteration shellsort breaks the list into disjoint sublists so that each
element in a sublist is a fixed number of positions apart for example let us assume for convenience that n the number of values to be sorted is a power of two
one possible implementation of shellsort will begin by breaking the list into n2


$$@@$$PAGE: 259
240

chap 7 internal sorting

59 20 17 13 28 14 23 83 36 98 11 70 65 41 42 15

36 20 11 13 28 14 23 15 59 98 17 70 65 41 42 83

28 14 11 13 36 20 17 15 59 41 23 70 65 98 42 83

11 13 17 14 23 15 28 20 36 41 42 70 59 83 65 98

11 13 14 15 17 20 23 28 36 41 42 59 65 70 83 98

figure 76 an example of shellsort sixteen items are sorted in four passes
the first pass sorts 8 sublists of size 2 and increment 8 the second pass sorts
4 sublists of size 4 and increment 4 the third pass sorts 2 sublists of size 8 and
increment 2 the fourth pass sorts 1 list of size 16 and increment 1 a regular
insertion sort

sublists of 2 elements each where the array index of the 2 elements in each sublist
differs by n2 if there are 16 elements in the array indexed from 0 to 15 there
would initially be 8 sublists of 2 elements each the first sublist would be the elements in positions 0 and 8 the second in positions 1 and 9 and so on each list of
two elements is sorted using insertion sort
the second pass of shellsort looks at fewer bigger lists for our example the
second pass would have n4 lists of size 4 with the elements in the list being n4
positions apart thus the second pass would have as its first sublist the 4 elements
in positions 0 4 8 and 12 the second sublist would have elements in positions 1
5 9 and 13 and so on each sublist of four elements would also be sorted using
an insertion sort
the third pass would be made on two lists one consisting of the odd positions
and the other consisting of the even positions
the culminating pass in this example would be a normal insertion sort of all
elements figure 76 illustrates the process for an array of 16 values where the sizes
of the increments the distances between elements on the successive passes are 8
4 2 and 1 figure 77 presents a c implementation for shellsort
shellsort will work correctly regardless of the size of the increments provided
that the final pass has increment 1 ie provided the final pass is a regular insertion
sort if shellsort will always conclude with a regular insertion sort then how
can it be any improvement on insertion sort the expectation is that each of the
relatively cheap sublist sorts will make the list more sorted than it was before


$$@@$$PAGE: 260
241

sec 74 mergesort

 modified version of insertion sort for varying increments
template typename e typename comp
void inssort2e a int n int incr 
for int iincr in iincr
for int ji jincr 
compprioraj ajincr jincr
swapa j jincr

template typename e typename comp
void shellsorte a int n   shellsort
for int in2 i2 i2
 for each increment
for int j0 ji j
 sort each sublist
inssort2ecompaj nj i
inssort2ecompa n 1

figure 77 an implementation for shell sort

it is not necessarily the case that this will be true but it is almost always true in
practice when the final insertion sort is conducted the list should be almost
sorted yielding a relatively cheap final insertion sort pass
some choices for increments will make shellsort run more efficiently than others in particular the choice of increments described above 2k  2k1   2 1
turns out to be relatively inefficient a better choice is the following series based
on division by three  121 40 13 4 1
the analysis of shellsort is difficult so we must accept without proof that
the averagecase performance of shellsort for divisions by three increments
is on15  other choices for the increment series can reduce this upper bound
somewhat thus shellsort is substantially better than insertion sort or any of the
n2  sorts presented in section 72 in fact shellsort is not terrible when compared with the asymptotically better sorts to be presented whenever n is of medium
size thought is tends to be a little slower than these other algorithms when they
are well implemented shellsort illustrates how we can sometimes exploit the special properties of an algorithm in this case insertion sort even if in general that
algorithm is unacceptably slow

74

mergesort

a natural approach to problem solving is divide and conquer in terms of sorting
we might consider breaking the list to be sorted into pieces process the pieces and
then put them back together somehow a simple way to do this would be to split
the list in half sort the halves and then merge the sorted halves together this is
the idea behind mergesort


$$@@$$PAGE: 261
242

chap 7 internal sorting

36 20 17 13 28 14 23 15
20 36 13 17 14 28 15 23
13 17 20 36 14 15 23 28
13 14 15 17 20 23 28 36
figure 78 an illustration of mergesort the first row shows eight numbers that
are to be sorted mergesort will recursively subdivide the list into sublists of one
element each then recombine the sublists the second row shows the four sublists
of size 2 created by the first merging pass the third row shows the two sublists
of size 4 created by the next merging pass on the sublists of row 2 the last row
shows the final sorted list created by merging the two sublists of row 3

mergesort is one of the simplest sorting algorithms conceptually and has good
performance both in the asymptotic sense and in empirical running time surprisingly even though it is based on a simple concept it is relatively difficult to implement in practice figure 78 illustrates mergesort a pseudocode sketch of
mergesort is as follows
list mergesortlist inlist 
if inlistlength  1 return inlist
list l1  half of the items from inlist
list l2  other half of the items from inlist
return mergemergesortl1 mergesortl2


before discussing how to implement mergesort we will first examine the merge
function merging two sorted sublists is quite simple function merge examines
the first element of each sublist and picks the smaller value as the smallest element
overall this smaller value is removed from its sublist and placed into the output
list merging continues in this way comparing the front elements of the sublists and
continually appending the smaller to the output list until no more input elements
remain
implementing mergesort presents a number of technical difficulties the first
decision is how to represent the lists mergesort lends itself well to sorting a singly
linked list because merging does not require random access to the list elements
thus mergesort is the method of choice when the input is in the form of a linked
list implementing merge for linked lists is straightforward because we need only
remove items from the front of the input lists and append items to the output list
breaking the input list into two equal halves presents some difficulty ideally we
would just break the lists into front and back halves however even if we know the
length of the list in advance it would still be necessary to traverse halfway down
the linked list to reach the beginning of the second half a simpler method which
does not rely on knowing the length of the list in advance assigns elements of the


$$@@$$PAGE: 262
sec 74 mergesort

243

template typename e typename comp
void mergesorte a e temp int left int right 
if left  right return
 list of one element
int mid  leftright2
mergesortecompa temp left mid
mergesortecompa temp mid1 right
for int ileft iright i
 copy subarray to temp
tempi  ai
 do the merge operation back to a
int i1  left int i2  mid  1
for int currleft currright curr 
if i1  mid1
 left sublist exhausted
acurr  tempi2
else if i2  right  right sublist exhausted
acurr  tempi1
else if comppriortempi1 tempi2
acurr  tempi1
else acurr  tempi2


figure 79 standard implementation for mergesort

input list alternating between the two sublists the first element is assigned to the
first sublist the second element to the second sublist the third to first sublist the
fourth to the second sublist and so on this requires one complete pass through
the input list to build the sublists
when the input to mergesort is an array splitting input into two subarrays is
easy if we know the array bounds merging is also easy if we merge the subarrays
into a second array note that this approach requires twice the amount of space
as any of the sorting methods presented so far which is a serious disadvantage for
mergesort it is possible to merge the subarrays without using a second array but
this is extremely difficult to do efficiently and is not really practical merging the
two subarrays into a second array while simple to implement presents another difficulty the merge process ends with the sorted list in the auxiliary array consider
how the recursive nature of mergesort breaks the original array into subarrays as
shown in figure 78 mergesort is recursively called until subarrays of size 1 have
been created requiring log n levels of recursion these subarrays are merged into
subarrays of size 2 which are in turn merged into subarrays of size 4 and so on
we need to avoid having each merge operation require a new array with some
difficulty an algorithm can be devised that alternates between two arrays a much
simpler approach is to copy the sorted sublists to the auxiliary array first and then
merge them back to the original array figure 79 shows a complete implementation
for mergesort following this approach
an optimized mergesort implementation is shown in figure 710 it reverses
the order of the second subarray during the initial copy now the current positions
of the two subarrays work inwards from the ends allowing the end of each subarray


$$@@$$PAGE: 263
244

chap 7 internal sorting

template typename e typename comp
void mergesorte a e temp int left int right 
if rightleft  threshold   small list
inssortecompaleft rightleft1
return

int i j k mid  leftright2
mergesortecompa temp left mid
mergesortecompa temp mid1 right
 do the merge operation first copy 2 halves to temp
for imid ileft i tempi  ai
for j1 jrightmid j temprightj1  ajmid
 merge sublists back to a
for ileftjrightkleft kright k
if comppriortempi tempj ak  tempi
else ak  tempj

figure 710 optimized implementation for mergesort

to act as a sentinel for the other unlike the previous implementation no test is
needed to check for when one of the two subarrays becomes empty this version
also uses insertion sort to sort small subarrays
analysis of mergesort is straightforward despite the fact that it is a recursive
algorithm the merging part takes time i where i is the total length of the two
subarrays being merged the array to be sorted is repeatedly split in half until
subarrays of size 1 are reached at which time they are merged to be of size 2 these
merged to subarrays of size 4 and so on as shown in figure 78 thus the depth
of the recursion is log n for n elements assume for simplicity that n is a power
of two the first level of recursion can be thought of as working on one array of
size n the next level working on two arrays of size n2 the next on four arrays
of size n4 and so on the bottom of the recursion has n arrays of size 1 thus
n arrays of size 1 are merged requiring n total steps n2 arrays of size 2
again requiring n total steps n4 arrays of size 4 and so on at each of the
log n levels of recursion n work is done for a total cost of n log n this
cost is unaffected by the relative order of the values being sorted thus this analysis
holds for the best average and worst cases

75

quicksort

while mergesort uses the most obvious form of divide and conquer split the list in
half then sort the halves it is not the only way that we can break down the sorting
problem and we saw that doing the merge step for mergesort when using an array
implementation is not so easy so perhaps a different divide and conquer strategy
might turn out to be more efficient


$$@@$$PAGE: 264
sec 75 quicksort

245

quicksort is aptly named because when properly implemented it is the fastest
known generalpurpose inmemory sorting algorithm in the average case it does
not require the extra array needed by mergesort so it is space efficient as well
quicksort is widely used and is typically the algorithm implemented in a library
sort routine such as the unix qsort function interestingly quicksort is hampered by exceedingly poor worstcase performance thus making it inappropriate
for certain applications
before we get to quicksort consider for a moment the practicality of using a
binary search tree for sorting you could insert all of the values to be sorted into
the bst one by one then traverse the completed tree using an inorder traversal
the output would form a sorted list this approach has a number of drawbacks
including the extra space required by bst pointers and the amount of time required
to insert nodes into the tree however this method introduces some interesting
ideas first the root of the bst ie the first node inserted splits the list into two
sublists the left subtree contains those values in the list less than the root value
while the right subtree contains those values in the list greater than or equal to the
root value thus the bst implicitly implements a divide and conquer approach
to sorting the left and right subtrees quicksort implements this concept in a much
more efficient way
quicksort first selects a value called the pivot this is conceptually like the
root nodes value in the bst assume that the input array contains k values less
than the pivot the records are then rearranged in such a way that the k values
less than the pivot are placed in the first or leftmost k positions in the array and
the values greater than or equal to the pivot are placed in the last or rightmost
n  k positions this is called a partition of the array the values placed in a given
partition need not and typically will not be sorted with respect to each other all
that is required is that all values end up in the correct partition the pivot value itself
is placed in position k quicksort then proceeds to sort the resulting subarrays now
on either side of the pivot one of size k and the other of size n  k  1 how are
these values sorted because quicksort is such a good algorithm using quicksort
on the subarrays would be appropriate
unlike some of the sorts that we have seen earlier in this chapter quicksort
might not seem very natural in that it is not an approach that a person is likely to
use to sort real objects but it should not be too surprising that a really efficient sort
for huge numbers of abstract objects on a computer would be rather different from
our experiences with sorting a relatively few physical objects
the c code for quicksort is shown in figure 711 parameters i and j
define the left and right indices respectively for the subarray being sorted the
initial call to quicksort would be qsortarray 0 n1
function partition will move records to the appropriate partition and then
return k the first position in the right partition note that the pivot value is initially


$$@@$$PAGE: 265
246

chap 7 internal sorting

template typename e typename comp
void qsorte a int i int j   quicksort
if j  i return  dont sort 0 or 1 element
int pivotindex  findpivota i j
swapa pivotindex j
 put pivot at end
 k will be the first position in the right subarray
int k  partitionecompa i1 j aj
swapa k j
 put pivot in place
qsortecompa i k1
qsortecompa k1 j

figure 711 implementation for quicksort

placed at the end of the array position j thus partition must not affect the
value of array position j after partitioning the pivot value is placed in position k
which is its correct position in the final sorted array by doing so we guarantee
that at least one value the pivot will not be processed in the recursive calls to
qsort even if a bad pivot is selected yielding a completely empty partition to
one side of the pivot the larger partition will contain at most n  1 elements
selecting a pivot can be done in many ways the simplest is to use the first
key however if the input is sorted or reverse sorted this will produce a poor
partitioning with all values to one side of the pivot it is better to pick a value
at random thereby reducing the chance of a bad input order affecting the sort
unfortunately using a random number generator is relatively expensive and we
can do nearly as well by selecting the middle position in the array here is a simple
findpivot function
template typename e
inline int findpivote a int i int j
 return ij2 

we now turn to function partition if we knew in advance how many keys
are less than the pivot partition could simply copy elements with key values
less than the pivot to the low end of the array and elements with larger keys to
the high end because we do not know in advance how many keys are less than
the pivot we use a clever algorithm that moves indices inwards from the ends of
the subarray swapping values as necessary until the two indices meet figure 712
shows a c implementation for the partition step
figure 713 illustrates partition initially variables l and r are immediately outside the actual bounds of the subarray being partitioned each pass through
the outer do loop moves the counters l and r inwards until eventually they meet
note that at each iteration of the inner while loops the bounds are moved prior
to checking against the pivot value this ensures that progress is made by each
while loop even when the two values swapped on the last iteration of the do
loop were equal to the pivot also note the check that r  l in the second while


$$@@$$PAGE: 266
247

sec 75 quicksort

template typename e typename comp
inline int partitione a int l int r e pivot 
do 
 move the bounds inward until they meet
while compprioral pivot  move l right and
while l  r  comppriorpivot ar  r left
swapa l r
 swap outofplace values
 while l  r
 stop when they cross
return l
 return first position in right partition

figure 712 the quicksort partition implementation

initial

72 6

57 88 85 42 83 73 48 60
r

pass 1

72 6
l

57 88 85 42 83 73 48 60
r

swap 1

48 6
l

57 88 85 42 83 73 72 60
r

pass 2

48 6

57 88 85 42 83 73 72 60
l
r

swap 2

48 6

57 42 85 88 83 73 72 60
l
r

pass 3

48 6

57 42 85 88 83 73 72 60
lr

l

figure 713 the quicksort partition step the first row shows the initial positions for a collection of ten key values the pivot value is 60 which has been
swapped to the end of the array the do loop makes three iterations each time
moving counters l and r inwards until they meet in the third pass in the end
the left partition contains four values and the right partition contains six values
function qsort will place the pivot value into position 4

loop this ensures that r does not run off the low end of the partition in the case
where the pivot is the least value in that partition function partition returns
the first index of the right partition so that the subarray bound for the recursive
calls to qsort can be determined figure 714 illustrates the complete quicksort
algorithm
to analyze quicksort we first analyze the findpivot and partition
functions operating on a subarray of length k clearly findpivot takes constant time function partition contains a do loop with two nested while
loops the total cost of the partition operation is constrained by how far l and r
can move inwards in particular these two bounds variables together can move a


$$@@$$PAGE: 267
248

chap 7 internal sorting

72 6

57 88 60 42 83 73 48 85
pivot  60

48 6

57 42 60 88 83 73 72 85

pivot  6

6

pivot  73

42 57 48

72 73 85 88 83

pivot  57
pivot  42

42 48

pivot  88

57

85 83 88 pivot  85

42 48

6

83 85

42 48 57 60 72 73 83 85 88
final sorted array

figure 714 an illustration of quicksort

total of s steps for a subarray of length s however this does not directly tell us
how much work is done by the nested while loops the do loop as a whole is
guaranteed to move both l and r inward at least one position on each first pass
each while loop moves its variable at least once except in the special case where
r is at the left edge of the array but this can happen only once thus we see that
the do loop can be executed at most s times the total amount of work done moving
l and r is s and each while loop can fail its test at most s times the total work
for the entire partition function is therefore s
knowing the cost of findpivot and partition we can determine the
cost of quicksort we begin with a worstcase analysis the worst case will occur
when the pivot does a poor job of breaking the array that is when there are no
elements in one partition and n  1 elements in the other in this case the divide
and conquer strategy has done a poor job of dividing so the conquer phase will
work on a subproblem only one less than the size of the original problem if this
happens at each partition step then the total cost of the algorithm will be
n
x

k  n2 

k1

in the worst case quicksort is n2  this is terrible no better than bubble
sort2 when will this worst case occur only when each pivot yields a bad partitioning of the array if the pivot values are selected at random then this is extremely
unlikely to happen when selecting the middle position of the current subarray it
2

the worst insult that i can think of for a sorting algorithm


$$@@$$PAGE: 268
249

sec 75 quicksort

is still unlikely to happen it does not take many good partitionings for quicksort
to work fairly well
quicksorts best case occurs when findpivot always breaks the array into
two equal halves quicksort repeatedly splits the array into smaller partitions as
shown in figure 714 in the best case the result will be log n levels of partitions
with the top level having one array of size n the second level two arrays of size n2
the next with four arrays of size n4 and so on thus at each level all partition
steps for that level do a total of n work for an overall cost of n log n work when
quicksort finds perfect pivots
quicksorts averagecase behavior falls somewhere between the extremes of
worst and best case averagecase analysis considers the cost for all possible arrangements of input summing the costs and dividing by the number of cases we
make one reasonable simplifying assumption at each partition step the pivot is
equally likely to end in any position in the sorted array in other words the pivot
is equally likely to break an array into partitions of sizes 0 and n1 or 1 and n2
and so on
given this assumption the averagecase cost is computed from the following
equation
n1

1x
tn  cn 
tk  tn  1  k
n

t0  t1  c

k0

this equation is in the form of a recurrence relation recurrence relations are
discussed in chapters 2 and 14 and this one is solved in section 1424 this
equation says that there is one chance in n that the pivot breaks the array into
subarrays of size 0 and n  1 one chance in n that the pivot breaks the array into
subarrays of size 1 and n  2 and so on the expression tk  tn  1  k is
the cost for the two recursive calls to quicksort on two arrays of size k and n1k
the initial cn term is the cost of doing the findpivot and partition steps for
some constant c the closedform solution to this recurrence relation is n log n
thus quicksort has averagecase cost n log n
this is an unusual situation that the average case cost and the worst case cost
have asymptotically different growth rates consider what average case actually
means we compute an average cost for inputs of size n by summing up for every
possible input of size n the product of the running time cost of that input times the
probability that that input will occur to simplify things we assumed that every
permutation is equally likely to occur thus finding the average means summing
up the cost for every permutation and dividing by the number of inputs n we
know that some of these n inputs cost on2  but the sum of all the permutation
costs has to be non log n given the extremely high cost of the worst inputs
there must be very few of them in fact there cannot be a constant fraction of the
inputs with cost on2  even say 1 of the inputs with cost on2  would lead to


$$@@$$PAGE: 269
250

chap 7 internal sorting

an average cost of on2  thus as n grows the fraction of inputs with high cost
must be going toward a limit of zero we can conclude that quicksort will have
good behavior if we can avoid those very few bad input permutations
the running time for quicksort can be improved by a constant factor and
much study has gone into optimizing this algorithm the most obvious place for
improvement is the findpivot function quicksorts worst case arises when the
pivot does a poor job of splitting the array into equal size subarrays if we are
willing to do more work searching for a better pivot the effects of a bad pivot can
be decreased or even eliminated one good choice is to use the median of three
algorithm which uses as a pivot the middle of three randomly selected values
using a random number generator to choose the positions is relatively expensive
so a common compromise is to look at the first middle and last positions of the
current subarray however our simple findpivot function that takes the middle
value as its pivot has the virtue of making it highly unlikely to get a bad input by
chance and it is quite cheap to implement this is in sharp contrast to selecting
the first or last element as the pivot which would yield bad performance for many
permutations that are nearly sorted or nearly reverse sorted
a significant improvement can be gained by recognizing that quicksort is relatively slow when n is small this might not seem to be relevant if most of the
time we sort large arrays nor should it matter how long quicksort takes in the
rare instance when a small array is sorted because it will be fast anyway but you
should notice that quicksort itself sorts many many small arrays this happens as
a natural byproduct of the divide and conquer approach
a simple improvement might then be to replace quicksort with a faster sort
for small numbers say insertion sort or selection sort however there is an even
better  and still simpler  optimization when quicksort partitions are below
a certain size do nothing the values within that partition will be out of order
however we do know that all values in the array to the left of the partition are
smaller than all values in the partition all values in the array to the right of the
partition are greater than all values in the partition thus even if quicksort only
gets the values to nearly the right locations the array will be close to sorted this
is an ideal situation in which to take advantage of the bestcase performance of
insertion sort the final step is a single call to insertion sort to process the entire
array putting the elements into final sorted order empirical testing shows that
the subarrays should be left unordered whenever they get down to nine or fewer
elements
the last speedup to be considered reduces the cost of making recursive calls
quicksort is inherently recursive because each quicksort operation must sort two
sublists thus there is no simple way to turn quicksort into an iterative algorithm
however quicksort can be implemented using a stack to imitate recursion as the
amount of information that must be stored is small we need not store copies of a


$$@@$$PAGE: 270
sec 76 heapsort

251

subarray only the subarray bounds furthermore the stack depth can be kept small
if care is taken on the order in which quicksorts recursive calls are executed we
can also place the code for findpivot and partition inline to eliminate the
remaining function calls note however that by not processing sublists of size nine
or less as suggested above about three quarters of the function calls will already
have been eliminated thus eliminating the remaining function calls will yield
only a modest speedup

76

heapsort

our discussion of quicksort began by considering the practicality of using a binary
search tree for sorting the bst requires more space than the other sorting methods and will be slower than quicksort or mergesort due to the relative expense of
inserting values into the tree there is also the possibility that the bst might be unbalanced leading to a n2  worstcase running time subtree balance in the bst
is closely related to quicksorts partition step quicksorts pivot serves roughly the
same purpose as the bst root value in that the left partition subtree stores values less than the pivot root value while the right partition subtree stores values
greater than or equal to the pivot root
a good sorting algorithm can be devised based on a tree structure more suited
to the purpose in particular we would like the tree to be balanced space efficient
and fast the algorithm should take advantage of the fact that sorting is a specialpurpose application in that all of the values to be stored are available at the start
this means that we do not necessarily need to insert one value at a time into the
tree structure
heapsort is based on the heap data structure presented in section 55 heapsort
has all of the advantages just listed the complete binary tree is balanced its array
representation is space efficient and we can load all values into the tree at once
taking advantage of the efficient buildheap function the asymptotic performance of heapsort is n log n in the best average and worst cases it is not as
fast as quicksort in the average case by a constant factor but heapsort has special
properties that will make it particularly useful when sorting data sets too large to fit
in main memory as discussed in chapter 8
a sorting algorithm based on maxheaps is quite straightforward first we use
the heap building algorithm of section 55 to convert the array into maxheap order
then we repeatedly remove the maximum value from the heap restoring the heap
property each time that we do so until the heap is empty note that each time
we remove the maximum element from the heap it is placed at the end of the
array assume the n elements are stored in array positions 0 through n  1 after
removing the maximum value from the heap and readjusting the maximum value
will now be placed in position n  1 of the array the heap is now considered to be


$$@@$$PAGE: 271
252

chap 7 internal sorting

of size n  1 removing the new maximum root value places the second largest
value in position n  2 of the array after removing each of the remaining values in
turn the array will be properly sorted from least to greatest this is why heapsort
uses a maxheap rather than a minheap as might have been expected figure 715
illustrates heapsort the complete c implementation is as follows
template typename e typename comp
void heapsorte a int n   heapsort
e maxval
heapecomp ha n n
 build the heap
for int i0 in i
 now sort
maxval  hremovefirst
 place maxval at end


because building the heap takes n time see section 55 and because
n deletions of the maximum element each take log n time we see that the entire heapsort operation takes n log n time in the worst average and best cases
while typically slower than quicksort by a constant factor heapsort has one special advantage over the other sorts studied so far building the heap is relatively
cheap requiring n time removing the maximum element from the heap requires log n time thus if we wish to find the k largest elements in an array
we can do so in time n  k log n if k is small this is a substantial improvement over the time required to find the k largest elements using one of the other
sorting methods described earlier many of which would require sorting all of the
array first one situation where we are able to take advantage of this concept is
in the implementation of kruskals minimumcost spanning tree mst algorithm
of section 1152 that algorithm requires that edges be visited in ascending order
so use a minheap but this process stops as soon as the mst is complete thus
only a relatively small fraction of the edges need be sorted

77

binsort and radix sort

imagine that for the past year as you paid your various bills you then simply piled
all the paperwork onto the top of a table somewhere now the year has ended and
its time to sort all of these papers by what the bill was for phone electricity rent
etc and date a pretty natural approach is to make some space on the floor and as
you go through the pile of papers put the phone bills into one pile the electric bills
into another pile and so on once this initial assignment of bills to piles is done in
one pass you can sort each pile by date relatively quickly because they are each
fairly small this is the basic idea behind a binsort
section 39 presented the following code fragment to sort a permutation of the
numbers 0 through n  1
for i0 in i
bai  ai


$$@@$$PAGE: 272
253

sec 77 binsort and radix sort

73

original numbers

73 6

6

57 88 60 42 83 72 48 85
88

57
42

60

83

72 48 85
88

build heap

88 85 83 72 73 42 57 6

83

85

48 60
72
6

73

85
73

48 88
72
6

83
60

42

57

48
83

remove 85

83 73 57 72 60 42 48 6

57

48 60

remove 88

85 73 83 72 60 42 57 6

42

73

85 88
72

57
60

42

48

6
73

remove 83

73 72 57 6

72

60 42 48 83 85 88
6

57
60

42

48

figure 715 an illustration of heapsort the top row shows the values in their
original order the second row shows the values after building the heap the
third row shows the result of the first removefirst operation on key value 88
note that 88 is now at the end of the array the fourth row shows the result of the
second removefirst operation on key value 85 the fifth row shows the result
of the third removefirst operation on key value 83 at this point the last
three positions of the array hold the three greatest values in sorted order heapsort
continues in this manner until the entire array is sorted


$$@@$$PAGE: 273
254

chap 7 internal sorting

template typename e class getkey
void binsorte a int n 
liste bmaxkeyvalue
e item
for int i0 in i baiappendgetkeykeyai
for int i0 imaxkeyvalue i
for bisetstart bigetvalueitem binext
outputitem

figure 716 the extended binsort algorithm

here the key value is used to determine the position for a record in the final sorted
array this is the most basic example of a binsort where key values are used
to assign records to bins this algorithm is extremely efficient taking n time
regardless of the initial ordering of the keys this is far better than the performance
of any sorting algorithm that we have seen so far the only problem is that this
algorithm has limited use because it works only for a permutation of the numbers
from 0 to n  1
we can extend this simple binsort algorithm to be more useful because binsort
must perform direct computation on the key value as opposed to just asking which
of two records comes first as our previous sorting algorithms did we will assume
that the records use an integer key type we further assume that it can be extracted
from a record using the key method supplied by a template parameter class named
getkey
the simplest extension is to allow for duplicate values among the keys this
can be done by turning array slots into arbitrarylength bins by turning b into an
array of linked lists in this way all records with key value i can be placed in bin
bi a second extension allows for a key range greater than n for example
a set of n records might have keys in the range 1 to 2n the only requirement is
that each possible key value have a corresponding bin in b the extended binsort
algorithm is shown in figure 716
this version of binsort can sort any collection of records whose key values fall
in the range from 0 to maxkeyvalue1 the total work required is simply that
needed to place each record into the appropriate bin and then take all of the records
out of the bins thus we need to process each record twice for n work
unfortunately there is a crucial oversight in this analysis binsort must also
look at each of the bins to see if it contains a record the algorithm must process
maxkeyvalue bins regardless of how many actually hold records if maxkeyvalue is small compared to n then this is not a great expense suppose that
maxkeyvalue  n2  in this case the total amount of work done will be n 
n2   n2  this results in a poor sorting algorithm and the algorithm becomes
even worse as the disparity between n and maxkeyvalue increases in addition


$$@@$$PAGE: 274
255

sec 77 binsort and radix sort

initial list

27 91

1 97 17 23 84 28 72

first pass
on right digit

second pass
on left digit

0
1

0

1

1

17
23

1

91

2

72

2

3

23

3

4

84

4

5

5

27

8

28

97

5

25

27

28

5

25

6
7

5 67 25

17

9
91
result of first pass
result of second pass 1

67

6

67

7

72

8

84

9

91

97

1 72 23 84 5 25 27 97 17 67 28
5 17 23 25 27 28 67 72 84 91 97

figure 717 an example of radix sort for twelve twodigit numbers in base ten
two passes are required to sort the list

a large key range requires an unacceptably large array b thus even the extended
binsort is useful only for a limited key range
a further generalization to binsort yields a bucket sort each bin is associated
with not just one key but rather a range of key values a bucket sort assigns records
to bins and then relies on some other sorting technique to sort the records within
each bin the hope is that the relatively inexpensive bucketing process will put
only a small number of records in each bin and that a cleanup sort within the
bins will then be relatively cheap
there is a way to keep the number of bins and the related processing small
while allowing the cleanup sort to be based on binsort consider a sequence of
records with keys in the range 0 to 99 if we have ten bins available we can first
assign records to bins by taking their key value modulo 10 thus every key will
be assigned to the bin matching its rightmost decimal digit we can then take these
records from the bins in order and reassign them to the bins on the basis of their
leftmost 10s place digit define values in the range 0 to 9 to have a leftmost digit
of 0 in other words assign the ith record from array a to a bin using the formula
ai10 if we now gather the values from the bins in order the result is a sorted
list figure 717 illustrates this process


$$@@$$PAGE: 275
256

chap 7 internal sorting

template typename e typename getkey
void radixe a e b
int n int k int r int cnt 
 cnti stores number of records in bini
int j
for int i0 rtoi1 ik i rtoir   for k digits
for j0 jr j cntj  0
 initialize cnt
 count the number of records for each bin on this pass
for j0 jn j cntgetkeykeyajrtoir
 index b cntj will be index for last slot of bin j
for j1 jr j cntj  cntj1  cntj
 put records into bins work from bottom of each bin
 since bins fill from bottom j counts downwards
for jn1 j0 j
bcntgetkeykeyajrtoir  aj
for j0 jn j aj  bj

 copy b back to a



figure 718 the radix sort algorithm

in this example we have r  10 bins and n  12 keys in the range 0 to r2  1
the total computation is n because we look at each record and each bin a
constant number of times this is a great improvement over the simple binsort
where the number of bins must be as large as the key range note that the example
uses r  10 so as to make the bin computations easy to visualize records were
placed into bins based on the value of first the rightmost and then the leftmost
decimal digits any number of bins would have worked this is an example of a
radix sort so called because the bin computations are based on the radix or the
base of the key values this sorting algorithm can be extended to any number of
keys in any key range we simply assign records to bins based on the keys digit
values working from the rightmost digit to the leftmost if there are k digits then
this requires that we assign keys to bins k times
as with mergesort an efficient implementation of radix sort is somewhat difficult to achieve in particular we would prefer to sort an array of values and avoid
processing linked lists if we know how many values will be in each bin then an
auxiliary array of size r can be used to hold the bins for example if during the
first pass the 0 bin will receive three records and the 1 bin will receive five records
then we could simply reserve the first three array positions for the 0 bin and the
next five array positions for the 1 bin exactly this approach is taken by the c
implementation of figure 718 at the end of each pass the records are copied back
to the original array


$$@@$$PAGE: 276
sec 77 binsort and radix sort

257

the first inner for loop initializes array cnt the second loop counts the
number of records to be assigned to each bin the third loop sets the values in cnt
to their proper indices within array b note that the index stored in cntj is the
last index for bin j bins are filled from high index to low index the fourth loop
assigns the records to the bins within array b the final loop simply copies the
records back to array a to be ready for the next pass variable rtoi stores ri for
use in bin computation on the ith iteration figure 719 shows how this algorithm
processes the input shown in figure 717
this algorithm requires k passes over the list of n numbers in base r with
n  r work done at each pass thus the total work is nk  rk what is
this in terms of n because r is the size of the base it might be rather small
one could use base 2 or 10 base 26 would be appropriate for sorting character
strings for now we will treat r as a constant value and ignore it for the purpose of
determining asymptotic complexity variable k is related to the key range it is the
maximum number of digits that a key may have in base r in some applications we
can determine k to be of limited size and so might wish to consider it a constant
in this case radix sort is n in the best average and worst cases making it the
sort with best asymptotic complexity that we have studied
is it a reasonable assumption to treat k as a constant or is there some relationship between k and n if the key range is limited and duplicate key values are
common there might be no relationship between k and n to make this distinction
clear use n to denote the number of distinct key values used by the n records
thus n  n because it takes a minimum of logr n base r digits to represent n
distinct key values we know that k  logr n 
now consider the situation in which no keys are duplicated if there are n
unique keys n  n  then it requires n distinct code values to represent them
thus k  logr n because it requires at least log n digits within a constant
factor to distinguish between the n distinct keys k is in log n this yields
an asymptotic complexity of n log n for radix sort to process n distinct key
values
it is possible that the key range is much larger logr n bits is merely the best
case possible for n distinct values thus the logr n estimate for k could be overly
optimistic the moral of this analysis is that for the general case of n distinct key
values radix sort is at best a n log n sorting algorithm
radix sort can be much improved by making base r be as large as possible
consider the case of an integer key value set r  2i for some i in other words
the value of r is related to the number of bits of the key processed on each pass
each time the number of bits is doubled the number of passes is cut in half when
processing an integer key value setting r  256 allows the key to be processed one
byte at a time processing a 32bit key requires only four passes it is not unreasonable on most computers to use r  216  64k resulting in only two passes for


$$@@$$PAGE: 277
258

chap 7 internal sorting

initial input array a

first pass values for count
rtoi  1

27 91 1 97 17 23 84 28 72 5

0

1

2

3

4

5

6

7

8

9

0

2

1

1

1

2

0

4

1

0

8

9

0

1

2

3

4

5

6

7

count array
index positions for array b

0

2

3

4

5

7

7

11 12 12

end of pass 1 array a

91 1

second pass values for count
rtoi  10

72 23 84 5

67 25

25 27 97 17 67 28

0

1

2

3

4

5

6

7

8

9 10 11

0

1

2

3

4

5

6

7

8

9

2

1

4

0

0

0

1

1

1

2

8

9

0

1

2

3

4

5

6

7

count array
index positions for array b

2

3

7

7

7

7

8

9 10 12

end of pass 2 array a

1

5 17 23 25 27 28 67 72 84 91 97

0

1

2

3

4

5

6

7

8

9 10 11

figure 719 an example showing function radix applied to the input of figure 717 row 1 shows the initial values within the input array row 2 shows the
values for array cnt after counting the number of records for each bin row 3
shows the index values stored in array cnt for example cnt0 is 0 indicating no input values are in bin 0 cnt1 is 2 indicating that array b positions 0
and 1 will hold the values for bin 1 cnt2 is 3 indicating that array b position
2 will hold the single value for bin 2 cnt7 is 11 indicating that array b
positions 7 through 10 will hold the four values for bin 7 row 4 shows the results
of the first pass of the radix sort rows 5 through 7 show the equivalent steps for
the second pass


$$@@$$PAGE: 278
sec 78 an empirical comparison of sorting algorithms

259

a 32bit key of course this requires a cnt array of size 64k performance will
be good only if the number of records is close to 64k or greater in other words
the number of records must be large compared to the key size for radix sort to be
efficient in many sorting applications radix sort can be tuned in this way to give
good performance
radix sort depends on the ability to make a fixed number of multiway choices
based on a digit value as well as random access to the bins thus radix sort
might be difficult to implement for certain key types for example if the keys
are real numbers or arbitrary length strings then some care will be necessary in
implementation in particular radix sort will need to be careful about deciding
when the last digit has been found to distinguish among real numbers or the last
character in variable length strings implementing the concept of radix sort with
the trie data structure section 131 is most appropriate for these situations
at this point the perceptive reader might begin to question our earlier assumption that key comparison takes constant time if the keys are normal integer
values stored in say an integer variable what is the size of this variable compared
to n in fact it is almost certain that 32 the number of bits in a standard int variable is greater than log n for any practical computation in this sense comparison
of two long integers requires log n work
computers normally do arithmetic in units of a particular size such as a 32bit
word regardless of the size of the variables comparisons use this native word
size and require a constant amount of time since the comparison is implemented in
hardware in practice comparisons of two 32bit values take constant time even
though 32 is much greater than log n to some extent the truth of the proposition
that there are constant time operations such as integer comparison is in the eye
of the beholder at the gate level of computer architecture individual bits are
compared however constant time comparison for integers is true in practice on
most computers they require a fixed number of machine instructions and we rely
on such assumptions as the basis for our analyses in contrast radix sort must do
several arithmetic calculations on key values each requiring constant time where
the number of such calculations is proportional to the key length thus radix sort
truly does n log n work to process n distinct key values

78

an empirical comparison of sorting algorithms

which sorting algorithm is fastest asymptotic complexity analysis lets us distinguish between n2  and n log n algorithms but it does not help distinguish
between algorithms with the same asymptotic complexity nor does asymptotic
analysis say anything about which algorithm is best for sorting small lists for
answers to these questions we can turn to empirical testing


$$@@$$PAGE: 279
260

chap 7 internal sorting

sort
insertion
bubble
selection
shell
shello
merge
mergeo
quick
quicko
heap
heapo
radix4
radix8

10
00023
00035
00039
00034
00034
00050
00024
00048
00031
00050
00033
00838
00799

100
007
020
012
008
008
010
007
008
006
011
007
081
044

1k
066
225
069
014
012
012
010
011
009
016
011
079
040

10k
6498
27794
7247
199
191
161
131
137
114
208
161
799
399

100k
73810
276910
73560
302
290
193
172
157
136
267
208
799
400

1m
674420
2820680
780000
554
530
219
197
162
143
391
334
808
404

up
004
7064
6976
044
036
083
047
037
032
157
101
797
400

down
12905
10869
6958
079
064
079
066
040
036
156
104
797
399

figure 720 empirical comparison of sorting algorithms run on a 34ghz intel
pentium 4 cpu running linux shellsort quicksort mergesort and heapsort
each are shown with regular and optimized versions radix sort is shown for 4and 8bitperpass versions all times shown are milliseconds

figure 720 shows timing results for actual implementations of the sorting algorithms presented in this chapter the algorithms compared include insertion sort
bubble sort selection sort shellsort quicksort mergesort heapsort and radix
sort shellsort shows both the basic version from section 73 and another with
increments based on division by three mergesort shows both the basic implementation from section 74 and the optimized version including calls to insertion sort
for lists of length below nine for quicksort two versions are compared the basic
implementation from section 75 and an optimized version that does not partition
sublists below length nine with insertion sort performed at the end the first
heapsort version uses the class definitions from section 55 the second version
removes all the class definitions and operates directly on the array using inlined
code for all access functions
except for the rightmost columns the input to each algorithm is a random array
of integers this affects the timing for some of the sorting algorithms for example selection sort is not being used to best advantage because the record size is
small so it does not get the best possible showing the radix sort implementation
certainly takes advantage of this key range in that it does not look at more digits
than necessary on the other hand it was not optimized to use bit shifting instead
of division even though the bases used would permit this
the various sorting algorithms are shown for lists of sizes 10 100 1000
10000 100000 and 1000000 the final two columns of each table show the
performance for the algorithms on inputs of size 10000 where the numbers are
in ascending sorted and descending reverse sorted order respectively these
columns demonstrate bestcase performance for some algorithms and worstcase


$$@@$$PAGE: 280
sec 79 lower bounds for sorting

261

performance for others they also show that for some algorithms the order of
input has little effect
these figures show a number of interesting results as expected the on2 
sorts are quite poor performers for large arrays insertion sort is by far the best of
this group unless the array is already reverse sorted shellsort is clearly superior
to any of these on2  sorts for lists of even 100 elements optimized quicksort is
clearly the best overall algorithm for all but lists of 10 elements even for small
arrays optimized quicksort performs well because it does one partition step before calling insertion sort compared to the other on log n sorts unoptimized
heapsort is quite slow due to the overhead of the class structure when all of this
is stripped away and the algorithm is implemented to manipulate an array directly
it is still somewhat slower than mergesort in general optimizing the various algorithms makes a noticeable improvement for larger array sizes
overall radix sort is a surprisingly poor performer if the code had been tuned
to use bit shifting of the key value it would likely improve substantially but this
would seriously limit the range of element types that the sort could support

79

lower bounds for sorting

this book contains many analyses for algorithms these analyses generally define
the upper and lower bounds for algorithms in their worst and average cases for
many of the algorithms presented so far analysis has been easy this section considers a more difficult task  an analysis for the cost of a problem as opposed to an
algorithm the upper bound for a problem can be defined as the asymptotic cost of
the fastest known algorithm the lower bound defines the best possible efficiency
for any algorithm that solves the problem including algorithms not yet invented
once the upper and lower bounds for the problem meet we know that no future
algorithm can possibly be asymptotically more efficient
a simple estimate for a problems lower bound can be obtained by measuring
the size of the input that must be read and the output that must be written certainly
no algorithm can be more efficient than the problems io time from this we see
that the sorting problem cannot be solved by any algorithm in less than n time
because it takes at least n steps to read and write the n values to be sorted alternatively any sorting algorithm must at least look at every input vale to recognize
whether the input values are in sort order so based on our current knowledge of
sorting algorithms and the size of the input we know that the problem of sorting is
bounded by n and on log n
computer scientists have spent much time devising efficient generalpurpose
sorting algorithms but no one has ever found one that is faster than on log n in
the worst or average cases should we keep searching for a faster sorting algorithm


$$@@$$PAGE: 281
262

chap 7 internal sorting

or can we prove that there is no faster sorting algorithm by finding a tighter lower
bound
this section presents one of the most important and most useful proofs in computer science no sorting algorithm based on key comparisons can possibly be
faster than n log n in the worst case this proof is important for three reasons
first knowing that widely used sorting algorithms are asymptotically optimal is reassuring in particular it means that you need not bang your head against the wall
searching for an on sorting algorithm or at least not one in any way based on key
comparisons second this proof is one of the few nontrivial lowerbounds proofs
that we have for any problem that is this proof provides one of the relatively few
instances where our lower bound is tighter than simply measuring the size of the
input and output as such it provides a useful model for proving lower bounds on
other problems finally knowing a lower bound for sorting gives us a lower bound
in turn for other problems whose solution could be used as the basis for a sorting
algorithm the process of deriving asymptotic bounds for one problem from the
asymptotic bounds of another is called a reduction a concept further explored in
chapter 17
except for the radix sort and binsort all of the sorting algorithms presented
in this chapter make decisions based on the direct comparison of two key values
for example insertion sort sequentially compares the value to be inserted into the
sorted list until a comparison against the next value in the list fails in contrast
radix sort has no direct comparison of key values all decisions are based on the
value of specific digits in the key value so it is possible to take approaches to sorting
that do not involve key comparisons of course radix sort in the end does not
provide a more efficient sorting algorithm than comparisonbased sorting thus
empirical evidence suggests that comparisonbased sorting is a good approach3
the proof that any comparison sort requires n log n comparisons in the
worst case is structured as follows first comparisonbased decisions can be modeled as the branches in a tree this means that any sorting algorithm based on comparisons between records can be viewed as a binary tree whose nodes correspond to
the comparisons and whose branches correspond to the possible outcomes next
the minimum number of leaves in the resulting tree is shown to be the factorial of
n finally the minimum depth of a tree with n leaves is shown to be in n log n
before presenting the proof of an n log n lower bound for sorting we first
must define the concept of a decision tree a decision tree is a binary tree that can
model the processing for any algorithm that makes binary decisions each binary
decision is represented by a branch in the tree for the purpose of modeling sorting
algorithms we count all comparisons of key values as decisions if two keys are
3

the truth is stronger than this statement implies in reality radix sort relies on comparisons as
well and so can be modeled by the technique used in this section the result is an n log n bound
in the general case even for algorithms that look like radix sort


$$@@$$PAGE: 282
263

sec 79 lower bounds for sorting

xyz
xyz yzx
xzy zxy
yxz zyx
yes
yxz
yxz
yzx
zyx
yes

a2a1
yzx zx
yzx
zyx
yes
no
a1a0
zyx zy yzx

a1a0
yx

no

yes

no
xyz
xyz
xzy
zxy

a2a1
yxz
xzy zy
xzy
zxy
yes
no
a1a0
zxy zx xzy

no
xyz

figure 721 decision tree for insertion sort when processing three values labeled x y and z initially stored at positions 0 1 and 2 respectively in input
array a

compared and the first is less than the second then this is modeled as a left branch
in the decision tree in the case where the first value is greater than the second the
algorithm takes the right branch
figure 721 shows the decision tree that models insertion sort on three input
values the first input value is labeled x the second y and the third z they are
initially stored in positions 0 1 and 2 respectively of input array a consider the
possible outputs initially we know nothing about the final positions of the three
values in the sorted output array the correct output could be any permutation of
the input values for three values there are n  6 permutations thus the root
node of the decision tree lists all six permutations that might be the eventual result
of the algorithm
when n  3 the first comparison made by insertion sort is between the second item in the input array y and the first item in the array x there are two
possibilities either the value of y is less than that of x or the value of y is not
less than that of x this decision is modeled by the first branch in the tree if y is
less than x then the left branch should be taken and y must appear before x in the
final output only three of the original six permutations have this property so the
left child of the root lists the three permutations where y appears before x yxz
yzx and zyx likewise if y were not less than x then the right branch would
be taken and only the three permutations in which y appears after x are possible
outcomes xyz xzy and zxy these are listed in the right child of the root
let us assume for the moment that y is less than x and so the left branch is
taken in this case insertion sort swaps the two values at this point the array


$$@@$$PAGE: 283
264

chap 7 internal sorting

stores yxz thus in figure 721 the left child of the root shows yxz above the
line next the third value in the array is compared against the second ie z is
compared with x again there are two possibilities if z is less than x then these
items should be swapped the left branch if z is not less than x then insertion
sort is complete the right branch
note that the right branch reaches a leaf node and that this leaf node contains
only one permutation yxz this means that only permutation yxz can be the
outcome based on the results of the decisions taken to reach this node in other
words insertion sort has found the single permutation of the original input that
yields a sorted list likewise if the second decision resulted in taking the left
branch a third comparison regardless of the outcome yields nodes in the decision
tree with only single permutations again insertion sort has found the correct
permutation that yields a sorted list
any sorting algorithm based on comparisons can be modeled by a decision tree
in this way regardless of the size of the input thus all sorting algorithms can
be viewed as algorithms to find the correct permutation of the input that yields
a sorted list each algorithm based on comparisons can be viewed as proceeding
by making branches in the tree based on the results of key comparisons and each
algorithm can terminate once a node with a single permutation has been reached
how is the worstcase cost of an algorithm expressed by the decision tree the
decision tree shows the decisions made by an algorithm for all possible inputs of a
given size each path through the tree from the root to a leaf is one possible series
of decisions taken by the algorithm the depth of the deepest node represents the
longest series of decisions required by the algorithm to reach an answer
there are many comparisonbased sorting algorithms and each will be modeled by a different decision tree some decision trees might be wellbalanced others might be unbalanced some trees will have more nodes than others those with
more nodes might be making unnecessary comparisons in fact a poor sorting
algorithm might have an arbitrarily large number of nodes in its decision tree with
leaves of arbitrary depth there is no limit to how slow the worst possible sorting algorithm could be however we are interested here in knowing what the best
sorting algorithm could have as its minimum cost in the worst case in other words
we would like to know what is the smallest depth possible for the deepest node in
the tree for any sorting algorithm
the smallest depth of the deepest node will depend on the number of nodes
in the tree clearly we would like to push up the nodes in the tree but there is
limited room at the top a tree of height 1 can only store one node the root the
tree of height 2 can store three nodes the tree of height 3 can store seven nodes
and so on
here are some important facts worth remembering
 a binary tree of height n can store at most 2n  1 nodes


$$@@$$PAGE: 284
sec 710 further reading

265

 equivalently a tree with n nodes requires at least dlogn  1e levels
what is the minimum number of nodes that must be in the decision tree for any
comparisonbased sorting algorithm for n values because sorting algorithms are
in the business of determining which unique permutation of the input corresponds
to the sorted list the decision tree for any sorting algorithm must contain at least
one leaf node for each possible permutation there are n permutations for a set of
n numbers see section 22
because there are at least n nodes in the tree we know that the tree must
have log n levels from stirlings approximation section 22 we know log n
is in n log n the decision tree for any comparisonbased sorting algorithm
must have nodes n log n levels deep thus in the worst case any such sorting
algorithm must require n log n comparisons
any sorting algorithm requiring n log n comparisons in the worst case requires n log n running time in the worst case because any sorting algorithm
requires n log n running time the problem of sorting also requires n log n
time we already know of sorting algorithms with on log n running time so we
can conclude that the problem of sorting requires n log n time as a corollary we know that no comparisonbased sorting algorithm can improve on existing
n log n time sorting algorithms by more than a constant factor

710

further reading

the definitive reference on sorting is donald e knuths sorting and searching
knu98 a wealth of details is covered there including optimal sorts for small
size n and special purpose sorting networks it is a thorough although somewhat
dated treatment on sorting for an analysis of quicksort and a thorough survey
on its optimizations see robert sedgewicks quicksort sed80 sedgewicks algorithms sed11 discusses most of the sorting algorithms described here and pays
special attention to efficient implementation the optimized mergesort version of
section 74 comes from sedgewick
while n log n is the theoretical lower bound in the worst case for sorting
many times the input is sufficiently well ordered that certain algorithms can take
advantage of this fact to speed the sorting process a simple example is insertion
sorts bestcase running time sorting algorithms whose running time is based on
the amount of disorder in the input are called adaptive for more information on
adaptive sorting algorithms see a survey of adaptive sorting algorithms by
estivillcastro and wood ecw92

711

exercises

71 using induction prove that insertion sort will always produce a sorted array


$$@@$$PAGE: 285
266

chap 7 internal sorting

72 write an insertion sort algorithm for integer key values however heres
the catch the input is a stack not an array and the only variables that
your algorithm may use are a fixed number of integers and a fixed number of
stacks the algorithm should return a stack containing the records in sorted
order with the least value being at the top of the stack your algorithm
should be n2  in the worst case
73 the bubble sort implementation has the following inner for loop
for int jn1 ji j

consider the effect of replacing this with the following statement
for int jn1 j0 j

would the new implementation work correctly would the change affect the
asymptotic complexity of the algorithm how would the change affect the
running time of the algorithm
74 when implementing insertion sort a binary search could be used to locate
the position within the first i  1 elements of the array into which element
i should be inserted how would this affect the number of comparisons required how would using such a binary search affect the asymptotic running
time for insertion sort
75 figure 75 shows the bestcase number of swaps for selection sort as n
this is because the algorithm does not check to see if the ith record is already
in the ith position that is it might perform unnecessary swaps
a modify the algorithm so that it does not make unnecessary swaps
b what is your prediction regarding whether this modification actually
improves the running time
c write two programs to compare the actual running times of the original selection sort and the modified algorithm which one is actually
faster
76 recall that a sorting algorithm is said to be stable if the original ordering for
duplicate keys is preserved of the sorting algorithms insertion sort bubble sort selection sort shellsort mergesort quicksort heapsort binsort
and radix sort which of these are stable and which are not for each one
describe either why it is or is not stable if a minor change to the implementation would make it stable describe the change
77 recall that a sorting algorithm is said to be stable if the original ordering for
duplicate keys is preserved we can make any algorithm stable if we alter
the input keys so that potentially duplicate key values are made unique in
a way that the first occurrence of the original duplicate value is less than the
second occurrence which in turn is less than the third and so on in the worst
case it is possible that all n input records have the same key value give an


$$@@$$PAGE: 286
sec 711 exercises

267

algorithm to modify the key values such that every modified key value is
unique the resulting key values give the same sort order as the original keys
the result is stable in that the duplicate original key values remain in their
original order and the process of altering the keys is done in linear time
using only a constant amount of additional space
78 the discussion of quicksort in section 75 described using a stack instead of
recursion to reduce the number of function calls made
a how deep can the stack get in the worst case
b quicksort makes two recursive calls the algorithm could be changed
to make these two calls in a specific order in what order should the
two calls be made and how does this affect how deep the stack can
become
79 give a permutation for the values 0 through 7 that will cause quicksort as
implemented in section 75 to have its worst case behavior
710 assume l is an array lengthl returns the number of records in the
array and qsortl i j sorts the records of l from i to j leaving
the records sorted in l using the quicksort algorithm what is the averagecase time complexity for each of the following code fragments
a for i0 ilengthl i
qsortl 0 i

b for i0 ilengthl i
qsortl 0 lengthl1

711 modify quicksort to find the smallest k values in an array of records your
output should be the array modified so that the k smallest values are sorted
in the first k positions of the array your algorithm should do the minimum
amount of work necessary that is no more of the array than necessary should
be sorted
712 modify quicksort to sort a sequence of variablelength strings stored one
after the other in a character array with a second array storing pointers to
strings used to index the strings your function should modify the index
array so that the first pointer points to the beginning of the lowest valued
string and so on
713 graph f1 n  n log n f2 n  n15  and f3 n  n2 in the range 1  n 
1000 to visually compare their growth rates typically the constant factor
in the runningtime expression for an implementation of insertion sort will
be less than the constant factors for shellsort or quicksort how many times
greater can the constant factor be for shellsort to be faster than insertion sort
when n  1000 how many times greater can the constant factor be for
quicksort to be faster than insertion sort when n  1000


$$@@$$PAGE: 287
268

chap 7 internal sorting

714 imagine that there exists an algorithm splitk that can split a list l of n
elements into k sublists each containing one or more elements such that
sublist i contains only elements whose values are less than all elements in
sublist j for i  j  k if n  k then k  n sublists are empty and the rest
are of length 1 assume that splitk has time complexity olength of l
furthermore assume that the k lists can be concatenated again in constant
time consider the following algorithm
list sortklist l 
list subk  to hold the sublists
if llength  1 
splitkl sub  splitk places sublists into sub
for i0 ik i
subi  sortksubi  sort each sublist
l  concatenation of k sublists in sub
return l



a what is the worstcase asymptotic running time for sortk why
b what is the averagecase asymptotic running time of sortk why
715 here is a variation on sorting the problem is to sort a collection of n nuts
and n bolts by size it is assumed that for each bolt in the collection there
is a corresponding nut of the same size but initially we do not know which
nut goes with which bolt the differences in size between two nuts or two
bolts can be too small to see by eye so you cannot rely on comparing the
sizes of two nuts or two bolts directly instead you can only compare the
sizes of a nut and a bolt by attempting to screw one into the other assume
this comparison to be a constant time operation this operation tells you
that either the nut is bigger than the bolt the bolt is bigger than the nut or
they are the same size what is the minimum number of comparisons needed
to sort the nuts and bolts in the worst case
716 a devise an algorithm to sort three numbers it should make as few comparisons as possible how many comparisons and swaps are required
in the best worst and average cases
b devise an algorithm to sort five numbers it should make as few comparisons as possible how many comparisons and swaps are required
in the best worst and average cases
c devise an algorithm to sort eight numbers it should make as few comparisons as possible how many comparisons and swaps are required
in the best worst and average cases
717 devise an efficient algorithm to sort a set of numbers with values in the range
0 to 30000 there are no duplicates keep memory requirements to a minimum


$$@@$$PAGE: 288
sec 712 projects

269

718 which of the following operations are best implemented by first sorting the
list of numbers for each operation briefly describe an algorithm to implement it and state the algorithms asymptotic complexity
a
b
c
d
e

find the minimum value
find the maximum value
compute the arithmetic mean
find the median ie the middle value
find the mode ie the value that appears the most times

719 consider a recursive mergesort implementation that calls insertion sort on
sublists smaller than some threshold if there are n calls to mergesort how
many calls will there be to insertion sort why
720 implement mergesort for the case where the input is a linked list
721 counting sort assuming the input key values are integers in the range 0 to
m  1 works by counting the number of records with each key value in the
first pass and then uses this information to place the records in order in a
second pass write an implementation of counting sort see the implementation of radix sort for some ideas what can we say about the relative values
of m and n for this to be effective if m  n what is the running time of
this algorithm
722 use an argument similar to that given in section 79 to prove that log n is a
worstcase lower bound for the problem of searching for a given value in a
sorted array containing n elements
723 a simpler way to do the quicksort partition step is to set index split
to the position of the first value greater than the pivot then from position split1 have another index curr move to the right until it finds a
value less than a pivot swap the values at split and next and increment split continue in this way swapping the smaller values to the left
side when curr reaches the end of the subarray split will be at the split
point between the two partitions unfortunately this approach requires more
swaps than does the version presented in section 75 resulting in a slower
implementation give an example and explain why

712

projects

71 one possible improvement for bubble sort would be to add a flag variable
and a test that determines if an exchange was made during the current iteration if no exchange was made then the list is sorted and so the algorithm
can stop early this makes the best case performance become on because
if the list is already sorted then no iterations will take place on the first pass
and the sort will stop right there


$$@@$$PAGE: 289
270

72

73

74

75

chap 7 internal sorting

modify the bubble sort implementation to add this flag and test compare
the modified implementation on a range of inputs to determine if it does or
does not improve performance in practice
double insertion sort is a variation on insertion sort that works from the
middle of the array out at each iteration some middle portion of the array
is sorted on the next iteration take the two adjacent elements to the sorted
portion of the array if they are out of order with respect to each other than
swap them now push the left element toward the right in the array so long
as it is greater than the element to its right and push the right element
toward the left in the array so long as it is less than the element to its left
the algorithm begins by processing the middle two elements of the array if
the array is even if the array is odd then skip processing the middle item
and begin with processing the elements to its immediate left and right
first explain what the cost of double insertion sort will be in comparison to
standard insertion sort and why note that the two elements being processed
in the current iteration once initially swapped to be sorted with with respect
to each other cannot cross as they are pushed into sorted position then implement double insertion sort being careful to properly handle both when
the array is odd and when it is even compare its running time in practice
against standard insertion sort finally explain how this speedup might affect the threshold level and running time for a quicksort implementation
perform a study of shellsort using different increments compare the version shown in section 73 where each increment is half the previous one
with others in particular try implementing division by 3 where the increments on a list of length n will be n3 n9 etc do other increment schemes
work as well
the implementation for mergesort given in section 74 takes an array as input and sorts that array at the beginning of section 74 there is a simple
pseudocode implementation for sorting a linked list using mergesort implement both a linked listbased version of mergesort and the arraybased
version of mergesort and compare their running times
starting with the c code for quicksort given in this chapter write a series
of quicksort implementations to test the following optimizations on a wide
range of input data sizes try these optimizations in various combinations to
try and develop the fastest possible quicksort implementation that you can
a look at more values when selecting a pivot
b do not make a recursive call to qsort when the list size falls below a
given threshold and use insertion sort to complete the sorting process
test various values for the threshold size
c eliminate recursion by using a stack and inline functions


$$@@$$PAGE: 290
sec 712 projects

271

76 it has been proposed that heapsort can be optimized by altering the heaps
siftdown function call the value being sifted down x siftdown does two
comparisons per level first the children of x are compared then the winner
is compared to x if x is too small it is swapped with its larger child and the
process repeated the proposed optimization dispenses with the test against
x instead the larger child automatically replaces x until x reaches the
bottom level of the heap at this point x might be too large to remain in
that position this is corrected by repeatedly comparing x with its parent
and swapping as necessary to bubble it up to its proper level the claim
is that this process will save a number of comparisons because most nodes
when sifted down end up near the bottom of the tree anyway implement both
versions of siftdown and do an empirical study to compare their running
times
77 radix sort is typically implemented to support only a radix that is a power
of two this allows for a direct conversion from the radix to some number
of bits in an integer key value for example if the radix is 16 then a 32bit
key will be processed in 8 steps of 4 bits each this can lead to a more efficient implementation because bit shifting can replace the division operations
shown in the implementation of section 77 reimplement the radix sort
code given in section 77 to use bit shifting in place of division compare
the running time of the old and new radix sort implementations
78 write your own collection of sorting programs to implement the algorithms
described in this chapter and compare their running times be sure to implement optimized versions trying to make each program as fast as possible
do you get the same relative timings as shown in figure 720 if not why do
you think this happened how do your results compare with those of your
classmates what does this say about the difficulty of doing empirical timing
studies


$$@@$$PAGE: 291

$$@@$$PAGE: 292
8
file processing and external
sorting

earlier chapters presented basic data structures and algorithms that operate on data
stored in main memory some applications require that large amounts of information be stored and processed  so much information that it cannot all fit into main
memory in that case the information must reside on disk and be brought into main
memory selectively for processing
you probably already realize that main memory access is much faster than access to data stored on disk or other storage devices the relative difference in access
times is so great that efficient diskbased programs require a different approach to
algorithm design than most programmers are used to as a result many programmers do a poor job when it comes to file processing applications
this chapter presents the fundamental issues relating to the design of algorithms and data structures for diskbased applications1 we begin with a description of the significant differences between primary memory and secondary storage
section 82 discusses the physical aspects of disk drives section 83 presents basic methods for managing buffer pools section 84 discusses the c model for
random access to data stored on disk section 85 discusses the basic principles for
sorting collections of records too large to fit in main memory

81

primary versus secondary storage

computer storage devices are typically classified into primary or main memory
and secondary or peripheral storage primary memory usually refers to random
1

computer technology changes rapidly i provide examples of disk drive specifications and other
hardware performance numbers that are reasonably up to date as of the time when the book was
written when you read it the numbers might seem out of date however the basic principles do not
change the approximate ratios for time space and cost between memory and disk have remained
surprisingly steady for over 20 years

273


$$@@$$PAGE: 293
274

chap 8 file processing and external sorting

medium
ram
disk
usb drive
floppy
tape
solid state

1996
4500
025

050
003


1997
700
010

036
001


2000
1500
0010

0250
0001


2004
03500
00010
01000
02500
00003


2006
01500
00005
00900




2008
00339
00001
00029




2011
00138
00001
00018


00021

figure 81 price comparison table for some writable electronic data storage
media in common use prices are in us dollarsmb

access memory ram while secondary storage refers to devices such as hard
disk drives solid state drives removable usb drives cds and dvds primary
memory also includes registers cache and video memories but we will ignore
them for this discussion because their existence does not affect the principal differences between primary and secondary memory
along with a faster cpu every new model of computer seems to come with
more main memory as memory size continues to increase is it possible that relatively slow disk storage will be unnecessary probably not because the desire to
store and process larger files grows at least as fast as main memory size prices
for both main memory and peripheral storage devices have dropped dramatically
in recent years as demonstrated by figure 81 however the cost per unit of disk
drive storage is about two orders of magnitude less than ram and has been for
many years
there is now a wide range of removable media available for transferring data
or storing data offline in relative safety these include floppy disks now largely
obsolete writable cds and dvds flash drives and magnetic tape optical storage such as cds and dvds costs roughly half the price of hard disk drive space
per megabyte and have become practical for use as backup storage within the past
few years tape used to be much cheaper than other media and was the preferred
means of backup but are not so popular now as other media have decreased in
price flash drives cost the most per megabyte but due to their storage capacity and flexibility quickly replaced floppy disks as the primary storage device for
transferring data between computer when direct network transfer is not available
secondary storage devices have at least two other advantages over ram memory perhaps most importantly disk flash and optical media are persistent
meaning that they are not erased from the media when the power is turned off in
contrast ram used for main memory is usually volatile  all information is lost
with the power a second advantage is that cds and usb drives can easily be
transferred between computers this provides a convenient way to take information
from one computer to another


$$@@$$PAGE: 294
sec 81 primary versus secondary storage

275

in exchange for reduced storage costs persistence and portability secondary
storage devices pay a penalty in terms of increased access time while not all accesses to disk take the same amount of time more on this later the typical time
required to access a byte of storage from a disk drive in 2011 is around 9 ms ie
9 thousandths of a second this might not seem slow but compared to the time
required to access a byte from main memory this is fantastically slow typical
access time from standard personal computer ram in 2011 is about 510 nanoseconds ie 510 billionths of a second thus the time to access a byte of data from
a disk drive is about six orders of magnitude greater than that required to access a
byte from main memory while disk drive and ram access times are both decreasing they have done so at roughly the same rate the relative speeds have remained
the same for over several decades in that the difference in access time between
ram and a disk drive has remained in the range between a factor of 100000 and
1000000
to gain some intuition for the significance of this speed difference consider the
time that it might take for you to look up the entry for disk drives in the index of
this book and then turn to the appropriate page call this your primary memory
access time if it takes you about 20 seconds to perform this access then an access
taking 500000 times longer would require months
it is interesting to note that while processing speeds have increased dramatically and hardware prices have dropped dramatically disk and memory access
times have improved by less than an order of magnitude over the past 15 years
however the situation is really much better than that modest speedup would suggest during the same time period the size of both disk and main memory has
increased by over three orders of magnitude thus the access times have actually
decreased in the face of a massive increase in the density of these storage devices
due to the relatively slow access time for data on disk as compared to main
memory great care is required to create efficient applications that process diskbased information the milliontoone ratio of disk access time versus main memory access time makes the following rule of paramount importance when designing
diskbased applications
minimize the number of disk accesses
there are generally two approaches to minimizing disk accesses the first is
to arrange information so that if you do access data from secondary memory you
will get what you need in as few accesses as possible and preferably on the first
access file structure is the term used for a data structure that organizes data
stored in secondary memory file structures should be organized so as to minimize
the required number of disk accesses the other way to minimize disk accesses is to
save information previously retrieved or retrieve additional data with each access
at little additional cost that can be used to minimize the need for future accesses


$$@@$$PAGE: 295
276

chap 8 file processing and external sorting

this requires the ability to guess accurately what information will be needed later
and store it in primary memory now this is referred to as caching

82

disk drives

a c programmer views a random access file stored on disk as a contiguous
series of bytes with those bytes possibly combining to form data records this
is called the logical file the physical file actually stored on disk is usually not
a contiguous series of bytes it could well be in pieces spread all over the disk
the file manager a part of the operating system is responsible for taking requests
for data from a logical file and mapping those requests to the physical location
of the data on disk likewise when writing to a particular logical byte position
with respect to the beginning of the file this position must be converted by the
file manager into the corresponding physical location on the disk to gain some
appreciation for the the approximate time costs for these operations you need to
understand the physical structure and basic workings of a disk drive
disk drives are often referred to as direct access storage devices this means
that it takes roughly equal time to access any record in the file this is in contrast
to sequential access storage devices such as tape drives which require the tape
reader to process data from the beginning of the tape until the desired position has
been reached as you will see the disk drive is only approximately direct access
at any given time some records are more quickly accessible than others
821

disk drive architecture

a hard disk drive is composed of one or more round platters stacked one on top of
another and attached to a central spindle platters spin continuously at a constant
rate each usable surface of each platter is assigned a readwrite head or io
head through which data are read or written somewhat like the arrangement of
a phonograph players arm reading sound from a phonograph record unlike a
phonograph needle the disk readwrite head does not actually touch the surface of
a hard disk instead it remains slightly above the surface and any contact during
normal operation would damage the disk this distance is very small much smaller
than the height of a dust particle it can be likened to a 5000kilometer airplane trip
across the united states with the plane flying at a height of one meter
a hard disk drive typically has several platters and several readwrite heads as
shown in figure 82a each head is attached to an arm which connects to the
boom2 the boom moves all of the heads in or out together when the heads are
in some position over the platters there are data on each platter directly accessible
2

this arrangement while typical is not necessarily true for all disk drives nearly everything
said here about the physical arrangement of disk drives represents a typical engineering compromise
not a fundamental design principle there are many ways to design disk drives and the engineering


$$@@$$PAGE: 296
277

sec 82 disk drives

boom
arm

platters

readwrite
heads

spindle
a

track
b

figure 82 a a typical disk drive arranged as a stack of platters b one track
on a disk drive platter

to each head the data on a single platter that are accessible to any one position of
the head for that platter are collectively called a track that is all data on a platter
that are a fixed distance from the spindle as shown in figure 82b the collection
of all tracks that are a fixed distance from the spindle is called a cylinder thus a
cylinder is all of the data that can be read when the arms are in a particular position
each track is subdivided into sectors between each sector there are intersector gaps in which no data are stored these gaps allow the read head to recognize the end of a sector note that each sector contains the same amount of data
because the outer tracks have greater length they contain fewer bits per inch than
do the inner tracks thus about half of the potential storage space is wasted because only the innermost tracks are stored at the highest possible data density this
arrangement is illustrated by figure 83a disk drives today actually group tracks
into zones such that the tracks in the innermost zone adjust their data density
going out to maintain the same radial data density then the tracks of the next zone
reset the data density to make better use of their storage ability and so on this
arrangement is shown in figure 83b
in contrast to the physical layout of a hard disk a cdrom consists of a single
spiral track bits of information along the track are equally spaced so the information density is the same at both the outer and inner portions of the track to keep
the information flow at a constant rate along the spiral the drive must speed up the
rate of disk spin as the io head moves toward the center of the disk this makes
for a more complicated and slower mechanism
three separate steps take place when reading a particular byte or series of bytes
of data from a hard disk first the io head moves so that it is positioned over the
track containing the data this movement is called a seek second the sector
containing the data rotates to come under the head when in use the disk is always
compromises change over time in addition most of the description given here for disk drives is a
simplified version of the reality but this is a useful working model to understand what is going on


$$@@$$PAGE: 297
278

chap 8 file processing and external sorting

intersector
gaps

bits of data

sectors
a

b

figure 83 the organization of a disk platter dots indicate density of information a nominal arrangement of tracks showing decreasing data density when
moving outward from the center of the disk b a zoned arrangement with the
sector size and density periodically reset in tracks further away from the center

spinning at the time of this writing typical disk spin rates are 7200 rotations per
minute rpm the time spent waiting for the desired sector to come under the
io head is called rotational delay or rotational latency the third step is the
actual transfer ie reading or writing of data it takes relatively little time to
read information once the first byte is positioned under the io head simply the
amount of time required for it all to move under the head in fact disk drives are
designed not to read one byte of data but rather to read an entire sector of data at
each request thus a sector is the minimum amount of data that can be read or
written at one time
in general it is desirable to keep all sectors for a file together on as few tracks
as possible this desire stems from two assumptions
1 seek time is slow it is typically the most expensive part of an io operation
and
2 if one sector of the file is read the next sector will probably soon be read
assumption 2 is called locality of reference a concept that comes up frequently
in computer applications
contiguous sectors are often grouped to form a cluster a cluster is the smallest
unit of allocation for a file so all files are a multiple of the cluster size the cluster
size is determined by the operating system the file manager keeps track of which
clusters make up each file
in microsoft windows systems there is a designated portion of the disk called
the file allocation table which stores information about which sectors belong
to which file in contrast unix does not use clusters the smallest unit of file


$$@@$$PAGE: 298
sec 82 disk drives

279

allocation and the smallest unit that can be readwritten is a sector which in unix
terminology is called a block unix maintains information about file organization
in certain disk blocks called inodes
a group of physically contiguous clusters from the same file is called an extent
ideally all clusters making up a file will be contiguous on the disk ie the file will
consist of one extent so as to minimize seek time required to access different
portions of the file if the disk is nearly full when a file is created there might not
be an extent available that is large enough to hold the new file furthermore if a file
grows there might not be free space physically adjacent thus a file might consist
of several extents widely spaced on the disk the fuller the disk and the more that
files on the disk change the worse this file fragmentation and the resulting seek
time becomes file fragmentation leads to a noticeable degradation in performance
as additional seeks are required to access data
another type of problem arises when the files logical record size does not
match the sector size if the sector size is not a multiple of the record size or
vice versa records will not fit evenly within a sector for example a sector might
be 2048 bytes long and a logical record 100 bytes this leaves room to store
20 records with 48 bytes left over either the extra space is wasted or else records
are allowed to cross sector boundaries if a record crosses a sector boundary two
disk accesses might be required to read it if the space is left empty instead such
wasted space is called internal fragmentation
a second example of internal fragmentation occurs at cluster boundaries files
whose size is not an even multiple of the cluster size must waste some space at
the end of the last cluster the worst case will occur when file size modulo cluster
size is one for example a file of 4097 bytes and a cluster of 4096 bytes thus
cluster size is a tradeoff between large files processed sequentially where a large
cluster size is desirable to minimize seeks and small files where small clusters are
desirable to minimize wasted storage
every disk drive organization requires that some disk space be used to organize
the sectors clusters and so forth the layout of sectors within a track is illustrated
by figure 84 typical information that must be stored on the disk itself includes
the file allocation table sector headers that contain address marks and information about the condition whether usable or not for each sector and gaps between
sectors the sector header also contains error detection codes to help verify that the
data have not been corrupted this is why most disk drives have a nominal size
that is greater than the actual amount of user data that can be stored on the drive
the difference is the amount of space required to organize the information on the
disk even more space will be lost due to fragmentation


$$@@$$PAGE: 299
280

chap 8 file processing and external sorting

intersector gap
sector
header

sector
data

sector
header

sector
data

intrasector gap
figure 84 an illustration of sector gaps within a track each sector begins with
a sector header containing the sector address and an error detection code for the
contents of that sector the sector header is followed by a small intrasector gap
followed in turn by the sector data each sector is separated from the next sector
by a larger intersector gap

822

disk access costs

when a seek is required it is usually the primary cost when accessing information
on disk this assumes of course that a seek is necessary when reading a file in
sequential order if the sectors comprising the file are contiguous on disk little
seeking is necessary however when accessing a random disk sector seek time
becomes the dominant cost for the data access while the actual seek time is highly
variable depending on the distance between the track where the io head currently
is and the track where the head is moving to we will consider only two numbers
one is the tracktotrack cost or the minimum time necessary to move from a
track to an adjacent track this is appropriate when you want to analyze access
times for files that are well placed on the disk the second number is the average
seek time for a random access these two numbers are often provided by disk
manufacturers a typical example is the western digital caviar serial ata drive
the manufacturers specifications indicate that the tracktotrack time is 20 ms and
the average seek time is 90 ms in 2008 a typical drive in this line might be 120gb
in size in 2011 that same line of drives had sizes of up to 2 or 3tb in both years
the advertised tracktotrack and average seek times were identical
for many years typical rotation speed for disk drives was 3600 rpm or one
rotation every 167 ms most disk drives in 2011 had a rotation speed of 7200 rpm
or 83 ms per rotation when reading a sector at random you can expect that the
disk will need to rotate halfway around to bring the desired sector under the io
head or 42 ms for a 7200rpm disk drive
once under the io head a sector of data can be transferred as fast as that
sector rotates under the head if an entire track is to be read then it will require one
rotation 83 ms at 7200 rpm to move the full track under the head if only part of
the track is to be read then proportionately less time will be required for example
if there are 16000 sectors on the track and one sector is to be read this will require
a trivial amount of time 116000 of a rotation
example 81 assume that an older disk drive has a total nominal capacity of 168gb spread among 10 platters yielding 168gbplatter each


$$@@$$PAGE: 300
281

sec 82 disk drives

platter contains 13085 tracks and each track contains after formatting
256 sectors of 512 bytessector tracktotrack seek time is 22 ms and average seek time for random access is 95 ms assume the operating system
maintains a cluster size of 8 sectors per cluster 4kb yielding 32 clusters
per track the disk rotation rate is 5400 rpm 111 ms per rotation based
on this information we can estimate the cost for various file processing operations
how much time is required to read the track on average it will require
half a rotation to bring the first sector of the track under the io head and
then one complete rotation to read the track
how long will it take to read a file of 1mb divided into 2048 sectorsized 512 byte records this file will be stored in 256 clusters because
each cluster holds 8 sectors the answer to the question depends largely
on how the file is stored on the disk that is whether it is all together or
broken into multiple extents we will calculate both cases to see how much
difference this makes
if the file is stored so as to fill all of the sectors of eight adjacent tracks
then the cost to read the first sector will be the time to seek to the first track
assuming this requires a random seek then a wait for the initial rotational
delay and then the time to read which is the same as the time to rotate the
disk again this requires
95  111  15  262 ms
at this point because we assume that the next seven tracks require only a
tracktotrack seek because they are adjacent each requires
22  111  15  189 ms
the total time required is therefore
262ms  7  189ms  1585ms
if the files clusters are spread randomly across the disk then we must
perform a seek for each cluster followed by the time for rotational delay
once the first sector of the cluster comes under the io head very little time
is needed to read the cluster because only 8256 of the track needs to rotate
under the head for a total time of about 59 ms for latency and read time
thus the total time required is about
25695  59  3942ms
or close to 4 seconds this is much longer than the time required when the
file is all together on disk


$$@@$$PAGE: 301
282

chap 8 file processing and external sorting

this example illustrates why it is important to keep disk files from becoming fragmented and why socalled disk defragmenters can speed up
file processing time file fragmentation happens most commonly when the
disk is nearly full and the file manager must search for free space whenever
a file is created or changed

83

buffers and buffer pools

given the specifications of the disk drive from example 81 we find that it takes
about 9511115  262 ms to read one track of data on average it takes about
9511121256111  151 ms on average to read a single sector of data
this is a good savings slightly over half the time but less than 1 of the data on
the track are read if we want to read only a single byte it would save us effectively
no time over that required to read an entire sector for this reason nearly all disk
drives automatically read or write an entire sectors worth of information whenever
the disk is accessed even when only one byte of information is requested
once a sector is read its information is stored in main memory this is known
as buffering or caching the information if the next disk request is to that same
sector then it is not necessary to read from disk again because the information
is already stored in main memory buffering is an example of one method for
minimizing disk accesses mentioned at the beginning of the chapter bring off
additional information from disk to satisfy future requests if information from files
were accessed at random then the chance that two consecutive disk requests are to
the same sector would be low however in practice most disk requests are close
to the location in the logical file at least of the previous request this means that
the probability of the next request hitting the cache is much higher than chance
would indicate
this principle explains one reason why average access times for new disk drives
are lower than in the past not only is the hardware faster but information is also
now stored using better algorithms and larger caches that minimize the number of
times information needs to be fetched from disk this same concept is also used
to store parts of programs in faster memory within the cpu using the cpu cache
that is prevalent in modern microprocessors
sectorlevel buffering is normally provided by the operating system and is often built directly into the disk drive controller hardware most operating systems
maintain at least two buffers one for input and one for output consider what
would happen if there were only one buffer during a bytebybyte copy operation
the sector containing the first byte would be read into the io buffer the output
operation would need to destroy the contents of the single io buffer to write this
byte then the buffer would need to be filled again from disk for the second byte


$$@@$$PAGE: 302
sec 83 buffers and buffer pools

283

only to be destroyed during output the simple solution to this problem is to keep
one buffer for input and a second for output
most disk drive controllers operate independently from the cpu once an io
request is received this is useful because the cpu can typically execute millions
of instructions during the time required for a single io operation a technique that
takes maximum advantage of this microparallelism is double buffering imagine
that a file is being processed sequentially while the first sector is being read the
cpu cannot process that information and so must wait or find something else to do
in the meantime once the first sector is read the cpu can start processing while
the disk drive in parallel begins reading the second sector if the time required for
the cpu to process a sector is approximately the same as the time required by the
disk controller to read a sector it might be possible to keep the cpu continuously
fed with data from the file the same concept can also be applied to output writing
one sector to disk while the cpu is writing to a second output buffer in memory
thus in computers that support double buffering it pays to have at least two input
buffers and two output buffers available
caching information in memory is such a good idea that it is usually extended
to multiple buffers the operating system or an application program might store
many buffers of information taken from some backing storage such as a disk file
this process of using buffers as an intermediary between a user and a disk file is
called buffering the file the information stored in a buffer is often called a page
and the collection of buffers is called a buffer pool the goal of the buffer pool
is to increase the amount of information stored in memory in hopes of increasing
the likelihood that new information requests can be satisfied from the buffer pool
rather than requiring new information to be read from disk
as long as there is an unused buffer available in the buffer pool new information can be read in from disk on demand when an application continues to read
new information from disk eventually all of the buffers in the buffer pool will become full once this happens some decision must be made about what information
in the buffer pool will be sacrificed to make room for newly requested information
when replacing information contained in the buffer pool the goal is to select a
buffer that has unnecessary information that is the information least likely to be
requested again because the buffer pool cannot know for certain what the pattern
of future requests will look like a decision based on some heuristic or best guess
must be used there are several approaches to making this decision
one heuristic is firstin firstout fifo this scheme simply orders the
buffers in a queue the buffer at the front of the queue is used next to store new
information and then placed at the end of the queue in this way the buffer to be
replaced is the one that has held its information the longest in hopes that this information is no longer needed this is a reasonable assumption when processing
moves along the file at some steady pace in roughly sequential order however


$$@@$$PAGE: 303
284

chap 8 file processing and external sorting

many programs work with certain key pieces of information over and over again
and the importance of information has little to do with how long ago the information was first accessed typically it is more important to know how many times the
information has been accessed or how recently the information was last accessed
another approach is called least frequently used lfu lfu tracks the number of accesses to each buffer in the buffer pool when a buffer must be reused the
buffer that has been accessed the fewest number of times is considered to contain
the least important information and so it is used next lfu while it seems intuitively reasonable has many drawbacks first it is necessary to store and update
access counts for each buffer second what was referenced many times in the past
might now be irrelevant thus some time mechanism where counts expire is
often desirable this also avoids the problem of buffers that slowly build up big
counts because they get used just often enough to avoid being replaced an alternative is to maintain counts for all sectors ever read not just the sectors currently
in the buffer pool this avoids immediately replacing the buffer just read which
has not yet had time to build a high access count
the third approach is called least recently used lru lru simply keeps the
buffers in a list whenever information in a buffer is accessed this buffer is brought
to the front of the list when new information must be read the buffer at the back
of the list the one least recently used is taken and its old information is either
discarded or written to disk as appropriate this is an easily implemented approximation to lfu and is often the method of choice for managing buffer pools unless
special knowledge about information access patterns for an application suggests a
specialpurpose buffer management scheme
the main purpose of a buffer pool is to minimize disk io when the contents of
a block are modified we could write the updated information to disk immediately
but what if the block is changed again if we write the blocks contents after every
change that might be a lot of disk write operations that can be avoided it is more
efficient to wait until either the file is to be closed or the contents of the buffer
containing that block is to be flushed from the buffer pool
when a buffers contents are to be replaced in the buffer pool we only want
to write the contents to disk if it is necessary that would be necessary only if the
contents have changed since the block was read in originally from the file the way
to insure that the block is written when necessary but only when necessary is to
maintain a boolean variable with the buffer often referred to as the dirty bit that
is turned on when the buffers contents are modified by the client at the time when
the block is flushed from the buffer pool it is written to disk if and only if the dirty
bit has been turned on
modern operating systems support virtual memory virtual memory is a technique that allows the programmer to write programs as though there is more of the
faster main memory such as ram than actually exists virtual memory makes use


$$@@$$PAGE: 304
285

sec 83 buffers and buffer pools

of a buffer pool to store data read from blocks on slower secondary memory such
as on the disk drive the disk stores the complete contents of the virtual memory
blocks are read into main memory as demanded by memory accesses naturally
programs using virtual memory techniques are slower than programs whose data
are stored completely in main memory the advantage is reduced programmer effort because a good virtual memory system provides the appearance of larger main
memory without modifying the program
example 82 consider a virtual memory whose size is ten sectors and
which has a buffer pool of five buffers each one sector in size associated
with it we will use a lru replacement scheme the following series of
memory requests occurs
9017668135171
after the first five requests the buffer pool will store the sectors in the order
6 7 1 0 9 because sector 6 is already at the front the next request can be
answered without reading new data from disk or reordering the buffers the
request to sector 8 requires emptying the contents of the least recently used
buffer which contains sector 9 the request to sector 1 brings the buffer
holding sector 1s contents back to the front processing the remaining
requests results in the buffer pool as shown in figure 85

example 83 figure 85 illustrates a buffer pool of five blocks mediating
a virtual memory of ten blocks at any given moment up to five sectors of
information can be in main memory assume that sectors 1 7 5 3 and 8
are currently in the buffer pool stored in this order and that we use the
lru buffer replacement strategy if a request for sector 9 is then received
then one sector currently in the buffer pool must be replaced because the
buffer containing sector 8 is the least recently used buffer its contents will
be copied back to disk at sector 8 the contents of sector 9 are then copied
into this buffer and it is moved to the front of the buffer pool leaving the
buffer containing sector 3 as the new leastrecently used buffer if the next
memory request were to sector 5 no data would need to be read from disk
instead the buffer already containing sector 5 would be moved to the front
of the buffer pool
when implementing buffer pools there are two basic approaches that can be
taken regarding the transfer of information between the user of the buffer pool and
the buffer pool class itself the first approach is to pass messages between the
two this approach is illustrated by the following abstract class


$$@@$$PAGE: 305
286

chap 8 file processing and external sorting

secondary storage
on disk

main memory
in ram

0

1

1

7

2

5

3

3

4

8

5
6
7
8
9
figure 85 an illustration of virtual memory the complete collection of information resides in the slower secondary storage on disk those sectors recently
accessed are held in the fast main memory in ram in this example copies of
sectors 1 7 5 3 and 8 from secondary storage are currently stored in the main
memory if a memory access to sector 9 is received one of the sectors currently
in main memory must be replaced
 adt for buffer pools using the messagepassing style
class bufferpool 
public
 copy sz bytes from space to position pos in the

buffered storage
virtual void insertvoid space int sz int pos  0
 copy sz bytes from position pos of the buffered

storage to space
virtual void getbytesvoid space int sz int pos  0


this simple class provides an interface with two member functions insert
and getbytes the information is passed between the buffer pool user and the
buffer pool through the space parameter this is storage space provided by the
bufferpool client and at least sz bytes long which the buffer pool can take information from the insert function or put information into the getbytes
function parameter pos indicates where the information will be placed in the
buffer pools logical storage space physically it will actually be copied to the appropriate byte position in some buffer in the buffer pool this adt is similar to
the read and write methods of the randomaccessfile class discussed in
section 84


$$@@$$PAGE: 306
sec 83 buffers and buffer pools

287

example 84 assume each sector of the disk file and thus each block in
the buffer pool stores 1024 bytes assume that the buffer pool is in the
state shown in figure 85 if the next request is to copy 40 bytes beginning at position 6000 of the file these bytes should be placed into sector 5
whose bytes go from position 5120 to position 6143 because sector 5
is currently in the buffer pool we simply copy the 40 bytes contained in
space to byte positions 880919 the buffer containing sector 5 is then
moved to the buffer pool ahead of the buffer containing sector 1
an alternative interface is to have the buffer pool provide to the user a direct
pointer to a buffer that contains the requested information such an interface might
look as follows
 adt for buffer pools using the bufferpassing style
class bufferpool 
public
 return pointer to the requested block
virtual void getblockint block  0
 set the dirty bit for the buffer holding block
virtual void dirtyblockint block  0
 tell the size of a buffer
virtual int blocksize  0


in this approach the buffer pool user is made aware that the storage space is
divided into blocks of a given size where each block is the size of a buffer the user
requests specific blocks from the buffer pool with a pointer to the buffer holding
the requested block being returned to the user the user might then read from or
write to this space if the user writes to the space the buffer pool must be informed
of this fact the reason is that when a given block is to be removed from the buffer
pool the contents of that block must be written to the backing storage if it has been
modified if the block has not been modified then it is unnecessary to write it out
example 85 we wish to write 40 bytes beginning at logical position
6000 in the file assume that the buffer pool is in the state shown in figure 85 using the second adt the client would need to know that blocks
buffers are of size 1024 and therefore would request access to sector 5
a pointer to the buffer containing sector 5 would be returned by the call to
getblock the client would then copy 40 bytes to positions 880919 of
the buffer and call dirtyblock to warn the buffer pool that the contents
of this block have been modified


$$@@$$PAGE: 307
288

chap 8 file processing and external sorting

a variation on this approach is to have the getblock function take another
parameter to indicate the mode of use for the information if the mode is read
then the buffer pool assumes that no changes will be made to the buffers contents
and so no write operation need be done when the buffer is reused to store another
block if the mode is write then the buffer pool assumes that the client will not
look at the contents of the buffer and so no read from the file is necessary if the
mode is read and write then the buffer pool would read the existing contents
of the block in from disk and write the contents of the buffer to disk when the
buffer is to be reused using the mode approach the dirtyblock method is
avoided
one problem with the bufferpassing adt is the risk of stale pointers when
the buffer pool user is given a pointer to some buffer space at time t1 that pointer
does indeed refer to the desired data at that time as further requests are made to
the buffer pool it is possible that the data in any given buffer will be removed and
replaced with new data if the buffer pool user at a later time t2 then refers to the
data referred to by the pointer given at time t1 it is possible that the data are no
longer valid because the buffer contents have been replaced in the meantime thus
the pointer into the buffer pools memory has become stale to guarantee that a
pointer is not stale it should not be used if intervening requests to the buffer pool
have taken place
we can solve this problem by introducing the concept of a user or possibly
multiple users gaining access to a buffer and then releasing the buffer when done
we will add method acquirebuffer and releasebuffer for this purpose
method acquirebuffer takes a block id as input and returns a pointer to the
buffer that will be used to store this block the buffer pool will keep a count of the
number of requests currently active for this block method releasebuffer will
reduce the count of active users for the associated block buffers associated with
active blocks will not be eligible for flushing from the buffer pool this will lead
to a problem if the client neglects to release active blocks when they are no longer
needed there would also be a problem if there were more total active blocks than
buffers in the buffer pool however the buffer pool should always be initialized to
include more buffers than should ever be active at one time
an additional problem with both adts presented so far comes when the user
intends to completely overwrite the contents of a block and does not need to read
in the old contents already on disk however the buffer pool cannot in general
know whether the user wishes to use the old contents or not this is especially true
with the messagepassing approach where a given message might overwrite only
part of the block in this case the block will be read into memory even when not
needed and then its contents will be overwritten
this inefficiency can be avoided at least in the bufferpassing version by separating the assignment of blocks to buffers from actually reading in data for the


$$@@$$PAGE: 308
sec 83 buffers and buffer pools

289

block in particular the following revised bufferpassing adt does not actually
read data in the acquirebuffer method users who wish to see the old contents must then issue a readblock request to read the data from disk into the
buffer and then a getdatapointer request to gain direct access to the buffers
data contents
 a single buffer in the buffer pool
class buffer 
public
 read the associated block from disk if necessary and
 return a pointer to the data
void readblock  0
 return a pointer to the buffers data array
 without reading from disk
void getdatapointer  0
 flag the buffers contents as having changed so that
 flushing the block will write it back to disk
void markdirty  0
 release the blocks access to this buffer further
 accesses to this buffer are illegal
void releasebuffer  0

 the bufferpool
class bufferpool 
public
 constructor the bufferpool has numbuff buffers that
 each contain buffsize bytes of data
bufferpoolint numbuff int buffsize  0
 relate a block to a buffer returning a pointer to a
 buffer object
buffer acquirebufferint block  0


again a mode parameter could be added to the acquirebuffer method
eliminating the need for the readblock and markdirty methods
clearly the bufferpassing approach places more obligations on the user of the
buffer pool these obligations include knowing the size of a block not corrupting
the buffer pools storage space and informing the buffer pool both when a block
has been modified and when it is no longer needed so many obligations make this
approach prone to error an advantage is that there is no need to do an extra copy
step when getting information from the user to the buffer if the size of the records
stored is small this is not an important consideration if the size of the records is
large especially if the record size and the buffer size are the same as typically is the
case when implementing btrees see section 105 then this efficiency issue might


$$@@$$PAGE: 309
290

chap 8 file processing and external sorting

become important note however that the inmemory copy time will always be far
less than the time required to write the contents of a buffer to disk for applications
where disk io is the bottleneck for the program even the time to copy lots of
information between the buffer pool user and the buffer might be inconsequential
another advantage to buffer passing is the reduction in unnecessary read operations
for data that will be overwritten anyway
you should note that these implementations for the buffer pool adt do not use
templates instead the space parameter and the buffer pointer are declared to be
void when a class uses a template that means that the record type is arbitrary
but that the class knows what the record type is in contrast using a void pointer
for the space means that not only is the record type arbitrary but also the buffer
pool does not even know what the users record type is in fact a given buffer pool
might have many users who store many types of records
in a buffer pool the user decides where a given record will be stored but has
no control over the precise mechanism by which data are transferred to the backing
storage this is in contrast to the memory manager described in section 123 in
which the user passes a record to the manager and has no control at all over where
the record is stored

84

the programmers view of files

the c programmers logical view of a random access file is a single stream
of bytes interaction with a file can be viewed as a communications channel for
issuing one of three instructions read bytes from the current position in the file
write bytes to the current position in the file and move the current position within
the file you do not normally see how the bytes are stored in sectors clusters and
so forth the mapping from logical to physical addresses is done by the file system
and sectorlevel buffering is done automatically by the disk controller
when processing records in a disk file the order of access can have a great
effect on io time a random access procedure processes records in an order
independent of their logical order within the file sequential access processes
records in order of their logical appearance within the file sequential processing
requires less seek time if the physical layout of the disk file matches its logical
layout as would be expected if the file were created on a disk with a high percentage
of free space
c provides several mechanisms for manipulating disk files one of the most
commonly used is the fstream class the following methods can be used to
manipulate information in the file
 openchar name openmode flags open file name name for
processing flags control various details such as whether the file permits


$$@@$$PAGE: 310
sec 85 external sorting









291

reading writing or both and whether its preexisting contents should be
deleted
readchar buff int count read count bytes from the current position in the file the current file position moves forward as the bytes
are read the bytes are read into array buff which must be at least count
bytes long
writechar buff int count write count bytes at the current position in the file overwriting the bytes already at that position the
current file position moves forward as these bytes are written the bytes to
be written come from array buff
seekgint pos and seekpint pos move the current position
in the file to pos this allows bytes at arbitrary places within the file to be
read or written there are actually two current positions one for reading
and one for writing function seekg changes the get or read position
while function seekp changes the put or write position
close close a file at the end of processing

note that the spirit if this adt is similar to the message passing version of
the adt for buffer pools described in section 83

85

external sorting

we now consider the problem of sorting collections of records too large to fit in
main memory because the records must reside in peripheral or external memory
such sorting methods are called external sorts this is in contrast to the internal
sorts discussed in chapter 7 which assume that the records to be sorted are stored in
main memory sorting large collections of records is central to many applications
such as processing payrolls and other large business databases as a consequence
many external sorting algorithms have been devised years ago sorting algorithm
designers sought to optimize the use of specific hardware configurations such as
multiple tape or disk drives most computing today is done on personal computers
and lowend workstations with relatively powerful cpus but only one or at most
two disk drives the techniques presented here are geared toward optimized processing on a single disk drive this approach allows us to cover the most important
issues in external sorting while skipping many less important machinedependent
details readers who have a need to implement efficient external sorting algorithms
that take advantage of more sophisticated hardware configurations should consult
the references in section 86
when a collection of records is too large to fit in main memory the only practical way to sort it is to read some records from disk do some rearranging then
write them back to disk this process is repeated until the file is sorted with each
record read perhaps many times given the high cost of disk io it should come as


$$@@$$PAGE: 311
292

chap 8 file processing and external sorting

no surprise that the primary goal of an external sorting algorithm is to minimize the
number of times information must be read from or written to disk a certain amount
of additional cpu processing can profitably be traded for reduced disk access
before discussing external sorting techniques consider again the basic model
for accessing information from disk the file to be sorted is viewed by the programmer as a sequential series of fixedsize blocks assume for simplicity that each
block contains the same number of fixedsize data records depending on the application a record might be only a few bytes  composed of little or nothing more
than the key  or might be hundreds of bytes with a relatively small key field
records are assumed not to cross block boundaries these assumptions can be
relaxed for specialpurpose sorting applications but ignoring such complications
makes the principles clearer
as explained in section 82 a sector is the basic unit of io in other words
all disk reads and writes are for one or more complete sectors sector sizes are
typically a power of two in the range 512 to 16k bytes depending on the operating
system and the size and speed of the disk drive the block size used for external
sorting algorithms should be equal to or a multiple of the sector size
under this model a sorting algorithm reads a block of data into a buffer in main
memory performs some processing on it and at some future time writes it back to
disk from section 81 we see that reading or writing a block from disk takes on
the order of one million times longer than a memory access based on this fact we
can reasonably expect that the records contained in a single block can be sorted by
an internal sorting algorithm such as quicksort in less time than is required to read
or write the block
under good conditions reading from a file in sequential order is more efficient
than reading blocks in random order given the significant impact of seek time on
disk access it might seem obvious that sequential processing is faster however
it is important to understand precisely under what circumstances sequential file
processing is actually faster than random access because it affects our approach to
designing an external sorting algorithm
efficient sequential access relies on seek time being kept to a minimum the
first requirement is that the blocks making up a file are in fact stored on disk in
sequential order and close together preferably filling a small number of contiguous
tracks at the very least the number of extents making up the file should be small
users typically do not have much control over the layout of their file on disk but
writing a file all at once in sequential order to a disk drive with a high percentage
of free space increases the likelihood of such an arrangement
the second requirement is that the disk drives io head remain positioned
over the file throughout sequential processing this will not happen if there is
competition of any kind for the io head for example on a multiuser timeshared
computer the sorting process might compete for the io head with the processes


$$@@$$PAGE: 312
sec 85 external sorting

293

of other users even when the sorting process has sole control of the io head it
is still likely that sequential processing will not be efficient imagine the situation
where all processing is done on a single disk drive with the typical arrangement
of a single bank of readwrite heads that move together over a stack of platters if
the sorting process involves reading from an input file alternated with writing to an
output file then the io head will continuously seek between the input file and the
output file similarly if two input files are being processed simultaneously such
as during a merge process then the io head will continuously seek between these
two files
the moral is that with a single disk drive there often is no such thing as efficient sequential processing of a data file thus a sorting algorithm might be more
efficient if it performs a smaller number of nonsequential disk operations rather
than a larger number of logically sequential disk operations that require a large
number of seeks in practice
as mentioned previously the record size might be quite large compared to the
size of the key for example payroll entries for a large business might each store
hundreds of bytes of information including the name id address and job title for
each employee the sort key might be the id number requiring only a few bytes
the simplest sorting algorithm might be to process such records as a whole reading
the entire record whenever it is processed however this will greatly increase the
amount of io required because only a relatively few records will fit into a single
disk block another alternative is to do a key sort under this method the keys are
all read and stored together in an index file where each key is stored along with a
pointer indicating the position of the corresponding record in the original data file
the key and pointer combination should be substantially smaller than the size of
the original record thus the index file will be much smaller than the complete data
file the index file will then be sorted requiring much less io because the index
records are smaller than the complete records
once the index file is sorted it is possible to reorder the records in the original
database file this is typically not done for two reasons first reading the records
in sorted order from the record file requires a random access for each record this
can take a substantial amount of time and is only of value if the complete collection
of records needs to be viewed or processed in sorted order as opposed to a search
for selected records second database systems typically allow searches to be done
on multiple keys for example todays processing might be done in order of id
numbers tomorrow the boss might want information sorted by salary thus there
might be no single sorted order for the full record instead multiple index files
are often maintained one for each sort key these ideas are explored further in
chapter 10


$$@@$$PAGE: 313
294
851

chap 8 file processing and external sorting

simple approaches to external sorting

if your operating system supports virtual memory the simplest external sort is
to read the entire file into virtual memory and run an internal sorting method such
as quicksort this approach allows the virtual memory manager to use its normal
buffer pool mechanism to control disk accesses unfortunately this might not always be a viable option one potential drawback is that the size of virtual memory
is usually limited to something much smaller than the disk space available thus
your input file might not fit into virtual memory limited virtual memory can be
overcome by adapting an internal sorting method to make use of your own buffer
pool
a more general problem with adapting an internal sorting algorithm to external sorting is that it is not likely to be as efficient as designing a new algorithm
with the specific goal of minimizing disk io consider the simple adaptation of
quicksort to use a buffer pool quicksort begins by processing the entire array of
records with the first partition step moving indices inward from the two ends this
can be implemented efficiently using a buffer pool however the next step is to
process each of the subarrays followed by processing of subsubarrays and so on
as the subarrays get smaller processing quickly approaches random access to the
disk drive even with maximum use of the buffer pool quicksort still must read
and write each record log n times on average we can do much better finally
even if the virtual memory manager can give good performance using a standard
quicksort this will come at the cost of using a lot of the systems working memory which will mean that the system cannot use this space for other work better
methods can save time while also using less memory
our approach to external sorting is derived from the mergesort algorithm the
simplest form of external mergesort performs a series of sequential passes over
the records merging larger and larger sublists on each pass the first pass merges
sublists of size 1 into sublists of size 2 the second pass merges the sublists of size
2 into sublists of size 4 and so on a sorted sublist is called a run thus each pass
is merging pairs of runs to form longer runs each pass copies the contents of the
file to another file here is a sketch of the algorithm as illustrated by figure 86
1 split the original file into two equalsized run files
2 read one block from each run file into input buffers
3 take the first record from each input buffer and write a run of length two to
an output buffer in sorted order
4 take the next record from each input buffer and write a run of length two to
a second output buffer in sorted order
5 repeat until finished alternating output between the two output run buffers
whenever the end of an input block is reached read the next block from the
appropriate input file when an output buffer is full write it to the appropriate
output file


$$@@$$PAGE: 314
295

sec 85 external sorting

36 17 28 23

20 36 14 28

13 17 20 36

20 13 14 15

13 17 15 23

14 15 23 28

runs of length 1

runs of length 2

runs of length 4

figure 86 a simple external mergesort algorithm input records are divided
equally between two input files the first runs from each input file are merged and
placed into the first output file the second runs from each input file are merged
and placed in the second output file merging alternates between the two output
files until the input files are empty the roles of input and output files are then
reversed allowing the runlength to be doubled with each pass

6 repeat steps 2 through 5 using the original output files as input files on the
second pass the first two records of each input run file are already in sorted
order thus these two runs may be merged and output as a single run of four
elements
7 each pass through the run files provides larger and larger runs until only one
run remains

example 86 using the input of figure 86 we first create runs of length
one split between two input files we then process these two input files
sequentially making runs of length two the first run has the values 20 and
36 which are output to the first output file the next run has 13 and 17
which is output to the second file the run 14 28 is sent to the first file
then run 15 23 is sent to the second file and so on once this pass has
completed the roles of the input files and output files are reversed the
next pass will merge runs of length two into runs of length four runs 20
36 and 13 17 are merged to send 13 17 20 36 to the first output file then
runs 14 28 and 15 23 are merged to send run 14 15 23 28 to the second
output file in the final pass these runs are merged to form the final run 13
14 15 17 20 23 28 36
this algorithm can easily take advantage of the double buffering techniques
described in section 83 note that the various passes read the input run files sequentially and write the output run files sequentially for sequential processing and
double buffering to be effective however it is necessary that there be a separate
io head available for each file this typically means that each of the input and
output files must be on separate disk drives requiring a total of four disk drives for
maximum efficiency


$$@@$$PAGE: 315
296

chap 8 file processing and external sorting

the external mergesort algorithm just described requires that log n passes be
made to sort a file of n records thus each record must be read from disk and
written to disk log n times the number of passes can be significantly reduced by
observing that it is not necessary to use mergesort on small runs a simple modification is to read in a block of data sort it in memory perhaps using quicksort
and then output it as a single sorted run
example 87 assume that we have blocks of size 4kb and records are
eight bytes with four bytes of data and a 4byte key thus each block contains 512 records standard mergesort would require nine passes to generate runs of 512 records whereas processing each block as a unit can be done
in one pass with an internal sort these runs can then be merged by mergesort standard mergesort requires eighteen passes to process 256k records
using an internal sort to create initial runs of 512 records reduces this to
one initial pass to create the runs and nine merge passes to put them all
together approximately half as many passes
we can extend this concept to improve performance even further available
main memory is usually much more than one block in size if we process larger
initial runs then the number of passes required by mergesort is further reduced for
example most modern computers can provide tens or even hundreds of megabytes
of ram to the sorting program if all of this memory excepting a small amount for
buffers and local variables is devoted to building initial runs as large as possible
then quite large files can be processed in few passes the next section presents a
technique for producing large runs typically twice as large as could fit directly into
main memory
another way to reduce the number of passes required is to increase the number
of runs that are merged together during each pass while the standard mergesort
algorithm merges two runs at a time there is no reason why merging needs to be
limited in this way section 853 discusses the technique of multiway merging
over the years many variants on external sorting have been presented but all
are based on the following two steps
1 break the file into large initial runs
2 merge the runs together to form a single sorted file
852

replacement selection

this section treats the problem of creating initial runs as large as possible from a
disk file assuming a fixed amount of ram is available for processing as mentioned previously a simple approach is to allocate as much ram as possible to a
large array fill this array from disk and sort the array using quicksort thus if


$$@@$$PAGE: 316
297

sec 85 external sorting

input
file

input buffer

ram

output buffer

output
run file

figure 87 overview of replacement selection input records are processed sequentially initially ram is filled with m records as records are processed they
are written to an output buffer when this buffer becomes full it is written to disk
meanwhile as replacement selection needs records it reads them from the input
buffer whenever this buffer becomes empty the next block of records is read
from disk

the size of memory available for the array is m records then the input file can be
broken into initial runs of length m  a better approach is to use an algorithm called
replacement selection that on average creates runs of 2m records in length replacement selection is actually a slight variation on the heapsort algorithm the
fact that heapsort is slower than quicksort is irrelevant in this context because io
time will dominate the total running time of any reasonable external sorting algorithm building longer initial runs will reduce the total io time required
replacement selection views ram as consisting of an array of size m in addition to an input buffer and an output buffer additional io buffers might be
desirable if the operating system supports double buffering because replacement
selection does sequential processing on both its input and its output imagine that
the input and output files are streams of records replacement selection takes the
next record in sequential order from the input stream when needed and outputs
runs one record at a time to the output stream buffering is used so that disk io is
performed one block at a time a block of records is initially read and held in the
input buffer replacement selection removes records from the input buffer one at
a time until the buffer is empty at this point the next block of records is read in
output to a buffer is similar once the buffer fills up it is written to disk as a unit
this process is illustrated by figure 87
replacement selection works as follows assume that the main processing is
done in an array of size m records
1 fill the array from disk set last  m  1
2 build a minheap recall that a minheap is defined such that the record at
each node has a key value less than the key values of its children
3 repeat until the array is empty
a send the record with the minimum key value the root to the output
buffer


$$@@$$PAGE: 317
298

chap 8 file processing and external sorting

b let r be the next record in the input buffer if rs key value is greater
than the key value just output 
i then place r at the root
ii else replace the root with the record in array position last and
place r at position last set last  last  1
c sift down the root to reorder the heap
when the test at step 3b is successful a new record is added to the heap
eventually to be output as part of the run as long as records coming from the input
file have key values greater than the last key value output to the run they can be
safely added to the heap records with smaller key values cannot be output as part
of the current run because they would not be in sorted order such values must be
stored somewhere for future processing as part of another run however because
the heap will shrink by one element in this case there is now a free space where the
last element of the heap used to be thus replacement selection will slowly shrink
the heap and at the same time use the discarded heap space to store records for the
next run once the first run is complete ie the heap becomes empty the array
will be filled with records ready to be processed for the second run figure 88
illustrates part of a run being created by replacement selection
it should be clear that the minimum length of a run will be m records if the size
of the heap is m  because at least those records originally in the heap will be part of
the run under good conditions eg if the input is sorted then an arbitrarily long
run is possible in fact the entire file could be processed as one run if conditions
are bad eg if the input is reverse sorted then runs of only size m result
what is the expected length of a run generated by replacement selection it
can be deduced from an analogy called the snowplow argument imagine that a
snowplow is going around a circular track during a heavy but steady snowstorm
after the plow has been around at least once snow on the track must be as follows
immediately behind the plow the track is empty because it was just plowed the
greatest level of snow on the track is immediately in front of the plow because
this is the place least recently plowed at any instant there is a certain amount of
snow s on the track snow is constantly falling throughout the track at a steady
rate with some snow falling in front of the plow and some behind the plow
on a circular track everything is actually in front of the plow but figure 89
illustrates the idea during the next revolution of the plow all snow s on the track
is removed plus half of what falls because everything is assumed to be in steady
state after one revolution s snow is still on the track so 2s snow must fall during
a revolution and 2s snow is removed during a revolution leaving s snow behind
at the beginning of replacement selection nearly all values coming from the
input file are greater ie in front of the plow than the latest key value output for
this run because the runs initial key values should be small as the run progresses


$$@@$$PAGE: 318
299

sec 85 external sorting

input

memory

output

12
16

19
25

12

31
21

56

40

16
19

29
25

31
21

56

16
40

29
19
25

31
21

56

40

19
14

21
25

19

31
29

56

40

40
21
25

31
29

56

14

21
25

35
40

31
29

56

21
14

figure 88 replacement selection example after building the heap root
value 12 is output and incoming value 16 replaces it value 16 is output next
replaced with incoming value 29 the heap is reordered with 19 rising to the
root value 19 is output next incoming value 14 is too small for this run and
is placed at end of the array moving value 40 to the root reordering the heap
results in 21 rising to the root which is output next


$$@@$$PAGE: 319
300

chap 8 file processing and external sorting

falling snow
future snow
existing snow
snowplow movement
start time t
figure 89 the snowplow analogy showing the action during one revolution of
the snowplow a circular track is laid out straight for purposes of illustration and
is shown in cross section at any time t  the most snow is directly in front of
the snowplow as the plow moves around the track the same amount of snow is
always in front of the plow as the plow moves forward less of this is snow that
was in the track at time t  more is snow that has fallen since

the latest key value output becomes greater and so new key values coming from the
input file are more likely to be too small ie after the plow such records go to
the bottom of the array the total length of the run is expected to be twice the size of
the array of course this assumes that incoming key values are evenly distributed
within the key range in terms of the snowplow analogy we assume that snow falls
evenly throughout the track sorted and reverse sorted inputs do not meet this
expectation and so change the length of the run
853

multiway merging

the second stage of a typical external sorting algorithm merges the runs created by
the first stage assume that we have r runs to merge if a simple twoway merge
is used then r runs regardless of their sizes will require log r passes through
the file while r should be much less than the total number of records because
the initial runs should each contain many records we would like to reduce still
further the number of passes required to merge the runs together note that twoway merging does not make good use of available memory because merging is a
sequential process on the two runs only one block of records per run need be in
memory at a time keeping more than one block of a run in memory at any time
will not reduce the disk io required by the merge process though if several blocks
are read from a file at once time at least they take advantage of sequential access
thus most of the space just used by the heap for replacement selection typically
many blocks in length is not being used by the merge process
we can make better use of this space and at the same time greatly reduce the
number of passes needed to merge the runs if we merge several runs at a time
multiway merging is similar to twoway merging if we have b runs to merge
with a block from each run available in memory then the bway merge algorithm


$$@@$$PAGE: 320
301

sec 85 external sorting

input runs
5 10 15 
output buffer
6

7 23 

5

6

7 10 12 

12 18 20 
figure 810 illustration of multiway merge the first value in each input run
is examined and the smallest sent to the output this value is removed from the
input and the process repeated in this example values 5 6 and 12 are compared
first value 5 is removed from the first run and sent to the output values 10 6
and 12 will be compared next after the first five values have been output the
current value of each block is the one underlined

simply looks at b values the frontmost value for each input run and selects the
smallest one to output this value is removed from its run and the process is
repeated when the current block for any run is exhausted the next block from that
run is read from disk figure 810 illustrates a multiway merge
conceptually multiway merge assumes that each run is stored in a separate
file however this is not necessary in practice we only need to know the position
of each run within a single file and useseekg to move to the appropriate block
whenever we need new data from a particular run naturally this approach destroys the ability to do sequential processing on the input file however if all runs
were stored on a single disk drive then processing would not be truly sequential
anyway because the io head would be alternating between the runs thus multiway merging replaces several potentially sequential passes with a single random
access pass if the processing would not be sequential anyway such as when all
processing is on a single disk drive no time is lost by doing so
multiway merging can greatly reduce the number of passes required if there
is room in memory to store one block for each run then all runs can be merged
in a single pass thus replacement selection can build initial runs in one pass
and multiway merging can merge all runs in one pass yielding a total cost of two
passes however for truly large files there might be too many runs for each to get
a block in memory if there is room to allocate b blocks for a bway merge and
the number of runs r is greater than b then it will be necessary to do multiple
merge passes in other words the first b runs are merged then the next b and
so on these superruns are then merged by subsequent passes b superruns at a
time


$$@@$$PAGE: 321
302

chap 8 file processing and external sorting

how big a file can be merged in one pass assuming b blocks were allocated to
the heap for replacement selection resulting in runs of average length 2b blocks
followed by a bway merge we can process on average a file of size 2b 2 blocks
in a single multiway merge 2b k1 blocks on average can be processed in k bway merges to gain some appreciation for how quickly this grows assume that
we have available 05mb of working memory and that a block is 4kb yielding
128 blocks in working memory the average run size is 1mb twice the working
memory size in one pass 128 runs can be merged thus a file of size 128mb
can on average be processed in two passes one to build the runs one to do the
merge with only 05mb of working memory as another example assume blocks
are 1kb long and working memory is 1mb  1024 blocks then 1024 runs of
average length 2mb which is about 2gb can be combined in a single merge
pass a larger block size would reduce the size of the file that can be processed
in one merge pass for a fixedsize working memory a smaller block size or larger
working memory would increase the file size that can be processed in one merge
pass two merge passes allow much bigger files to be processed with 05mb of
working memory and 4kb blocks a file of size 16 gigabytes could be processed in
two merge passes which is big enough for most applications thus this is a very
effective algorithm for single disk drive external sorting
figure 811 shows a comparison of the running time to sort varioussized files
for the following implementations 1 standard mergesort with two input runs and
two output runs 2 twoway mergesort with large initial runs limited by the size
of available memory and 3 rway mergesort performed after generating large
initial runs in each case the file was composed of a series of fourbyte records
a twobyte key and a twobyte data value or 256k records per megabyte of file
size we can see from this table that using even a modest memory size two blocks
to create initial runs results in a tremendous savings in time doing 4way merges
of the runs provides another considerable speedup however largescale multiway
merges for r beyond about 4 or 8 runs does not help much because a lot of time is
spent determining which is the next smallest element among the r runs
we see from this experiment that building large initial runs reduces the running
time to slightly more than one third that of standard mergesort depending on file
and memory sizes using a multiway merge further cuts the time nearly in half
in summary a good external sorting algorithm will seek to do the following
 make the initial runs as long as possible
 at all stages overlap input processing and output as much as possible
 use as much working memory as possible applying more memory usually
speeds processing in fact more memory will have a greater effect than a
faster disk a faster cpu is unlikely to yield much improvement in running
time for external sorting because disk io speed is the limiting factor


$$@@$$PAGE: 322
303

sec 86 further reading

file
size
mb
1
4
16
256

sort 1

061
4864
256
21504
1128
94208
22039
1769k

sort 2
memory size in blocks
2
4
16
256
027
024
019
010
2048
1792
1280
256
130
119
096
061
10240
9216
7168
3072
612
563
478
336
49152 45056 36864 20480
13247 12368 11001 8666
1048k 983k
852k
589k

sort 3
memory size in blocks
2
4
16
021
015
013
2048
1024
512
115
068
066
10240
5120
2048
542
319
310
49152 24516 12288
11573
6931
6871
1049k 524k
262k

figure 811 a comparison of three external sorts on a collection of small records
for files of various sizes each entry in the table shows time in seconds and total
number of blocks read and written by the program file sizes are in megabytes
for the third sorting algorithm on a file size of 4mb the time and blocks shown
in the last column are for a 32way merge marked with an asterisk 32 is used
instead of 16 because 32 is a root of the number of blocks in the file while 16 is
not thus allowing the same number of runs to be merged at every pass

 if possible use additional disk drives for more overlapping of processing
with io and to allow for sequential file processing

86

further reading

a good general text on file processing is folk and zoellicks file structures a
conceptual toolkit fz98 a somewhat more advanced discussion on key issues in
file processing is betty salzbergs file structures an analytical approach sal88
a great discussion on external sorting methods can be found in salzbergs book
the presentation in this chapter is similar in spirit to salzbergs
for details on disk drive modeling and measurement see the article by ruemmler and wilkes an introduction to disk drive modeling rw94 see andrew
s tanenbaums structured computer organization tan06 for an introduction to
computer hardware and organization an excellent detailed description of memory and hard disk drives can be found online at the pc guide by charles m
kozierok koz05 wwwpcguidecom the pc guide also gives detailed descriptions of the microsoft windows and unix linux file systems
see outperforming lru with an adaptive replacement cache algorithm
by megiddo and modha mm04 for an example of a more sophisticated algorithm
than lru for managing buffer pools
the snowplow argument comes from donald e knuths sorting and searching
knu98 which also contains a wide variety of external sorting algorithms


$$@@$$PAGE: 323
304

87

chap 8 file processing and external sorting

exercises

81 computer memory and storage prices change rapidly find out what the
current prices are for the media listed in figure 81 does your information
change any of the basic conclusions regarding disk processing
82 assume a disk drive from the late 1990s is configured as follows the total storage is approximately 675mb divided among 15 surfaces each surface has 612 tracks there are 144 sectorstrack 512 bytessector and 8 sectorscluster the disk turns at 3600 rpm the tracktotrack seek time is
20 ms and the average seek time is 80 ms now assume that there is a
360kb file on the disk on average how long does it take to read all of the
data in the file assume that the first track of the file is randomly placed on
the disk that the entire file lies on adjacent tracks and that the file completely
fills each track on which it is found a seek must be performed each time the
io head moves to a new track show your calculations
83 using the specifications for the disk drive given in exercise 82 calculate the
expected time to read one entire track one sector and one byte show your
calculations
84 using the disk drive specifications given in exercise 82 calculate the time
required to read a 10mb file assuming
a the file is stored on a series of contiguous tracks as few tracks as possible
b the file is spread randomly across the disk in 4kb clusters
show your calculations
85 assume that a disk drive is configured as follows the total storage is approximately 1033mb divided among 15 surfaces each surface has 2100
tracks there are 64 sectorstrack 512 bytessector and 8 sectorscluster the
disk turns at 7200 rpm the tracktotrack seek time is 3 ms and the average
seek time is 20 ms now assume that there is a 512kb file on the disk on
average how long does it take to read all of the data on the file assume that
the first track of the file is randomly placed on the disk that the entire file lies
on contiguous tracks and that the file completely fills each track on which it
is found show your calculations
86 using the specifications for the disk drive given in exercise 85 calculate the
expected time to read one entire track one sector and one byte show your
calculations
87 using the disk drive specifications given in exercise 85 calculate the time
required to read a 10mb file assuming
a the file is stored on a series of contiguous tracks as few tracks as possible
b the file is spread randomly across the disk in 4kb clusters


$$@@$$PAGE: 324
sec 87 exercises

88

89

810

811

812

813

3

305

show your calculations
a typical disk drive from 2004 has the following specifications3 the total
storage is approximately 120gb on 6 platter surfaces or 20gbplatter each
platter has 16k tracks with 2560 sectorstrack a sector holds 512 bytes and
16 sectorscluster the disk turns at 7200 rpm the tracktotrack seek time
is 20 ms and the average seek time is 100 ms now assume that there is a
6mb file on the disk on average how long does it take to read all of the data
on the file assume that the first track of the file is randomly placed on the
disk that the entire file lies on contiguous tracks and that the file completely
fills each track on which it is found show your calculations
using the specifications for the disk drive given in exercise 88 calculate the
expected time to read one entire track one sector and one byte show your
calculations
using the disk drive specifications given in exercise 88 calculate the time
required to read a 10mb file assuming
a the file is stored on a series of contiguous tracks as few tracks as possible
b the file is spread randomly across the disk in 8kb clusters
show your calculations
at the end of 2004 the fastest disk drive i could find specifications for was
the maxtor atlas this drive had a nominal capacity of 734gb using 4 platters 8 surfaces or 9175gbsurface assume there are 16384 tracks with an
average of 1170 sectorstrack and 512 bytessector4 the disk turns at 15000
rpm the tracktotrack seek time is 04 ms and the average seek time is 36
ms how long will it take on average to read a 6mb file assuming that the
first track of the file is randomly placed on the disk that the entire file lies on
contiguous tracks and that the file completely fills each track on which it is
found show your calculations
using the specifications for the disk drive given in exercise 811 calculate
the expected time to read one entire track one sector and one byte show
your calculations
using the disk drive specifications given in exercise 811 calculate the time
required to read a 10mb file assuming
a the file is stored on a series of contiguous tracks as few tracks as possible

to make the exercise doable this specification is completely fictitious with respect to the track
and sector layout while sectors do have 512 bytes and while the number of platters and amount of
data per track is plausible the reality is that all modern drives use a zoned organization to keep the
data density from inside to outside of the disk reasonably high the rest of the numbers are typical
for a drive from 2004
4
again this track layout does does not account for the zoned arrangement on modern disk drives


$$@@$$PAGE: 325
306

chap 8 file processing and external sorting

b the file is spread randomly across the disk in 8kb clusters
show your calculations
814 prove that two tracks selected at random from a disk are separated on average
by one third the number of tracks on the disk
815 assume that a file contains one million records sorted by key value a query
to the file returns a single record containing the requested key value files
are stored on disk in sectors each containing 100 records assume that the
average time to read a sector selected at random is 100 ms in contrast it
takes only 20 ms to read the sector adjacent to the current position of the io
head the batch algorithm for processing queries is to first sort the queries
by order of appearance in the file and then read the entire file sequentially
processing all queries in sequential order as the file is read this algorithm
implies that the queries must all be available before processing begins the
interactive algorithm is to process each query in order of its arrival searching for the requested sector each time unless by chance two queries in a row
are to the same sector carefully define under what conditions the batch
method is more efficient than the interactive method
816 assume that a virtual memory is managed using a buffer pool the buffer
pool contains five buffers and each buffer stores one block of data memory
accesses are by block id assume the following series of memory accesses
takes place
5 2 5 12 3 6 5 9 3 2 4 1 5 9 8 15 3 7 2 5 9 10 4 6 8 5
for each of the following buffer pool replacement strategies show the contents of the buffer pool at the end of the series and indicate how many times
a block was found in the buffer pool instead of being read into memory
assume that the buffer pool is initially empty
a firstin first out
b least frequently used with counts kept only for blocks currently in
memory counts for a page are lost when that page is removed and the
oldest item with the smallest count is removed when there is a tie
c least frequently used with counts kept for all blocks and the oldest
item with the smallest count is removed when there is a tie
d least recently used
e most recently used replace the block that was most recently accessed
817 suppose that a record is 32 bytes a block is 1024 bytes thus there are
32 records per block and that working memory is 1mb there is also additional space available for io buffers program variables etc what is the
expected size for the largest file that can be merged using replacement selection followed by a single pass of multiway merge explain how you got your
answer


$$@@$$PAGE: 326
sec 88 projects

307

818 assume that working memory size is 256kb broken into blocks of 8192
bytes there is also additional space available for io buffers program variables etc what is the expected size for the largest file that can be merged
using replacement selection followed by two passes of multiway merge explain how you got your answer
819 prove or disprove the following proposition given space in memory for a
heap of m records replacement selection will completely sort a file if no
record in the file is preceded by m or more keys of greater value
820 imagine a database containing ten million records with each record being
100 bytes long provide an estimate of the time it would take in seconds to
sort the database on a typical desktop or laptop computer
821 assume that a company has a computer configuration satisfactory for processing their monthly payroll further assume that the bottleneck in payroll
processing is a sorting operation on all of the employee records and that
an external sorting algorithm is used the companys payroll program is so
good that it plans to hire out its services to do payroll processing for other
companies the president has an offer from a second company with 100
times as many employees she realizes that her computer is not up to the
job of sorting 100 times as many records in an acceptable amount of time
describe what impact each of the following modifications to the computing
system is likely to have in terms of reducing the time required to process the
larger payroll database
a
b
c
d

a factor of two speedup to the cpu
a factor of two speedup to disk io time
a factor of two speedup to main memory access time
a factor of two increase to main memory size

822 how can the external sorting algorithm described in this chapter be extended
to handle variablelength records

88

projects

81 for a database application assume it takes 10 ms to read a block from disk
1 ms to search for a record in a block stored in memory and that there is
room in memory for a buffer pool of 5 blocks requests come in for records
with the request specifying which block contains the record if a block is
accessed there is a 10 probability for each of the next ten requests that the
request will be to the same block what will be the expected performance
improvement for each of the following modifications to the system
a get a cpu that is twice as fast
b get a disk drive that is twice as fast


$$@@$$PAGE: 327
308

chap 8 file processing and external sorting

c get enough memory to double the buffer pool size
write a simulation to analyze this problem
82 pictures are typically stored as an array row by row on disk consider the
case where the picture has 16 colors thus each pixel can be represented using 4 bits if you allow 8 bits per pixel no processing is required to unpack
the pixels because a pixel corresponds to a byte the lowest level of addressing on most machines if you pack two pixels per byte space is saved but
the pixels must be unpacked which takes more time to read from disk and
access every pixel of the image 8 bits per pixel or 4 bits per pixel with
2 pixels per byte program both and compare the times
83 implement a diskbased buffer pool class based on the lru buffer pool replacement strategy disk blocks are numbered consecutively from the beginning of the file with the first block numbered as 0 assume that blocks are
4096 bytes in size with the first 4 bytes used to store the block id corresponding to that buffer use the first bufferpool abstract class given in
section 83 as the basis for your implementation
84 implement an external sort based on replacement selection and multiway
merging as described in this chapter test your program both on files with
small records and on files with large records for what size record do you
find that key sorting would be worthwhile
85 implement a quicksort for large files on disk by replacing all array access in
the normal quicksort application with access to a virtual array implemented
using a buffer pool that is whenever a record in the array would be read or
written by quicksort use a call to a buffer pool function instead compare
the running time of this implementation with implementations for external
sorting based on mergesort as described in this chapter
86 section 851 suggests that an easy modification to the basic 2way mergesort
is to read in a large chunk of data into main memory sort it with quicksort
and write it out for initial runs then a standard 2way merge is used in
a series of passes to merge the runs together however this makes use of
only two blocks of working memory at a time each block read is essentially
random access because the various files are read in an unknown order even
though each of the input and output files is processed sequentially on each
pass a possible improvement would be on the merge passes to divide
working memory into four equal sections one section is allocated to each
of the two input files and two output files all reads during merge passes
would be in full sections rather than single blocks while the total number
of blocks read and written would be the same as a regular 2way mergesort it
is possible that this would speed processing because a series of blocks that are
logically adjacent in the various input and output files would be readwritten
each time implement this variation and compare its running time against


$$@@$$PAGE: 328
sec 88 projects

309

a standard series of 2way merge passes that readwrite only a single block
at a time before beginning implementation write down your hypothesis on
how the running time will be affected by this change after implementing
did you find that this change has any meaningful effect on performance


$$@@$$PAGE: 329

$$@@$$PAGE: 330
9
searching

organizing and retrieving information is at the heart of most computer applications and searching is surely the most frequently performed of all computing tasks
search can be viewed abstractly as a process to determine if an element with a particular value is a member of a particular set the more common view of searching
is an attempt to find the record within a collection of records that has a particular
key value or those records in a collection whose key values meet some criterion
such as falling within a range of values
we can define searching formally as follows suppose that we have a collection
l of n records of the form
k1  i1  k2  i2   kn  in 
where ij is information associated with key kj from record j for 1  j  n given
a particular key value k the search problem is to locate a record kj  ij  in l
such that kj  k if one exists searching is a systematic method for locating the
record or records with key value kj  k
a successful search is one in which a record with key kj  k is found an
unsuccessful search is one in which no record with kj  k is found and no such
record exists
an exactmatch query is a search for the record whose key value matches a
specified key value a range query is a search for all records whose key value falls
within a specified range of key values
we can categorize search algorithms into three general approaches
1 sequential and list methods
2 direct access by key value hashing
3 tree indexing methods
this and the following chapter treat these three approaches in turn any of
these approaches are potentially suitable for implementing the dictionary adt
311


$$@@$$PAGE: 331
312

chap 9 searching

introduced in section 44 however each has different performance characteristics
that make it the method of choice in particular circumstances
the current chapter considers methods for searching data stored in lists list in
this context means any list implementation including a linked list or an array most
of these methods are appropriate for sequences ie duplicate key values are allowed although special techniques applicable to sets are discussed in section 93
the techniques from the first three sections of this chapter are most appropriate for
searching a collection of records stored in ram section 94 discusses hashing
a technique for organizing data in an array such that the location of each record
within the array is a function of its key value hashing is appropriate when records
are stored either in ram or on disk
chapter 10 discusses treebased methods for organizing information on disk
including a commonly used file structure called the btree nearly all programs that
must organize large collections of records stored on disk use some variant of either
hashing or the btree hashing is practical for only certain access functions exactmatch queries and is generally appropriate only when duplicate key values are
not allowed btrees are the method of choice for dynamic diskbased applications
anytime hashing is not appropriate

91

searching unsorted and sorted arrays

the simplest form of search has already been presented in example 31 the sequential search algorithm sequential search on an unsorted list requires n time
in the worst case
how many comparisons does linear search do on average a major consideration is whether k is in list l at all we can simplify our analysis by ignoring
everything about the input except the position of k if it is found in l thus we have
n  1 distinct possible events that k is in one of positions 0 to n  1 in l each
position having its own probability or that it is not in l at all we can express the
probability that k is not in l as
pk 
 l  1 

n
x

pk  li

i1

where px is the probability of event x
let pi be the probability that k is in position i of l indexed from 0 to n  1
for any position i in the list we must look at i  1 records to reach it so we say
that the cost when k is in position i is i  1 when k is not in l sequential search
will require n comparisons let pn be the probability that k is not in l then the
average cost tn will be


$$@@$$PAGE: 332
313

sec 91 searching unsorted and sorted arrays

tn  npn 

n1
x

i  1pi 

i0

what happens to the equation if we assume all the pi s are equal except p0 

tn  pn n 

n1
x

i  1p

i0
n
x

 pn n  p

i

i1

nn  1
2
1  pn nn  1
 pn n 
n
2
n  1  pn n  1

2
 pn n  p

depending on the value of pn  n1
2  tn  n
for large collections of records that are searched repeatedly sequential search
is unacceptably slow one way to reduce search time is to preprocess the records
by sorting them given a sorted array an obvious improvement over simple linear
search is to test if the current element in l is greater than k if it is then we know
that k cannot appear later in the array and we can quit the search early but this
still does not improve the worstcase cost of the algorithm
we can also observe that if we look first at position 1 in sorted array l and find
that k is bigger then we rule out position 0 as well as position 1 because more
is often better what if we look at position 2 in l and find that k is bigger yet
this rules out positions 0 1 and 2 with one comparison what if we carry this to
the extreme and look first at the last position in l and find that k is bigger then
we know in one comparison that k is not in l this is very useful to know but
what is wrong with the conclusion that we should always start by looking at the last
position the problem is that while we learn a lot sometimes in one comparison
we might learn that k is not in the list usually we learn only a little bit that the
last element is not k
the question then becomes what is the right amount to jump this leads us
to an algorithm known as jump search for some value j we check every jth
element in l that is we check elements lj l2j and so on so long as k is
greater than the values we are checking we continue on but when we reach a


$$@@$$PAGE: 333
314

chap 9 searching

value in l greater than k we do a linear search on the piece of length j  1 that
we know brackets k if it is in the list
if we define m such that mj  n  m  1j then the total cost of this
algorithm is at most m  j  1 3way comparisons they are 3way because at
each comparison of k with some li we need to know if k is less than equal to
or greater than li therefore the cost to run the algorithm on n items with a
jump of size j is
 
n
tn j  m  j  1 
 j  1
j
what is the best value that we can pick for j we want to minimize the cost
 

n
min
j1
1jn
j
take the derivative and solve for f 0 j  0 to find the minimum which is


j  n in this case the worst case cost will be roughly 2 n
this example invokes a basic principle of algorithm design we want to balance the work done while selecting a sublist with the work done while searching a
sublist in general it is a good strategy to make subproblems of equal effort this
is an example of a divide and conquer algorithm
what if we extend this idea to three levels we would first make jumps of
some size j to find a sublist of size j  1 whose end values bracket value k we
would then work through this sublist by making jumps of some smaller size say
j1  finally once we find a bracketed sublist of size j1  1 we would do sequential
search to complete the process
this probably sounds convoluted to do two levels of jumping to be followed by
a sequential search while it might make sense to do a twolevel algorithm that is
jump search jumps to find a sublist and then does sequential search on the sublist
it almost never seems to make sense to do a threelevel algorithm instead when
we go beyond two levels we nearly always generalize by using recursion this
leads us to the most commonly used search algorithm for sorted arrays the binary
search described in section 35
if we know nothing about the distribution of key values then binary search
is the best algorithm available for searching a sorted array see exercise 922
however sometimes we do know something about the expected key distribution
consider the typical behavior of a person looking up a word in a large dictionary
most people certainly do not use sequential search typically people use a modified form of binary search at least until they get close to the word that they are
looking for the search generally does not start at the middle of the dictionary a
person looking for a word starting with s generally assumes that entries beginning
with s start about three quarters of the way through the dictionary thus he or


$$@@$$PAGE: 334
315

sec 91 searching unsorted and sorted arrays

she will first open the dictionary about three quarters of the way through and then
make a decision based on what is found as to where to look next in other words
people typically use some knowledge about the expected distribution of key values
to compute where to look next this form of computed binary search is called
a dictionary search or interpolation search in a dictionary search we search l
at a position p that is appropriate to the value of k as follows
p

k  l1
ln  l1

this equation is computing the position of k as a fraction of the distance between the smallest and largest key values this will next be translated into that
position which is the same fraction of the way through the array and this position
is checked first as with binary search the value of the key found eliminates all
records either above or below that position the actual value of the key found can
then be used to compute a new position within the remaining range of the array
the next check is made based on the new computation this proceeds until either
the desired record is found or the array is narrowed until no records are left
a variation on dictionary search is known as quadratic binary search qbs
and we will analyze this in detail because its analysis is easier than that of the
general dictionary search qbs will first compute p and then examine ldpne if

k  ldpne then qbs will sequentially probe to the left by steps of size n that
is we step through

ldpn  i ne i  1 2 3 
until we reach a value less than or equal to k similarly for k  ldpne we will

step to the right by n until we reach a value in l that is greater than k we are

now within n positions of k assume for now that it takes a constant number of

comparisons to bracket k within a sublist of size n we then take this sublist and
repeat the process recursively that is at the next level we compute an interpolation
to start somewhere
pin the subarray we then step to the left or right as appropriate
by steps of size
n

what is the cost for qbs note that cn  cn2  and we will be repeatedly
taking square roots of the current sublist size until we find the item that we are
looking for because n  2log n and we can cut log n in half only log log n times
the cost is log log n if the number of probes on jump search is constant
say that the number of comparisons needed is i in which case the cost is i
since we have to do i comparisons if pi is the probability of needing exactly i
probes then

n
x
ipneed exactly i probes
i1

 1p1  2p2  3p3     



npn


$$@@$$PAGE: 335
316

chap 9 searching

we now show that this is the same as


n
x

pneed at least i probes

i1

 1  1  p1   1  p1  p2       pn
 p1    pn   p2    pn  
p3    pn     

 1p1  2p2  3p3      npn
we require at least two probes to set the bounds so the cost is


2

n
x

pneed at least i probes

i3

we now make take advantage of a useful fact known as cebysevs inequality
cebysevs inequality states that pneed exactly i probes or pi  is
pi 

p1  pn
1

2
i  2 n
4i  22

because p1  p  14 for any probability p this assumes uniformly distributed
data thus the expected number of probes is


2

n
x
i3



1x 1
1
1

2

2
 24112
2
2
4i  2
4
i
46
i1

is qbs better than binary search theoretically yes because olog log n
grows slower than olog n however we have a situation here which illustrates
the limits to the model of asymptotic complexity in some practical situations yes
c1 log n does grow faster than c2 log log n in fact it is exponentially faster but
even so for practical input sizes the absolute cost difference is fairly small thus
the constant factors might play a role first we compare lg lg n to lg n
n
16
256
216
232

factor
lg n lg lg n difference
4
2
2
8
3
27
16
4
4
32
5
64


$$@@$$PAGE: 336
317

sec 92 selforganizing lists

it is not always practical to reduce an algorithms growth rate there is a practicality window for every problem in that we have a practical limit to how big an
input we wish to solve for if our problem size never grows too big it might not
matter if we can reduce the cost by an extra log factor because the constant factors
in the two algorithms might differ by more than the log of the log of the input size
for our two algorithms let us look further and check the actual number of
comparisons used for binary search we need about log n  1 total comparisons
quadratic binary search requires about 24 lg lg n comparisons if we incorporate
this observation into our table we get a different picture about the relative differences
n
16
256
64k
232

lg n  1
3
7
15
31

factor
24 lg lg n difference
48
worse
72
 same
96
16
12
26

but we still are not done this is only a count of raw comparisons binary search is inherently much simpler than qbs because binary search only
needs to calculate the midpoint position of the array before each comparison while
quadratic binary search must calculate an interpolation point which is more expensive so the constant factors for qbs are even higher
not only are the constant factors worse on average but qbs is far more dependent than binary search on good data distribution to perform well for example
imagine that you are searching a telephone directory for the name young normally you would look near the back of the book if you found a name beginning
with z you might look just a little ways toward the front if the next name you
find also begins with z you would look a little further toward the front if this
particular telephone directory were unusual in that half of the entries begin with z
then you would need to move toward the front many times each time eliminating
relatively few records from the search in the extreme the performance of interpolation search might not be much better than sequential search if the distribution of
key values is badly calculated
while it turns out that qbs is not a practical algorithm this is not a typical
situation fortunately algorithm growth rates are usually well behaved so that asymptotic algorithm analysis nearly always gives us a practical indication for which
of two algorithms is better

92

selforganizing lists

while ordering of lists is most commonly done by key value this is not the only
viable option another approach to organizing lists to speed search is to order the


$$@@$$PAGE: 337
318

chap 9 searching

records by expected frequency of access while the benefits might not be as great
as when organized by key value the cost to organize at least approximately by
frequency of access can be much cheaper and thus can speed up sequential search
in some situations
assume that we know for each key ki  the probability pi that the record with
key ki will be requested assume also that the list is ordered so that the most
frequently requested record is first then the next most frequently requested record
and so on search in the list will be done sequentially beginning with the first
position over the course of many searches the expected number of comparisons
required for one search is
c n  1p0  2p1    npn1 
in other words the cost to access the record in l0 is 1 because one key value is
looked at and the probability of this occurring is p0  the cost to access the record
in l1 is 2 because we must look at the first and the second records key values
with probability p1  and so on for n records assuming that all searches are for
records that actually exist the probabilities p0 through pn1 must sum to one
certain probability distributions give easily computed results
example 91 calculate the expected cost to search a list when each record
has equal chance of being accessed the classic sequential search through
an unsorted list setting pi  1n yields
cn 

n
x

in  n  12

i1

this result matches our expectation that half the records will be accessed on
average by normal sequential search if the records truly have equal access
probabilities then ordering records by frequency yields no benefit we saw
in section 91 the more general case where we must consider the probability
labeled pn  that the search key does not match that for any record in the
array in that case in accordance with our general formula we get
1pn 
thus

n1
n  1  npn n  pn  2pn
n  1  p0 n  1
pn n 


2
2
2

n1
2

 c n  n depending on the value of p0 

a geometric probability distribution can yield quite different results


$$@@$$PAGE: 338
319

sec 92 selforganizing lists

example 92 calculate the expected cost for searching a list ordered by
frequency when the probabilities are defined as

12i if 0  i  n  2
pi 
12n if i  n  1
then
cn 

n1
x

n
x

i0

i1

i  12i1 

i2i   2

for this example the expected number of accesses is a constant this is
because the probability for accessing the first record is high one half the
second is much lower one quarter but still much higher than for the third
record and so on this shows that for some probability distributions ordering the list by frequency can yield an efficient search technique
in many search applications real access patterns follow a rule of thumb called
the 8020 rule the 8020 rule says that 80 of the record accesses are to 20
of the records the values of 80 and 20 are only estimates every data access pattern has its own values however behavior of this nature occurs surprisingly often
in practice which explains the success of caching techniques widely used by web
browsers for speeding access to web pages and by disk drive and cpu manufacturers for speeding access to data stored in slower memory see the discussion on
buffer pools in section 83 when the 8020 rule applies we can expect considerable improvements to search performance from a list ordered by frequency of
access over standard sequential search in an unordered list
example 93 the 8020 rule is an example of a zipf distribution naturally occurring distributions often follow a zipf distribution examples
include the observed frequency for the use of words in a natural language
such as english and the size of the population for cities ie view the
relative proportions for the populations as equivalent to the frequency of
use zipf distributions are related to the harmonic series defined in equation 210 define the zipf frequency for item i in the distribution for n
records as 1ihn  see exercise 94 the expected cost for the series
whose members follow this zipf distribution will be
n
x
cn 
iihn  nhn  n loge n
i1

when a frequency distribution follows the 8020 rule the average search
looks at about 1015 of the records in a list ordered by frequency


$$@@$$PAGE: 339
320

chap 9 searching

this is potentially a useful observation that typical reallife distributions of
record accesses if the records were ordered by frequency would require that we
visit on average only 1015 of the list when doing sequential search this means
that if we had an application that used sequential search and we wanted to make it
go a bit faster by a constant amount we could do so without a major rewrite to
the system to implement something like a search tree but that is only true if there
is an easy way to at least approximately order the records by frequency
in most applications we have no means of knowing in advance the frequencies
of access for the data records to complicate matters further certain records might
be accessed frequently for a brief period of time and then rarely thereafter thus
the probability of access for records might change over time in most database
systems this is to be expected selforganizing lists seek to solve both of these
problems
selforganizing lists modify the order of records within the list based on the
actual pattern of record access selforganizing lists use a heuristic for deciding
how to to reorder the list these heuristics are similar to the rules for managing
buffer pools see section 83 in fact a buffer pool is a form of selforganizing
list ordering the buffer pool by expected frequency of access is a good strategy
because typically we must search the contents of the buffers to determine if the
desired information is already in main memory when ordered by frequency of
access the buffer at the end of the list will be the one most appropriate for reuse
when a new page of information must be read below are three traditional heuristics
for managing selforganizing lists
1 the most obvious way to keep a list ordered by frequency would be to store
a count of accesses to each record and always maintain records in this order this method will be referred to as count count is similar to the least
frequently used buffer replacement strategy whenever a record is accessed
it might move toward the front of the list if its number of accesses becomes
greater than a record preceding it thus count will store the records in the
order of frequency that has actually occurred so far besides requiring space
for the access counts count does not react well to changing frequency of
access over time once a record has been accessed a large number of times
under the frequency count system it will remain near the front of the list
regardless of further access history
2 bring a record to the front of the list when it is found pushing all the other
records back one position this is analogous to the least recently used buffer
replacement strategy and is called movetofront this heuristic is easy to
implement if the records are stored using a linked list when records are
stored in an array bringing a record forward from near the end of the array
will result in a large number of records slightly changing position movetofronts cost is bounded in the sense that it requires at most twice the num


$$@@$$PAGE: 340
321

sec 92 selforganizing lists

ber of accesses required by the optimal static ordering for n records when
at least n searches are performed in other words if we had known the series of at least n searches in advance and had stored the records in order of
frequency so as to minimize the total cost for these accesses this cost would
be at least half the cost required by the movetofront heuristic this will
be proved using amortized analysis in section 143 finally movetofront
responds well to local changes in frequency of access in that if a record is
frequently accessed for a brief period of time it will be near the front of the
list during that period of access movetofront does poorly when the records
are processed in sequential order especially if that sequential order is then
repeated multiple times
3 swap any record found with the record immediately preceding it in the list
this heuristic is called transpose transpose is good for list implementations
based on either linked lists or arrays frequently used records will over time
move to the front of the list records that were once frequently accessed but
are no longer used will slowly drift toward the back thus it appears to have
good properties with respect to changing frequency of access unfortunately
there are some pathological sequences of access that can make transpose
perform poorly consider the case where the last record of the list call it x is
accessed this record is then swapped with the nexttolast record call it y
making y the last record if y is now accessed it swaps with x a repeated
series of accesses alternating between x and y will continually search to the
end of the list because neither record will ever make progress toward the
front however such pathological cases are unusual in practice a variation
on transpose would be to move the accessed record forward in the list by
some fixed number of steps
example 94 assume that we have eight records with key values a to h
and that they are initially placed in alphabetical order now consider the
result of applying the following access pattern
f d f g e g f a d f g e
assume that when a records frequency count goes up it moves forward in
the list to become the last record with that value for its frequency count
after the first two accesses f will be the first record and d will be the
second the final list resulting from these accesses will be
f g d e a b c h
and the total cost for the twelve accesses will be 45 comparisons
if the list is organized by the movetofront heuristic then the final list
will be
e g f d a b c h


$$@@$$PAGE: 341
322

chap 9 searching

and the total number of comparisons required is 54
finally if the list is organized by the transpose heuristic then the final
list will be
a b f d g e c h
and the total number of comparisons required is 62
while selforganizing lists do not generally perform as well as search trees or a
sorted list both of which require olog n search time there are many situations in
which selforganizing lists prove a valuable tool obviously they have an advantage
over sorted lists in that they need not be sorted this means that the cost to insert
a new record is low which could more than make up for the higher search cost
when insertions are frequent selforganizing lists are simpler to implement than
search trees and are likely to be more efficient for small lists nor do they require
additional space finally in the case of an application where sequential search is
almost fast enough changing an unsorted list to a selforganizing list might speed
the application enough at a minor cost in additional code
as an example of applying selforganizing lists consider an algorithm for compressing and transmitting messages the list is selforganized by the movetofront
rule transmission is in the form of words and numbers by the following rules
1 if the word has been seen before transmit the current position of the word in
the list move the word to the front of the list
2 if the word is seen for the first time transmit the word place the word at the
front of the list
both the sender and the receiver keep track of the position of words in the list
in the same way using the movetofront rule so they agree on the meaning of
the numbers that encode repeated occurrences of words consider the following
example message to be transmitted for simplicity ignore case in letters
the car on the left hit the car i left
the first three words have not been seen before so they must be sent as full
words the fourth word is the second appearance of the which at this point is
the third word in the list thus we only need to transmit the position value 3
the next two words have not yet been seen so must be sent as full words the
seventh word is the third appearance of the which coincidentally is again in the
third position the eighth word is the second appearance of car which is now in
the fifth position of the list i is a new word and the last word left is now in
the fifth position thus the entire transmission would be
the car on 3 left hit 3 5 i 5


$$@@$$PAGE: 342
323

sec 93 bit vectors for representing sets

0

1

2

3

4

5

6

7

8

9 10 11 12 13 14 15

0

0

1

1

0

1

0

1

0

0

0

1

0

1

0

0

figure 91 the bit array for the set of primes in the range 0 to 15 the bit at
position i is set to 1 if and only if i is prime

this approach to compression is similar in spirit to zivlempel coding which
is a class of coding algorithms commonly used in file compression utilities zivlempel coding replaces repeated occurrences of strings with a pointer to the location in the file of the first occurrence of the string the codes are stored in a
selforganizing list in order to speed up the time required to search for a string that
has previously been seen

93

bit vectors for representing sets

determining whether a value is a member of a particular set is a special case of
searching for keys in a sequence of records thus any of the search methods
discussed in this book can be used to check for set membership however we
can also take advantage of the restricted circumstances imposed by this problem to
develop another representation
in the case where the set values fall within a limited range we can represent the
set using a bit array with a bit position allocated for each potential member those
members actually in the set store a value of 1 in their corresponding bit those
members not in the set store a value of 0 in their corresponding bit for example
consider the set of primes between 0 and 15 figure 91 shows the corresponding
bit array to determine if a particular value is prime we simply check the corresponding bit this representation scheme is called a bit vector or a bitmap the
mark array used in several of the graph algorithms of chapter 11 is an example of
such a set representation
if the set fits within a single computer word then set union intersection and
difference can be performed by logical bitwise operations the union of sets a
and b is the bitwise or function whose symbol is  in c  the intersection
of sets a and b is the bitwise and function whose symbol is  in c  for
example if we would like to compute the set of numbers between 0 and 15 that are
both prime and odd numbers we need only compute the expression
0011010100010100  0101010101010101
the set difference a  b can be implemented in c using the expression ab
 is the symbol for bitwise negation for larger sets that do not fit into a single
computer word the equivalent operations can be performed in turn on the series of
words making up the entire bit vector


$$@@$$PAGE: 343
324

chap 9 searching

this method of computing sets from bit vectors is sometimes applied to document retrieval consider the problem of picking from a collection of documents
those few which contain selected keywords for each keyword the document retrieval system stores a bit vector with one bit for each document if the user wants to
know which documents contain a certain three keywords the corresponding three
bit vectors are anded together those bit positions resulting in a value of 1 correspond to the desired documents alternatively a bit vector can be stored for each
document to indicate those keywords appearing in the document such an organization is called a signature file the signatures can be manipulated to find documents
with desired combinations of keywords

94

hashing

this section presents a completely different approach to searching arrays by direct
access based on key value the process of finding a record using some computation to map its key value to a position in the array is called hashing most hashing schemes place records in the array in whatever order satisfies the needs of the
address calculation thus the records are not ordered by value or frequency the
function that maps key values to positions is called a hash function and will be
denoted by h the array that holds the records is called the hash table and will be
denoted by ht a position in the hash table is also known as a slot the number
of slots in hash table ht will be denoted by the variable m  with slots numbered
from 0 to m  1 the goal for a hashing system is to arrange things such that for
any key value k and some hash function h i  hk is a slot in the table such
that 0  hk  m  and we have the key of the record stored at hti equal to
k
hashing is not good for applications where multiple records with the same key
value are permitted hashing is not a good method for answering range searches in
other words we cannot easily find all records if any whose key values fall within
a certain range nor can we easily find the record with the minimum or maximum
key value or visit the records in key order hashing is most appropriate for answering the question what record if any has key value k for applications where
access involves only exactmatch queries hashing is usually the search method of
choice because it is extremely efficient when implemented correctly as you will
see in this section however there are many approaches to hashing and it is easy
to devise an inefficient implementation hashing is suitable for both inmemory
and diskbased searching and is one of the two most widely used methods for organizing large databases stored on disk the other is the btree which is covered in
chapter 10
as a simple though unrealistic example of hashing consider storing n records
each with a unique key value in the range 0 to n  1 in this simple case a record


$$@@$$PAGE: 344
sec 94 hashing

325

with key k can be stored in htk and the hash function is simply hk  k to
find the record with key value k simply look in htk
typically there are many more values in the key range than there are slots in
the hash table for a more realistic example suppose that the key can take any
value in the range 0 to 65535 ie the key is a twobyte unsigned integer and that
we expect to store approximately 1000 records at any given time it is impractical
in this situation to use a hash table with 65536 slots because most of the slots will
be left empty instead we must devise a hash function that allows us to store the
records in a much smaller table because the possible key range is larger than the
size of the table at least some of the slots must be mapped to from multiple key
values given a hash function h and two keys k1 and k2  if hk1     hk2 
where  is a slot in the table then we say that k1 and k2 have a collision at slot 
under hash function h
finding a record with key value k in a database organized by hashing follows
a twostep procedure
1 compute the table location hk
2 starting with slot hk locate the record containing key k using if necessary a collision resolution policy
941

hash functions

hashing generally takes records whose key values come from a large range and
stores those records in a table with a relatively small number of slots collisions
occur when two records hash to the same slot in the table if we are carefulor
luckywhen selecting a hash function then the actual number of collisions will
be few unfortunately even under the best of circumstances collisions are nearly
unavoidable1 for example consider a classroom full of students what is the
probability that some pair of students shares the same birthday ie the same day
of the year not necessarily the same year if there are 23 students then the odds
are about even that two will share a birthday this is despite the fact that there are
365 days in which students can have birthdays ignoring leap years on most of
which no student in the class has a birthday with more students the probability
of a shared birthday increases the mapping of students to days based on their
1

the exception to this is perfect hashing perfect hashing is a system in which records are
hashed such that there are no collisions a hash function is selected for the specific set of records
being hashed which requires that the entire collection of records be available before selecting the
hash function perfect hashing is efficient because it always finds the record that we are looking
for exactly where the hash function computes it to be so only one access is required selecting a
perfect hash function can be expensive but might be worthwhile when extremely efficient search
performance is required an example is searching for data on a readonly cd here the database will
never change the time for each access is expensive and the database designer can build the hash
table before issuing the cd


$$@@$$PAGE: 345
326

chap 9 searching

birthday is similar to assigning records to slots in a table of size 365 using the
birthday as a hash function note that this observation tells us nothing about which
students share a birthday or on which days of the year shared birthdays fall
to be practical a database organized by hashing must store records in a hash
table that is not so large that it wastes space typically this means that the hash
table will be around half full because collisions are extremely likely to occur
under these conditions by chance any record inserted into a table that is half full
will have a collision half of the time does this mean that we need not worry about
the ability of a hash function to avoid collisions absolutely not the difference
between a good hash function and a bad hash function makes a big difference in
practice technically any function that maps all possible key values to a slot in
the hash table is a hash function in the extreme case even a function that maps
all records to the same slot is a hash function but it does nothing to help us find
records during a search operation
we would like to pick a hash function that stores the actual records in the collection such that each slot in the hash table has equal probability of being filled unfortunately we normally have no control over the key values of the actual records
so how well any particular hash function does this depends on the distribution of
the keys within the allowable key range in some cases incoming data are well
distributed across their key range for example if the input is a set of random
numbers selected uniformly from the key range any hash function that assigns the
key range so that each slot in the hash table receives an equal share of the range
will likely also distribute the input records uniformly within the table however
in many applications the incoming records are highly clustered or otherwise poorly
distributed when input records are not well distributed throughout the key range
it can be difficult to devise a hash function that does a good job of distributing the
records throughout the table especially if the input distribution is not known in
advance
there are many reasons why data values might be poorly distributed
1 natural frequency distributions tend to follow a common pattern where a few
of the entities occur frequently while most entities occur relatively rarely
for example consider the populations of the 100 largest cities in the united
states if you plot these populations on a number line most of them will be
clustered toward the low side with a few outliers on the high side this is an
example of a zipf distribution see section 92 viewed the other way the
home town for a given person is far more likely to be a particular large city
than a particular small town
2 collected data are likely to be skewed in some way field samples might be
rounded to say the nearest 5 ie all numbers end in 5 or 0
3 if the input is a collection of common english words the beginning letter
will be poorly distributed


$$@@$$PAGE: 346
sec 94 hashing

327

note that in examples 2 and 3 either high or loworder bits of the key are poorly
distributed
when designing hash functions we are generally faced with one of two situations
1 we know nothing about the distribution of the incoming keys in this case
we wish to select a hash function that evenly distributes the key range across
the hash table while avoiding obvious opportunities for clustering such as
hash functions that are sensitive to the high or loworder bits of the key
value
2 we know something about the distribution of the incoming keys in this case
we should use a distributiondependent hash function that avoids assigning
clusters of related key values to the same hash table slot for example if
hashing english words we should not hash on the value of the first character
because this is likely to be unevenly distributed
below are several examples of hash functions that illustrate these points
example 95 consider the following hash function used to hash integers
to a table of sixteen slots
int hint x 
return x  16


the value returned by this hash function depends solely on the least
significant four bits of the key because these bits are likely to be poorly
distributed as an example a high percentage of the keys might be even
numbers which means that the low order bit is zero the result will also
be poorly distributed this example shows that the size of the table m can
have a big effect on the performance of a hash system because this value is
typically used as the modulus to ensure that the hash function produces a
number in the range 0 to m  1

example 96 a good hash function for numerical values comes from the
midsquare method the midsquare method squares the key value and
then takes the middle r bits of the result giving a value in the range 0 to
2r  1 this works well because most or all bits of the key value contribute
to the result for example consider records whose keys are 4digit numbers
in base 10 the goal is to hash these key values to a table of size 100
ie a range of 0 to 99 this range is equivalent to two digits in base 10
that is r  2 if the input is the number 4567 squaring yields an 8digit
number 20857489 the middle two digits of this result are 57 all digits


$$@@$$PAGE: 347
328

chap 9 searching

4567
4567
31969
27402
22835
18268
20857489
4567
figure 92 an illustration of the midsquare method showing the details of
long multiplication in the process of squaring the value 4567 the bottom of the
figure indicates which digits of the answer are most influenced by each digit of
the operands

equivalently all bits when the number is viewed in binary contribute to the
middle two digits of the squared value figure 92 illustrates the concept
thus the result is not dominated by the distribution of the bottom digit or
the top digit of the original key value

example 97 here is a hash function for strings of characters
int hchar x 
int i sum
for sum0 i0 xi  0 i
sum  int xi
return sum  m


this function sums the ascii values of the letters in a string if the hash
table size m is small this hash function should do a good job of distributing
strings evenly among the hash table slots because it gives equal weight to
all characters this is an example of the folding approach to designing a
hash function note that the order of the characters in the string has no
effect on the result a similar method for integers would add the digits of
the key value assuming that there are enough digits to 1 keep any one
or two digits with bad distribution from skewing the results of the process
and 2 generate a sum much larger than m  as with many other hash
functions the final step is to apply the modulus operator to the result using
table size m to generate a value within the table range if the sum is not
sufficiently large then the modulus operator will yield a poor distribution
for example because the ascii value for a is 65 and z is 90 sum will
always be in the range 650 to 900 for a string of ten upper case letters for
a hash table of size 100 or less a reasonable distribution results for a hash
table of size 1000 the distribution is terrible because only slots 650 to 900


$$@@$$PAGE: 348
sec 94 hashing

329

can possibly be the home slot for some key value and the values are not
evenly distributed even within those slots

example 98 here is a much better hash function for strings
 use folding on a string summed 4 bytes at a time
int sfoldchar key 
unsigned int lkey  unsigned int key
int intlength  strlenkey4
unsigned int sum  0
forint i0 iintlength i
sum  lkeyi
 now deal with the extra chars at the end
int extra  strlenkey  intlength4
char temp4
lkey  unsigned int temp
lkey0  0
forint i0 iextra i
tempi  keyintlength4i
sum  lkey0
return sum  m


this function takes a string as input it processes the string four bytes
at a time and interprets each of the fourbyte chunks as a single unsigned
long integer value the integer values for the fourbyte chunks are added
together in the end the resulting sum is converted to the range 0 to m  1
using the modulus operator2
for example if the string aaaabbbb is passed to sfold then the first
four bytes aaaa will be interpreted as the integer value 1633771873
and the next four bytes bbbb will be interpreted as the integer value
1650614882 their sum is 3284386755 when viewed as an unsigned
integer if the table size is 101 then the modulus function will cause this
key to hash to slot 75 in the table note that for any sufficiently long string
the sum for the integer quantities will typically cause a 32bit integer to
overflow thus losing some of the highorder bits because the resulting
values are so large but this causes no problems when the goal is to compute
a hash function
2

recall from section 22 that the implementation for n mod m on many c and java compilers
will yield a negative number if n is negative implementors for hash functions need to be careful that
their hash function does not generate a negative number this can be avoided either by insuring
that n is positive when computing n mod m or adding m to the result if n mod m is negative
all computation in sfold is done using unsigned long values in part to protect against taking the
modulus of an negative number


$$@@$$PAGE: 349
330

chap 9 searching

0 1000

9530

1
2
3 3013
4
5
6
7 9877

2007

1057

8
9 9879
figure 93 an illustration of open hashing for seven numbers stored in a tenslot
hash table using the hash function hk  k mod 10 the numbers are inserted
in the order 9877 2007 1000 9530 3013 9879 and 1057 two of the values
hash to slot 0 one value hashes to slot 2 three of the values hash to slot 7 and
one value hashes to slot 9

942

open hashing

while the goal of a hash function is to minimize collisions some collisions are
unavoidable in practice thus hashing implementations must include some form of
collision resolution policy collision resolution techniques can be broken into two
classes open hashing also called separate chaining and closed hashing also
called open addressing3 the difference between the two has to do with whether
collisions are stored outside the table open hashing or whether collisions result
in storing one of the records at another slot in the table closed hashing open
hashing is treated in this section and closed hashing in section 943
the simplest form of open hashing defines each slot in the hash table to be
the head of a linked list all records that hash to a particular slot are placed on
that slots linked list figure 93 illustrates a hash table where each slot stores one
record and a link pointer to the rest of the list
records within a slots list can be ordered in several ways by insertion order
by key value order or by frequencyofaccess order ordering the list by key value
provides an advantage in the case of an unsuccessful search because we know to
stop searching the list once we encounter a key that is greater than the one being
3

yes it is confusing when open hashing means the opposite of open addressing but unfortunately that is the way it is


$$@@$$PAGE: 350
sec 94 hashing

331

searched for if records on the list are unordered or ordered by frequency then an
unsuccessful search will need to visit every record on the list
given a table of size m storing n records the hash function will ideally
spread the records evenly among the m positions in the table yielding on average
nm records for each list assuming that the table has more slots than there are
records to be stored we can hope that few slots will contain more than one record
in the case where a list is empty or has only one record a search requires only one
access to the list thus the average cost for hashing should be 1 however if
clustering causes many records to hash to only a few of the slots then the cost to
access a record will be much higher because many elements on the linked list must
be searched
open hashing is most appropriate when the hash table is kept in main memory
with the lists implemented by a standard inmemory linked list storing an open
hash table on disk in an efficient way is difficult because members of a given
linked list might be stored on different disk blocks this would result in multiple
disk accesses when searching for a particular key value which defeats the purpose
of using hashing
there are similarities between open hashing and binsort one way to view
open hashing is that each record is simply placed in a bin while multiple records
may hash to the same bin this initial binning should still greatly reduce the number
of records accessed by a search operation in a similar fashion a simple binsort
reduces the number of records in each bin to a small number that can be sorted in
some other way
943

closed hashing

closed hashing stores all records directly in the hash table each record r with key
value kr has a home position that is hkr  the slot computed by the hash function
if r is to be inserted and another record already occupies rs home position then
r will be stored at some other slot in the table it is the business of the collision
resolution policy to determine which slot that will be naturally the same policy
must be followed during search as during insertion so that any record not found in
its home position can be recovered by repeating the collision resolution process
bucket hashing
one implementation for closed hashing groups hash table slots into buckets the
m slots of the hash table are divided into b buckets with each bucket consisting
of mb slots the hash function assigns each record to the first slot within one
of the buckets if this slot is already occupied then the bucket slots are searched
sequentially until an open slot is found if a bucket is entirely full then the record
is stored in an overflow bucket of infinite capacity at the end of the table all


$$@@$$PAGE: 351
332

chap 9 searching

hash
table
0 1000

overflow
1057

9530
1

2 9877
2007
3 3013

4 9879

figure 94 an illustration of bucket hashing for seven numbers stored in a fivebucket hash table using the hash function hk  k mod 5 each bucket contains two slots the numbers are inserted in the order 9877 2007 1000 9530
3013 9879 and 1057 two of the values hash to bucket 0 three values hash to
bucket 2 one value hashes to bucket 3 and one value hashes to bucket 4 because
bucket 2 cannot hold three values the third one ends up in the overflow bucket

buckets share the same overflow bucket a good implementation will use a hash
function that distributes the records evenly among the buckets so that as few records
as possible go into the overflow bucket figure 94 illustrates bucket hashing
when searching for a record the first step is to hash the key to determine which
bucket should contain the record the records in this bucket are then searched if
the desired key value is not found and the bucket still has free slots then the search
is complete if the bucket is full then it is possible that the desired record is stored
in the overflow bucket in this case the overflow bucket must be searched until the
record is found or all records in the overflow bucket have been checked if many
records are in the overflow bucket this will be an expensive process
a simple variation on bucket hashing is to hash a key value to some slot in
the hash table as though bucketing were not being used if the home position is
full then the collision resolution process is to move down through the table toward
the end of the bucket while searching for a free slot in which to store the record
if the bottom of the bucket is reached then the collision resolution routine wraps
around to the top of the bucket to continue the search for an open slot for example
assume that buckets contain eight records with the first bucket consisting of slots 0
through 7 if a record is hashed to slot 5 the collision resolution process will
attempt to insert the record into the table in the order 5 6 7 0 1 2 3 and finally 4
if all slots in this bucket are full then the record is assigned to the overflow bucket


$$@@$$PAGE: 352
333

sec 94 hashing

hash
table
0 1000

overflow
1057

1 9530
2
3 3013
4
5
6 2007
7 9877
8
9 9879
figure 95 an variant of bucket hashing for seven numbers stored in a 10slot
hash table using the hash function hk  k mod 10 each bucket contains two
slots the numbers are inserted in the order 9877 2007 1000 9530 3013 9879
and 1057 value 9877 first hashes to slot 7 so when value 2007 attempts to do
likewise it is placed in the other slot associated with that bucket which is slot 6
when value 1057 is inserted there is no longer room in the bucket and it is placed
into overflow the other collision occurs after value 1000 is inserted to slot 0
causing 9530 to be moved to slot 1

the advantage of this approach is that initial collisions are reduced because any
slot can be a home position rather than just the first slot in the bucket figure 95
shows another example for this form of bucket hashing
bucket methods are good for implementing hash tables stored on disk because
the bucket size can be set to the size of a disk block whenever search or insertion
occurs the entire bucket is read into memory because the entire bucket is then
in memory processing an insert or search operation requires only one disk access
unless the bucket is full if the bucket is full then the overflow bucket must be
retrieved from disk as well naturally overflow should be kept small to minimize
unnecessary disk accesses
linear probing
we now turn to the most commonly used form of hashing closed hashing with no
bucketing and a collision resolution policy that can potentially use any slot in the
hash table
during insertion the goal of collision resolution is to find a free slot in the hash
table when the home position for the record is already occupied we can view any
collision resolution method as generating a sequence of hash table slots that can


$$@@$$PAGE: 353
334

chap 9 searching

 insert e into hash table ht
template typename key typename e
void hashdictkey e
hashinsertconst key k const e e 
int home
 home position for e
int pos  home  hk
 init probe sequence
for int i1 emptykey  htposkey i 
pos  home  pk i  m  probe
assertk  htposkey duplicates not allowed

kvpairkeye tempk e
htpos  temp

figure 96 insertion method for a dictionary implemented by a hash table

potentially hold the record the first slot in the sequence will be the home position
for the key if the home position is occupied then the collision resolution policy
goes to the next slot in the sequence if this is occupied as well then another slot
must be found and so on this sequence of slots is known as the probe sequence
and it is generated by some probe function that we will call p the insert function
is shown in figure 96
method hashinsert first checks to see if the home slot for the key is empty
if the home slot is occupied then we use the probe function pk i to locate a free
slot in the table function p has two parameters the key k and a count i for where
in the probe sequence we wish to be that is to get the first position in the probe
sequence after the home slot for key k we call pk 1 for the next slot in the
probe sequence call pk 2 note that the probe function returns an offset from
the original home position rather than a slot in the hash table thus the for loop
in hashinsert is computing positions in the table at each iteration by adding
the value returned from the probe function to the home position the ith call to p
returns the ith offset to be used
searching in a hash table follows the same probe sequence that was followed
when inserting records in this way a record not in its home position can be recovered a c implementation for the search procedure is shown in figure 97
the insert and search routines assume that at least one slot on the probe sequence of every key will be empty otherwise they will continue in an infinite
loop on unsuccessful searches thus the dictionary should keep a count of the
number of records stored and refuse to insert into a table that has only one free
slot
the discussion on bucket hashing presented a simple method of collision resolution if the home position for the record is occupied then move down the bucket
until a free slot is found this is an example of a technique for collision resolution
known as linear probing the probe function for simple linear probing is
pk i  i


$$@@$$PAGE: 354
sec 94 hashing

335

 search for the record with key k
template typename key typename e
e hashdictkey e
hashsearchconst key k const 
int home
 home position for k
int pos  home  hk  initial position is home slot
for int i  1 k  htposkey 
emptykey  htposkey i
pos  home  pk i  m  next on probe sequence
if k  htposkey
 found it
return htposvalue
else return null
 k not in hash table

figure 97 search method for a dictionary implemented by a hash table

that is the ith offset on the probe sequence is just i meaning that the ith step is
simply to move down i slots in the table
once the bottom of the table is reached the probe sequence wraps around to
the beginning of the table linear probing has the virtue that all slots in the table
will be candidates for inserting a new record before the probe sequence returns to
the home position
while linear probing is probably the first idea that comes to mind when considering collision resolution policies it is not the only one possible probe function p
allows us many options for how to do collision resolution in fact linear probing is
one of the worst collision resolution methods the main problem is illustrated by
figure 98 here we see a hash table of ten slots used to store fourdigit numbers
with hash function hk  k mod 10 in figure 98a five numbers have been
placed in the table leaving five slots remaining
the ideal behavior for a collision resolution mechanism is that each empty slot
in the table will have equal probability of receiving the next record inserted assuming that every slot in the table has equal probability of being hashed to initially in
this example assume that the hash function gives each slot roughly equal probability of being the home position for the next key however consider what happens
to the next record if its key has its home position at slot 0 linear probing will
send the record to slot 2 the same will happen to records whose home position
is at slot 1 a record with home position at slot 2 will remain in slot 2 thus the
probability is 310 that the next record inserted will end up in slot 2 in a similar
manner records hashing to slots 7 or 8 will end up in slot 9 however only records
hashing to slot 3 will be stored in slot 3 yielding one chance in ten of this happening likewise there is only one chance in ten that the next record will be stored
in slot 4 one chance in ten for slot 5 and one chance in ten for slot 6 thus the
resulting probabilities are not equal
to make matters worse if the next record ends up in slot 9 which already has
a higher than normal chance of happening then the following record will end up


$$@@$$PAGE: 355
336

chap 9 searching

0 9050

0 9050

1 1001

1 1001

2

2

3

3

4

4

5

5

6

6

7 9877

7 9877

8 2037

8 2037

9

9 1059
a

b

figure 98 example of problems with linear probing a four values are inserted
in the order 1001 9050 9877 and 2037 using hash function hk  k mod 10
b the value 1059 is added to the hash table

in slot 2 with probability 610 this is illustrated by figure 98b this tendency
of linear probing to cluster items together is known as primary clustering small
clusters tend to merge into big clusters making the problem worse the objection
to primary clustering is that it leads to long probe sequences
improved collision resolution methods
how can we avoid primary clustering one possible improvement might be to use
linear probing but to skip slots by a constant c other than 1 this would make the
probe function
pk i  ci
and so the ith slot in the probe sequence will be hk  ic mod m  in this way
records with adjacent home positions will not follow the same probe sequence for
example if we were to skip by twos then our offsets from the home slot would
be 2 then 4 then 6 and so on
one quality of a good probe sequence is that it will cycle through all slots in
the hash table before returning to the home position clearly linear probing which
skips slots by one each time does this unfortunately not all values for c will
make this happen for example if c  2 and the table contains an even number
of slots then any key whose home position is in an even slot will have a probe
sequence that cycles through only the even slots likewise the probe sequence
for a key whose home position is in an odd slot will cycle through the odd slots
thus this combination of table size and linear probing constant effectively divides


$$@@$$PAGE: 356
337

sec 94 hashing

the records into two sets stored in two disjoint sections of the hash table so long
as both sections of the table contain the same number of records this is not really
important however just from chance it is likely that one section will become fuller
than the other leading to more collisions and poorer performance for those records
the other section would have fewer records and thus better performance but the
overall system performance will be degraded as the additional cost to the side that
is more full outweighs the improved performance of the lessfull side
constant c must be relatively prime to m to generate a linear probing sequence
that visits all slots in the table that is c and m must share no factors for a hash
table of size m  10 if c is any one of 1 3 7 or 9 then the probe sequence
will visit all slots for any key when m  11 any value for c between 1 and 10
generates a probe sequence that visits all slots for every key
consider the situation where c  2 and we wish to insert a record with key k1
such that hk1   3 the probe sequence for k1 is 3 5 7 9 and so on if another
key k2 has home position at slot 5 then its probe sequence will be 5 7 9 and so on
the probe sequences of k1 and k2 are linked together in a manner that contributes
to clustering in other words linear probing with a value of c  1 does not solve
the problem of primary clustering we would like to find a probe function that does
not link keys together in this way we would prefer that the probe sequence for k1
after the first step on the sequence should not be identical to the probe sequence of
k2  instead their probe sequences should diverge
the ideal probe function would select the next position on the probe sequence
at random from among the unvisited slots that is the probe sequence should be a
random permutation of the hash table positions unfortunately we cannot actually
select the next position in the probe sequence at random because then we would not
be able to duplicate this same probe sequence when searching for the key however
we can do something similar called pseudorandom probing in pseudorandom
probing the ith slot in the probe sequence is hk  ri  mod m where ri is the
ith value in a random permutation of the numbers from 1 to m  1 all insertion
and search operations use the same random permutation the probe function is
pk i  permi  1
where perm is an array of length m  1 containing a random permutation of the
values from 1 to m  1
example 99 consider a table of size m  101 with perm1  5
perm2  2 and perm3  32 assume that we have two keys k1 and
k2 where hk1   30 and hk2   35 the probe sequence for k1 is 30
then 35 then 32 then 62 the probe sequence for k2 is 35 then 40 then
37 then 67 thus while k2 will probe to k1 s home position as its second
choice the two keys probe sequences diverge immediately thereafter


$$@@$$PAGE: 357
338

chap 9 searching

another probe function that eliminates primary clustering is called quadratic
probing here the probe function is some quadratic function
pk i  c1 i2  c2 i  c3
for some choice of constants c1  c2  and c3  the simplest variation is pk i  i2
ie c1  1 c2  0 and c3  0 then the ith value in the probe sequence would
be hk  i2  mod m  under quadratic probing two keys with different home
positions will have diverging probe sequences
example 910 given a hash table of size m  101 assume for keys k1
and k2 that hk1   30 and hk2   29 the probe sequence for k1 is 30
then 31 then 34 then 39 the probe sequence for k2 is 29 then 30 then
33 then 38 thus while k2 will probe to k1 s home position as its second
choice the two keys probe sequences diverge immediately thereafter
unfortunately quadratic probing has the disadvantage that typically not all hash
table slots will be on the probe sequence using pk i  i2 gives particularly inconsistent results for many hash table sizes this probe function will cycle through
a relatively small number of slots if all slots on that cycle happen to be full then
the record cannot be inserted at all for example if our hash table has three slots
then records that hash to slot 0 can probe only to slots 0 and 1 that is the probe
sequence will never visit slot 2 in the table thus if slots 0 and 1 are full then
the record cannot be inserted even though the table is not full a more realistic
example is a table with 105 slots the probe sequence starting from any given slot
will only visit 23 other slots in the table if all 24 of these slots should happen to
be full even if other slots in the table are empty then the record cannot be inserted
because the probe sequence will continually hit only those same 24 slots
fortunately it is possible to get good results from quadratic probing at low
cost the right combination of probe function and table size will visit many slots
in the table in particular if the hash table size is a prime number and the probe
function is pk i  i2  then at least half the slots in the table will be visited
thus if the table is less than half full we can be certain that a free slot will be
found alternatively if the hash table size is a power of two and the probe function
is pk i  i2  i2 then every slot in the table will be visited by the probe
function
both pseudorandom probing and quadratic probing eliminate primary clustering which is the problem of keys sharing substantial segments of a probe sequence
if two keys hash to the same home position however then they will always follow
the same probe sequence for every collision resolution method that we have seen so
far the probe sequences generated by pseudorandom and quadratic probing for
example are entirely a function of the home position not the original key value


$$@@$$PAGE: 358
sec 94 hashing

339

this is because function p ignores its input parameter k for these collision resolution methods if the hash function generates a cluster at a particular home position
then the cluster remains under pseudorandom and quadratic probing this problem
is called secondary clustering
to avoid secondary clustering we need to have the probe sequence make use of
the original key value in its decisionmaking process a simple technique for doing
this is to return to linear probing by a constant step size for the probe function but
to have that constant be determined by a second hash function h2  thus the probe
sequence would be of the form pk i  i  h2 k this method is called double
hashing
example 911 assume a hash table has size m  101 and that there
are three keys k1  k2  and k3 with hk1   30 hk2   28 hk3   30
h2 k1   2 h2 k2   5 and h2 k3   5 then the probe sequence
for k1 will be 30 32 34 36 and so on the probe sequence for k2 will
be 28 33 38 43 and so on the probe sequence for k3 will be 30 35
40 45 and so on thus none of the keys share substantial portions of the
same probe sequence of course if a fourth key k4 has hk4   28 and
h2 k4   2 then it will follow the same probe sequence as k1  pseudorandom or quadratic probing can be combined with double hashing to solve
this problem
a good implementation of double hashing should ensure that all of the probe
sequence constants are relatively prime to the table size m  this can be achieved
easily one way is to select m to be a prime number and have h2 return a value in
the range 1  h2 k  m  1 another way is to set m  2m for some value m
and have h2 return an odd value between 1 and 2m 
figure 99 shows an implementation of the dictionary adt by means of a hash
table the simplest hash function is used with collision resolution by linear probing as the basis for the structure of a hash table implementation a suggested
project at the end of this chapter asks you to improve the implementation with
other hash functions and collision resolution policies
944

analysis of closed hashing

how efficient is hashing we can measure hashing performance in terms of the
number of record accesses required when performing an operation the primary
operations of concern are insertion deletion and search it is useful to distinguish
between successful and unsuccessful searches before a record can be deleted it
must be found thus the number of accesses required to delete a record is equivalent to the number required to successfully search for it to insert a record an
empty slot along the records probe sequence must be found this is equivalent to


$$@@$$PAGE: 359
340

chap 9 searching

 dictionary implemented with a hash table
template typename key typename e
class hashdict  public dictionarykeye 
private
kvpairkeye ht
 the hash table
int m
 size of ht
int currcnt  the current number of elements in ht
key emptykey  usersupplied key value for an empty slot
int pkey k int i const  probe using linear probing
 return i 
int hint x const  return x  m   poor hash function
int hchar x const   hash function for character keys
int i sum
for sum0 i0 xi  0 i sum  int xi
return sum  m

void hashinsertconst key const e
e hashsearchconst key const
public
hashdictint sz key k  k defines an empty slot
m  sz
emptykey  k
currcnt  0
ht  new kvpairkeyesz  make ht of size sz
for int i0 im i
htisetkeyemptykey  initialize ht

hashdict  delete ht 
 find some record with key value k
e findconst key k const
 return hashsearchk 
int size  return currcnt   number stored in table
 insert element it with key k into the dictionary
void insertconst key k const e it 
assertcurrcnt  m hash table is full
hashinsertk it
currcnt

figure 99 a partial implementation for the dictionary adt using a hash table this uses a poor hash function and a poor collision resolution policy linear
probing which can easily be replaced member functions hashinsert and
hashsearch appear in figures 96 and 97 respectively


$$@@$$PAGE: 360
341

sec 94 hashing

an unsuccessful search for the record recall that a successful search for the record
during insertion should generate an error because two records with the same key
are not allowed to be stored in the table
when the hash table is empty the first record inserted will always find its home
position free thus it will require only one record access to find a free slot if all
records are stored in their home positions then successful searches will also require
only one record access as the table begins to fill up the probability that a record
can be inserted into its home position decreases if a record hashes to an occupied
slot then the collision resolution policy must locate another slot in which to store
it finding records not stored in their home position also requires additional record
accesses as the record is searched for along its probe sequence as the table fills
up more and more records are likely to be located ever further from their home
positions
from this discussion we see that the expected cost of hashing is a function of
how full the table is define the load factor for the table as   nm  where n
is the number of records currently in the table
an estimate of the expected cost for an insertion or an unsuccessful search
can be derived analytically as a function of  in the case where we assume that
the probe sequence follows a random permutation of the slots in the hash table
assuming that every slot in the table has equal probability of being the home slot
for the next record the probability of finding the home position occupied is  the
probability of finding both the home position occupied and the next slot on the
n n 1
probe sequence occupied is m
m 1  the probability of i collisions is
n n  1    n  i  1

m m  1    m  i  1
if n and m are large then this is approximately nm i  the expected number
of probes is one plus the sum over i  1 of the probability of i collisions which is
approximately
1


x
nm i  11  
i1

the cost for a successful search or a deletion has the same cost as originally
inserting that record however the expected value for the insertion cost depends
on the value of  not at the time of deletion but rather at the time of the original
insertion we can derive an estimate of this cost essentially an average over all the
insertion costs by integrating from 0 to the current value of  yielding a result of
z
1
1
1  1
dx  loge

 0 1x

1


$$@@$$PAGE: 361
342

chap 9 searching

5

insert

4

delete

3

2

1
0

2

4

6

8

10

figure 910 growth of expected record accesses with  the horizontal axis is
the value for  the vertical axis is the expected number of accesses to the hash
table solid lines show the cost for random probing a theoretical lower bound
on the cost while dashed lines show the cost for linear probing a relatively poor
collision resolution strategy the two leftmost lines show the cost for insertion
equivalently unsuccessful search the two rightmost lines show the cost for deletion equivalently successful search

it is important to realize that these equations represent the expected cost for
operations using the unrealistic assumption that the probe sequence is based on a
random permutation of the slots in the hash table thus avoiding all expense resulting from clustering thus these costs are lowerbound estimates in the average
case the true average cost under linear probing is 12 1  11  2  for insertions
or unsuccessful searches and 21 1  11   for deletions or successful searches
proofs for these results can be found in the references cited in section 95
figure 910 shows the graphs of these four equations to help you visualize the
expected performance of hashing based on the load factor the two solid lines show
the costs in the case of a random probe sequence for 1 insertion or unsuccessful
search and 2 deletion or successful search as expected the cost for insertion or
unsuccessful search grows faster because these operations typically search further
down the probe sequence the two dashed lines show equivalent costs for linear
probing as expected the cost of linear probing grows faster than the cost for
random probing
from figure 910 we see that the cost for hashing when the table is not too full
is typically close to one record access this is extraordinarily efficient much better
than binary search which requires log n record accesses as  increases so does


$$@@$$PAGE: 362
sec 94 hashing

343

the expected cost for small values of  the expected cost is low it remains below
two until the hash table is about half full when the table is nearly empty adding
a new record to the table does not increase the cost of future search operations
by much however the additional search cost caused by each additional insertion
increases rapidly once the table becomes half full based on this analysis the rule
of thumb is to design a hashing system so that the hash table never gets above half
full beyond that point performance will degrade rapidly this requires that the
implementor have some idea of how many records are likely to be in the table at
maximum loading and select the table size accordingly
you might notice that a recommendation to never let a hash table become more
than half full contradicts the diskbased spacetime tradeoff principle which strives
to minimize disk space to increase information density hashing represents an unusual situation in that there is no benefit to be expected from locality of reference
in a sense the hashing system implementor does everything possible to eliminate
the effects of locality of reference given the disk block containing the last record
accessed the chance of the next record access coming to the same disk block is
no better than random chance in a welldesigned hash system this is because a
good hashing implementation breaks up relationships between search keys instead
of improving performance by taking advantage of locality of reference hashing
trades increased hash table space for an improved chance that the record will be
in its home position thus the more space available for the hash table the more
efficient hashing should be
depending on the pattern of record accesses it might be possible to reduce the
expected cost of access even in the face of collisions recall the 8020 rule 80
of the accesses will come to 20 of the data in other words some records are
accessed more frequently if two records hash to the same home position which
would be better placed in the home position and which in a slot further down the
probe sequence the answer is that the record with higher frequency of access
should be placed in the home position because this will reduce the total number of
record accesses ideally records along a probe sequence will be ordered by their
frequency of access
one approach to approximating this goal is to modify the order of records along
the probe sequence whenever a record is accessed if a search is made to a record
that is not in its home position a selforganizing list heuristic can be used for
example if the linear probing collision resolution policy is used then whenever a
record is located that is not in its home position it can be swapped with the record
preceding it in the probe sequence that other record will now be further from
its home position but hopefully it will be accessed less frequently note that this
approach will not work for the other collision resolution policies presented in this
section because swapping a pair of records to improve access to one might remove
the other from its probe sequence


$$@@$$PAGE: 363
344

chap 9 searching

another approach is to keep access counts for records and periodically rehash
the entire table the records should be inserted into the hash table in frequency
order ensuring that records that were frequently accessed during the last series of
requests have the best chance of being near their home positions
945

deletion

when deleting records from a hash table there are two important considerations
1 deleting a record must not hinder later searches in other words the search
process must still pass through the newly emptied slot to reach records whose
probe sequence passed through this slot thus the delete process cannot
simply mark the slot as empty because this will isolate records further down
the probe sequence for example in figure 98a keys 9877 and 2037 both
hash to slot 7 key 2037 is placed in slot 8 by the collision resolution policy
if 9877 is deleted from the table a search for 2037 must still pass through
slot 7 as it probes to slot 8
2 we do not want to make positions in the hash table unusable because of
deletion the freed slot should be available to a future insertion
both of these problems can be resolved by placing a special mark in place
of the deleted record called a tombstone the tombstone indicates that a record
once occupied the slot but does so no longer if a tombstone is encountered when
searching along a probe sequence the search procedure continues with the search
when a tombstone is encountered during insertion that slot can be used to store the
new record however to avoid inserting duplicate keys it will still be necessary for
the search procedure to follow the probe sequence until a truly empty position has
been found simply to verify that a duplicate is not in the table however the new
record would actually be inserted into the slot of the first tombstone encountered
the use of tombstones allows searches to work correctly and allows reuse of
deleted slots however after a series of intermixed insertion and deletion operations some slots will contain tombstones this will tend to lengthen the average
distance from a records home position to the record itself beyond where it could
be if the tombstones did not exist a typical database application will first load a
collection of records into the hash table and then progress to a phase of intermixed
insertions and deletions after the table is loaded with the initial collection of
records the first few deletions will lengthen the average probe sequence distance
for records it will add tombstones over time the average distance will reach
an equilibrium point because insertions will tend to decrease the average distance
by filling in tombstone slots for example after initially loading records into the
database the average path distance might be 12 ie an average of 02 accesses
per search beyond the home position will be required after a series of insertions
and deletions this average distance might increase to 16 due to tombstones this


$$@@$$PAGE: 364
sec 95 further reading

345

seems like a small increase but it is three times longer on average beyond the home
position than before deletions
two possible solutions to this problem are
1 do a local reorganization upon deletion to try to shorten the average path
length for example after deleting a key continue to follow the probe sequence of that key and swap records further down the probe sequence into
the slot of the recently deleted record being careful not to remove any key
from its probe sequence this will not work for all collision resolution policies
2 periodically rehash the table by reinserting all records into a new hash table
not only will this remove the tombstones but it also provides an opportunity
to place the most frequently accessed records into their home positions

95

further reading

for a comparison of the efficiencies for various selforganizing techniques see
bentley and mcgeoch amortized analysis of selforganizing sequential search
heuristics bm85 the text compression example of section 92 comes from
bentley et al a locally adaptive data compression scheme bstw86 for
more on zivlempel coding see data compression methods and theory by
james a storer sto88 knuth covers selforganizing lists and zipf distributions
in volume 3 of the art of computer programmingknu98
introduction to modern information retrieval by salton and mcgill sm83 is
an excellent source for more information about document retrieval techniques
see the paper practical minimal perfect hash functions for large databases
by fox et al fhcd92 for an introduction and a good algorithm for perfect hashing
for further details on the analysis for various collision resolution policies see
knuth volume 3 knu98 and concrete mathematics a foundation for computer
science by graham knuth and patashnik gkp94
the model of hashing presented in this chapter has been of a fixedsize hash
table a problem not addressed is what to do when the hash table gets half full and
more records must be inserted this is the domain of dynamic hashing methods
a good introduction to this topic is dynamic hashing schemes by rj enbody
and hc du ed88

96

exercises

91 create a graph showing expected cost versus the probability of an unsuccessful search when performing sequential search see section 91 what


$$@@$$PAGE: 365
346

92

93

94

95

96

chap 9 searching

can you say qualitatively about the rate of increase in expected cost as the
probability of unsuccessful search grows
modify the binary search routine of section 35 to implement interpolation
search assume that keys are in the range 1 to 10000 and that all key values
within the range are equally likely to occur
write an algorithm to find the kth smallest value in an unsorted array of n
numbers k  n your algorithm should require n time in the average
case hint your algorithm should look similar to quicksort
example 993 discusses a distribution where the relative frequencies of the
records match the harmonic series that is for every occurrence of the first
record the second record will appear half as often the third will appear one
third as often the fourth one quarter as often and so on the actual probability for the ith record was defined to be 1ihn  explain why this is
correct
graph the equations tn  log2 n and tn  n loge n which gives the
better performance binary search on a sorted list or sequential search on a
list ordered by frequency where the frequency conforms to a zipf distribution characterize the difference in running times
assume that the values a through h are stored in a selforganizing list initially in ascending order consider the three selforganizing list heuristics
count movetofront and transpose for count assume that the record is
moved ahead in the list passing over any other record that its count is now
greater than for each show the resulting list and the total number of comparisons required resulting from the following series of accesses
d h h g h e g h g h e c e h g

97 for each of the three selforganizing list heuristics count movetofront and
transpose describe a series of record accesses for which it would require the
greatest number of comparisons of the three
98 write an algorithm to implement the frequency count selforganizing list
heuristic assuming that the list is implemented using an array in particular write a function freqcount that takes as input a value to be searched
for and which adjusts the list appropriately if the value is not already in the
list add it to the end of the list with a frequency count of one
99 write an algorithm to implement the movetofront selforganizing list heuristic assuming that the list is implemented using an array in particular write
a function movetofront that takes as input a value to be searched for and
which adjusts the list appropriately if the value is not already in the list add
it to the beginning of the list
910 write an algorithm to implement the transpose selforganizing list heuristic
assuming that the list is implemented using an array in particular write


$$@@$$PAGE: 366
sec 96 exercises

347

a function transpose that takes as input a value to be searched for and
which adjusts the list appropriately if the value is not already in the list add
it to the end of the list
911 write functions for computing union intersection and set difference on arbitrarily long bit vectors used to represent set membership as described in
section 93 assume that for each operation both vectors are of equal length
912 compute the probabilities for the following situations these probabilities
can be computed analytically or you may write a computer program to generate the probabilities by simulation
a out of a group of 23 students what is the probability that 2 students
share the same birthday
b out of a group of 100 students what is the probability that 3 students
share the same birthday
c how many students must be in the class for the probability to be at least
50 that there are 2 who share a birthday in the same month
913 assume that you are hashing key k to a hash table of n slots indexed from
0 to n  1 for each of the following functions hk is the function acceptable as a hash function ie would the hash program work correctly for
both insertions and searches and if so is it a good hash function function
randomn returns a random integer between 0 and n  1 inclusive
a
b
c
d

hk  kn where k and n are integers
hk  1
hk  k  randomn mod n
hk  k mod n where n is a prime number

914 assume that you have a sevenslot closed hash table the slots are numbered
0 through 6 show the final hash table that would result if you used the
hash function hk  k mod 7 and linear probing on this list of numbers
3 12 9 2 after inserting the record with key value 2 list for each empty
slot the probability that it will be the next one filled
915 assume that you have a tenslot closed hash table the slots are numbered 0
through 9 show the final hash table that would result if you used the hash
function hk  k mod 10 and quadratic probing on this list of numbers
3 12 9 2 79 46 after inserting the record with key value 46 list for each
empty slot the probability that it will be the next one filled
916 assume that you have a tenslot closed hash table the slots are numbered
0 through 9 show the final hash table that would result if you used the
hash function hk  k mod 10 and pseudorandom probing on this list of
numbers 3 12 9 2 79 44 the permutation of offsets to be used by the
pseudorandom probing will be 5 9 2 1 4 8 6 3 7 after inserting the
record with key value 44 list for each empty slot the probability that it will
be the next one filled


$$@@$$PAGE: 367
348

chap 9 searching

917 what is the result of running sfold from section 941 on the following
strings assume a hash table size of 101 slots
a hello world
b now hear this
c hear this now
918 using closed hashing with double hashing to resolve collisions insert the
following keys into a hash table of thirteen slots the slots are numbered
0 through 12 the hash functions to be used are h1 and h2 defined below you should show the hash table after all eight keys have been inserted
be sure to indicate how you are using h1 and h2 to do the hashing function revk reverses the decimal digits of k for example rev37  73
rev7  7
h1k  k mod 13
h2k  revk  1 mod 11
keys 2 8 31 20 19 18 53 27
919 write an algorithm for a deletion function for hash tables that replaces the
record with a special value indicating a tombstone modify the functions
hashinsert and hashsearch to work correctly with tombstones
920 consider the following permutation for the numbers 1 to 6
2 4 6 1 3 5
analyze what will happen if this permutation is used by an implementation of
pseudorandom probing on a hash table of size seven will this permutation
solve the problem of primary clustering what does this say about selecting
a permutation for use when implementing pseudorandom probing

97

projects

91 implement a binary search and the quadratic binary search of section 91
run your implementations over a large range of problem sizes timing the
results for each algorithm graph and compare these timing results
92 implement the three selforganizing list heuristics count movetofront and
transpose compare the cost for running the three heuristics on various input
data the cost metric should be the total number of comparisons required
when searching the list it is important to compare the heuristics using input
data for which selforganizing lists are reasonable that is on frequency distributions that are uneven one good approach is to read text files the list
should store individual words in the text file begin with an empty list as
was done for the text compression example of section 92 each time a word
is encountered in the text file search for it in the selforganizing list if the


$$@@$$PAGE: 368
sec 97 projects

93
94

95

96

349

word is found reorder the list as appropriate if the word is not in the list
add it to the end of the list and then reorder as appropriate
implement the text compression system described in section 92
implement a system for managing document retrieval your system should
have the ability to insert abstract references to documents into the system
associate keywords with a given document and to search for documents with
specified keywords
implement a database stored on disk using bucket hashing define records to
be 128 bytes long with a 4byte key and 120 bytes of data the remaining
4 bytes are available for you to store necessary information to support the
hash table a bucket in the hash table will be 1024 bytes long so each bucket
has space for 8 records the hash table should consist of 27 buckets total
space for 216 records with slots indexed by positions 0 to 215 followed by
the overflow bucket at record position 216 in the file the hash function for
key value k should be k mod 213 note that this means the last three
slots in the table will not be home positions for any record the collision
resolution function should be linear probing with wraparound within the
bucket for example if a record is hashed to slot 5 the collision resolution
process will attempt to insert the record into the table in the order 5 6 7 0
1 2 3 and finally 4 if a bucket is full the record should be placed in the
overflow section at the end of the file
your hash table should implement the dictionary adt of section 44 when
you do your testing assume that the system is meant to store about 100 or so
records at a time
implement the dictionary adt of section 44 by means of a hash table with
linear probing as the collision resolution policy you might wish to begin
with the code of figure 99 using empirical simulation determine the cost
of insert and delete as  grows ie reconstruct the dashed lines of figure 910 then repeat the experiment using quadratic probing and pseudorandom probing what can you say about the relative performance of these
three collision resolution policies


$$@@$$PAGE: 369

$$@@$$PAGE: 370
10
indexing

many largescale computing applications are centered around data sets that are too
large to fit into main memory the classic example is a large database of records
with multiple search keys requiring the ability to insert delete and search for
records hashing provides outstanding performance for such situations but only
in the limited case in which all searches are of the form find the record with key
value k many applications require more general search capabilities one example is a range query search for all records whose key lies within some range other
queries might involve visiting all records in order of their key value or finding the
record with the greatest key value hash tables are not organized to support any of
these queries efficiently
this chapter introduces file structures used to organize a large collection of
records stored on disk such file structures support efficient insertion deletion and
search operations for exactmatch queries range queries and largestsmallest key
value searches
before discussing such file structures we must become familiar with some basic fileprocessing terminology an entrysequenced file stores records in the order
that they were added to the file entrysequenced files are the diskbased equivalent
to an unsorted list and so do not support efficient search the natural solution is to
sort the records by order of the search key however a typical database such as a
collection of employee or customer records maintained by a business might contain multiple search keys to answer a question about a particular customer might
require a search on the name of the customer businesses often wish to sort and
output the records by zip code order for a bulk mailing government paperwork
might require the ability to search by social security number thus there might
not be a single correct order in which to store the records
indexing is the process of associating a key with the location of a corresponding data record section 85 discussed the concept of a key sort in which an index
file is created whose records consist of keypointer pairs here each key is associated with a pointer to a complete record in the main database file the index file
351


$$@@$$PAGE: 371
352

chap 10 indexing

could be sorted or organized using a tree structure thereby imposing a logical order on the records without physically rearranging them one database might have
several associated index files each supporting efficient access through a different
key field
each record of a database normally has a unique identifier called the primary
key for example the primary key for a set of personnel records might be the
social security number or id number for the individual unfortunately the id
number is generally an inconvenient value on which to perform a search because
the searcher is unlikely to know it instead the searcher might know the desired
employees name alternatively the searcher might be interested in finding all
employees whose salary is in a certain range if these are typical search requests
to the database then the name and salary fields deserve separate indices however
key values in the name and salary indices are not likely to be unique
a key field such as salary where a particular key value might be duplicated in
multiple records is called a secondary key most searches are performed using a
secondary key the secondary key index or more simply secondary index will
associate a secondary key value with the primary key of each record having that
secondary key value at this point the full database might be searched directly
for the record with that primary key or there might be a primary key index or
primary index that relates each primary key value with a pointer to the actual
record on disk in the latter case only the primary index provides the location of
the actual record on disk while the secondary indices refer to the primary index
indexing is an important technique for organizing large databases and many
indexing methods have been developed direct access through hashing is discussed
in section 94 a simple list sorted by key value can also serve as an index to the
record file indexing disk files by sorted lists are discussed in the following section
unfortunately a sorted list does not perform well for insert and delete operations
a third approach to indexing is the tree index trees are typically used to organize large databases that must support record insertion deletion and key range
searches section 102 briefly describes isam a tentative step toward solving the
problem of storing a large database that must support insertion and deletion of
records its shortcomings help to illustrate the value of tree indexing techniques
section 103 introduces the basic issues related to tree indexing section 104 introduces the 23 tree a balanced tree structure that is a simple form of the btree
covered in section 105 btrees are the most widely used indexing method for
large diskbased databases and for implementing file systems since they have
such great practical importance many variations have been invented section 105
begins with a discussion of the variant normally referred to simply as a btree
section 1051 presents the most widely implemented variant the b tree


$$@@$$PAGE: 372
353

sec 101 linear indexing

linear index
37

73

42

52

73

52

98

98

37 42

database records
figure 101 linear indexing for variablelength records each record in the
index file is of fixed length and contains a pointer to the beginning of the corresponding record in the database file

101

linear indexing

a linear index is an index file organized as a sequence of keypointer pairs where
the keys are in sorted order and the pointers either 1 point to the position of the
complete record on disk 2 point to the position of the primary key in the primary
index or 3 are actually the value of the primary key depending on its size a
linear index might be stored in main memory or on disk a linear index provides
a number of advantages it provides convenient access to variablelength database
records because each entry in the index file contains a fixedlength key field and
a fixedlength pointer to the beginning of a variablelength record as shown in
figure 101 a linear index also allows for efficient search and random access to
database records because it is amenable to binary search
if the database contains enough records the linear index might be too large
to store in main memory this makes binary search of the index more expensive
because many disk accesses would typically be required by the search process one
solution to this problem is to store a secondlevel linear index in main memory that
indicates which disk block in the index file stores a desired key for example the
linear index on disk might reside in a series of 1024byte blocks if each keypointer
pair in the linear index requires 8 bytes a 4byte key and a 4byte pointer then
128 keypointer pairs are stored per block the secondlevel index stored in main
memory consists of a simple table storing the value of the key in the first position
of each block in the linear index file this arrangement is shown in figure 102 if
the linear index requires 1024 disk blocks 1mb the secondlevel index contains
only 1024 entries one per disk block to find which disk block contains a desired
search key value first search through the 1024entry table to find the greatest value
less than or equal to the search key this directs the search to the proper block in
the index file which is then read into memory at this point a binary search within
this block will produce a pointer to the actual record in the database because the


$$@@$$PAGE: 373
354

chap 10 indexing

1

2003 5894 10528

second level index
1

2001 2003

5688 5894

9942 10528

10984

linear index disk blocks
figure 102 a simple twolevel linear index the linear index is stored on disk
the smaller secondlevel index is stored in main memory each element in the
secondlevel index stores the first key value in the corresponding disk block of the
index file in this example the first disk block of the linear index stores keys in
the range 1 to 2001 and the second disk block stores keys in the range 2003 to
5688 thus the first entry of the secondlevel index is key value 1 the first key
in the first block of the linear index while the second entry of the secondlevel
index is key value 2003

secondlevel index is stored in main memory accessing a record by this method
requires two disk reads one from the index file and one from the database file for
the actual record
every time a record is inserted to or deleted from the database all associated
secondary indices must be updated updates to a linear index are expensive because the entire contents of the array might be shifted another problem is that
multiple records with the same secondary key each duplicate that key value within
the index when the secondary key field has many duplicates such as when it has
a limited range eg a field to indicate job category from among a small number of
possible job categories this duplication might waste considerable space
one improvement on the simple sorted array is a twodimensional array where
each row corresponds to a secondary key value a row contains the primary keys
whose records have the indicated secondary key value figure 103 illustrates this
approach now there is no duplication of secondary key values possibly yielding a
considerable space savings the cost of insertion and deletion is reduced because
only one row of the table need be adjusted note that a new row is added to the array
when a new secondary key value is added this might lead to moving many records
but this will happen infrequently in applications suited to using this arrangement
a drawback to this approach is that the array must be of fixed size which
imposes an upper limit on the number of primary keys that might be associated
with a particular secondary key furthermore those secondary keys with fewer
records than the width of the array will waste the remainder of their row a better
approach is to have a onedimensional array of secondary key values where each
secondary key is associated with a linked list this works well if the index is stored
in main memory but not so well when it is stored on disk because the linked list
for a given key might be scattered across several disk blocks


$$@@$$PAGE: 374
355

sec 101 linear indexing

jones

aa10

ab12

ab39

smith

ax33

ax35

zx45

zukowski

zq99

ff37

figure 103 a twodimensional linear index each row lists the primary keys
associated with a particular secondary key value in this example the secondary
key is a name the primary key is a unique fourcharacter code

secondary
key

primary
key

jones

aa10

smith

ab12

zukowski

ab39
ff37
ax33
ax35
zx45
zq99

figure 104 illustration of an inverted list each secondary key value is stored
in the secondary key list each secondary key value on the list has a pointer to a
list of the primary keys whose associated records have that secondary key value

consider a large database of employee records if the primary key is the employees id number and the secondary key is the employees name then each
record in the name index associates a name with one or more id numbers the
id number index in turn associates an id number with a unique pointer to the full
record on disk the secondary key index in such an organization is also known
as an inverted list or inverted file it is inverted in that searches work backwards
from the secondary key to the primary key to the actual data record it is called a
list because each secondary key value has conceptually a list of primary keys associated with it figure 104 illustrates this arrangement here we have last names
as the secondary key the primary key is a fourcharacter unique identifier
figure 105 shows a better approach to storing inverted lists an array of secondary key values is shown as before associated with each secondary key is a
pointer to an array of primary keys the primary key array uses a linkedlist implementation this approach combines the storage for all of the secondary key lists
into a single array probably saving space each record in this array consists of a


$$@@$$PAGE: 375
356

chap 10 indexing

secondary
key
index

primary
key next

jones

0

0 aa10

4

smith

1

1 ax33

6

zukowski

3

2 zx45
3 zq99
4 ab12

5

5 ab39

7

6 ax35

2

7

ff37

figure 105 an inverted list implemented as an array of secondary keys and
combined lists of primary keys each record in the secondary key array contains
a pointer to a record in the primary key array the next field of the primary key
array indicates the next record with that secondary key value

primary key value and a pointer to the next element on the list it is easy to insert
and delete secondary keys from this array making this a good implementation for
diskbased inverted files

102

isam

how do we handle large databases that require frequent update the main problem
with the linear index is that it is a single large array that does not adjust well to
updates because a single update can require changing the position of every key in
the index inverted lists reduce this problem but they are only suitable for secondary key indices with many fewer secondary key values than records the linear
index would perform well as a primary key index if it could somehow be broken
into pieces such that individual updates affect only a part of the index this concept will be pursued throughout the rest of this chapter eventually culminating in
the b tree the most widely used indexing method today but first we begin by
studying isam an early attempt to solve the problem of large databases requiring
frequent update its weaknesses help to illustrate why the b tree works so well
before the invention of effective tree indexing schemes a variety of diskbased
indexing methods were in use all were rather cumbersome largely because no
adequate method for handling updates was known typically updates would cause
the index to degrade in performance isam is one example of such an index and
was widely used by ibm prior to adoption of the btree
isam is based on a modified form of the linear index as illustrated by figure 106 records are stored in sorted order by primary key the disk file is divided


$$@@$$PAGE: 376
357

sec 102 isam

inmemory
table of
cylinder keys
cylinder 1
cylinder
index

cylinder 2
cylinder
index

records

records

cylinder
overflow

cylinder
overflow

system
overflow
figure 106 illustration of the isam indexing system

among a number of cylinders on disk1 each cylinder holds a section of the list in
sorted order initially each cylinder is not filled to capacity and the extra space is
set aside in the cylinder overflow in memory is a table listing the lowest key value
stored in each cylinder of the file each cylinder contains a table listing the lowest
key value for each block in that cylinder called the cylinder index when new
records are inserted they are placed in the correct cylinders overflow area in effect a cylinder acts as a bucket if a cylinders overflow area fills completely then
a systemwide overflow area is used search proceeds by determining the proper
cylinder from the systemwide table kept in main memory the cylinders block
table is brought in from disk and consulted to determine the correct block if the
record is found in that block then the search is complete otherwise the cylinders overflow area is searched if that is full and the record is not found then the
systemwide overflow is searched
after initial construction of the database so long as no new records are inserted
or deleted access is efficient because it requires only two disk fetches the first
disk fetch recovers the block table for the desired cylinder the second disk fetch
recovers the block that under good conditions contains the record after many
inserts the overflow list becomes too long resulting in significant search time as
the cylinder overflow area fills up under extreme conditions many searches might
eventually lead to the system overflow area the solution to this problem is to
periodically reorganize the entire database this means rebalancing the records
1

recall from section 821 that a cylinder is all of the tracks readable from a particular placement
of the heads on the multiple platters of a disk drive


$$@@$$PAGE: 377
358

chap 10 indexing

among the cylinders sorting the records within each cylinder and updating both
the system index table and the withincylinder block table such reorganization
was typical of database systems during the 1960s and would normally be done
each night or weekly

103

treebased indexing

linear indexing is efficient when the database is static that is when records are
inserted and deleted rarely or never isam is adequate for a limited number of
updates but not for frequent changes because it has essentially two levels of
indexing isam will also break down for a truly large database where the number
of cylinders is too great for the toplevel index to fit in main memory
in their most general form database applications have the following characteristics
1 large sets of records that are frequently updated
2 search is by one or a combination of several keys
3 key range queries or minmax queries are used
for such databases a better organization must be found one approach would
be to use the binary search tree bst to store primary and secondary key indices
bsts can store duplicate key values they provide efficient insertion and deletion as
well as efficient search and they can perform efficient range queries when there
is enough main memory the bst is a viable option for implementing both primary
and secondary key indices
unfortunately the bst can become unbalanced even under relatively good
conditions the depth of leaf nodes can easily vary by a factor of two this might
not be a significant concern when the tree is stored in main memory because the
time required is still log n for search and update when the tree is stored on
disk however the depth of nodes in the tree becomes crucial every time a bst
node b is visited it is necessary to visit all nodes along the path from the root to b
each node on this path must be retrieved from disk each disk access returns a
block of information if a node is on the same block as its parent then the cost to
find that node is trivial once its parent is in main memory thus it is desirable to
keep subtrees together on the same block unfortunately many times a node is not
on the same block as its parent thus each access to a bst node could potentially
require that another block to be read from disk using a buffer pool to store multiple
blocks in memory can mitigate disk access problems if bst accesses display good
locality of reference but a buffer pool cannot eliminate disk io entirely the
problem becomes greater if the bst is unbalanced because nodes deep in the tree
have the potential of causing many disk blocks to be read thus there are two
significant issues that must be addressed to have efficient search from a diskbased


$$@@$$PAGE: 378
359

sec 103 treebased indexing

figure 107 breaking the bst into blocks the bst is divided among disk
blocks each with space for three nodes the path from the root to any leaf is
contained on two blocks

5

4

3

2

7

4

6
a

2

1

6

3

5

7

b

figure 108 an attempt to rebalance a bst after insertion can be expensive
a a bst with six nodes in the shape of a complete binary tree b a node with
value 1 is inserted into the bst of a to maintain both the complete binary tree
shape and the bst property a major reorganization of the tree is required

bst the first is how to keep the tree balanced the second is how to arrange the
nodes on blocks so as to keep the number of blocks encountered on any path from
the root to the leaves at a minimum
we could select a scheme for balancing the bst and allocating bst nodes to
blocks in a way that minimizes disk io as illustrated by figure 107 however
maintaining such a scheme in the face of insertions and deletions is difficult in
particular the tree should remain balanced when an update takes place but doing
so might require much reorganization each update should affect only a few blocks
or its cost will be too high as you can see from figure 108 adopting a rule such
as requiring the bst to be complete can cause a great deal of rearranging of data
within the tree
we can solve these problems by selecting another tree structure that automatically remains balanced after updates and which is amenable to storing in blocks
there are a number of balanced tree data structures and there are also techniques
for keeping bsts balanced examples are the avl and splay trees discussed in
section 132 as an alternative section 104 presents the 23 tree which has the
property that its leaves are always at the same level the main reason for discussing
the 23 tree here in preference to the other balanced search trees is that it naturally


$$@@$$PAGE: 379
360

chap 10 indexing

18 33

12

10

23 30

15

20 21

24

48

31

45 47

50 52

figure 109 a 23 tree

leads to the btree of section 105 which is by far the most widely used indexing
method today

104

23 trees

this section presents a data structure called the 23 tree the 23 tree is not a binary
tree but instead its shape obeys the following definition
1 a node contains one or two keys
2 every internal node has either two children if it contains one key or three
children if it contains two keys hence the name
3 all leaves are at the same level in the tree so the tree is always height balanced
in addition to these shape properties the 23 tree has a search tree property
analogous to that of a bst for every node the values of all descendants in the left
subtree are less than the value of the first key while values in the center subtree
are greater than or equal to the value of the first key if there is a right subtree
equivalently if the node stores two keys then the values of all descendants in
the center subtree are less than the value of the second key while values in the
right subtree are greater than or equal to the value of the second key to maintain
these shape and search properties requires that special action be taken when nodes
are inserted and deleted the 23 tree has the advantage over the bst in that the
23 tree can be kept height balanced at relatively low cost
figure 109 illustrates the 23 tree nodes are shown as rectangular boxes with
two key fields these nodes actually would contain complete records or pointers
to complete records but the figures will show only the keys internal nodes with
only two children have an empty right key field leaf nodes might contain either
one or two keys figure 1010 is an implementation for the 23 tree node class
ttnode is assumed to be a private class of the the 23 tree class tttree and thus
the data members of ttnode will be made public to simplify the presentation
note that this sample declaration does not distinguish between leaf and internal
nodes and so is space inefficient because leaf nodes store three pointers each the


$$@@$$PAGE: 380
361

sec 104 23 trees

template typename key typename e
class ttnode 
 23 tree node structure
public
e lval
 the nodes left record
key lkey
 left records key
e rval
 the nodes right record
key rkey
 right records key
ttnode left
 pointer to left child
ttnode center
 pointer to middle child
ttnode right
 pointer to right child
ttnode 
center  left  right  null
lkey  rkey  emptykey

ttnodekey lk e lv key rk e rv ttnodekeye p1
ttnodekeye p2 ttnodekeye p3 
lkey  lk rkey  rk
lval  lv rval  rv
left  p1 center  p2 right  p3

ttnode  
bool isleaf  return left  null 
ttnodekeye addttnodekeye it

figure 1010 the 23 tree node implementation

techniques of section 531 can be applied here to implement separate internal and
leaf node types
from the defining rules for 23 trees we can derive relationships between the
number of nodes in the tree and the depth of the tree a 23 tree of height k has at
least 2k1 leaves because if every internal node has two children it degenerates to
the shape of a complete binary tree a 23 tree of height k has at most 3k1 leaves
because each internal node can have at most three children
searching for a value in a 23 tree is similar to searching in a bst search
begins at the root if the root does not contain the search key k then the search
progresses to the only subtree that can possibly contain k the values stored in
the root node determine which is the correct subtree for example if searching for
the value 30 in the tree of figure 109 we begin with the root node because 30 is
between 18 and 33 it can only be in the middle subtree searching the middle child
of the root node yields the desired record if searching for 15 then the first step is
again to search the root node because 15 is less than 18 the first left branch is
taken at the next level we take the second branch to the leaf node containing 15
if the search key were 16 then upon encountering the leaf containing 15 we would
find that the search key is not in the tree figure 1011 is an implementation for the
23 tree search method


$$@@$$PAGE: 381
362

chap 10 indexing

 find the record that matches a given key value
template typename key typename e
e tttreekey e
findhelpttnodekeye root key k const 
if root  null return null
 value not found
if k  rootlkey return rootlval
if k  rootrkey return rootrval
if k  rootlkey
 go left
return findhelprootleft k
else if rootrkey  emptykey
 2 child node
return findhelprootcenter k
 go center
else if k  rootrkey
return findhelprootcenter k
 go center
else return findhelprootright k
 go right

figure 1011 implementation for the 23 tree search method

18 33

12

10

23 30

15 15

20 21

24

48

31

45 47

50 52

14
figure 1012 simple insert into the 23 tree of figure 109 the value 14 is
inserted into the tree at the leaf node containing 15 because there is room in the
node for a second key it is simply added to the left position with 15 moved to the
right position

insertion into a 23 tree is similar to insertion into a bst to the extent that the
new record is placed in the appropriate leaf node unlike bst insertion a new
child is not created to hold the record being inserted that is the 23 tree does not
grow downward the first step is to find the leaf node that would contain the record
if it were in the tree if this leaf node contains only one value then the new record
can be added to that node with no further modification to the tree as illustrated in
figure 1012 in this example a record with key value 14 is inserted searching
from the root we come to the leaf node that stores 15 we add 14 as the left value
pushing the record with key 15 to the rightmost position
if we insert the new record into a leaf node l that already contains two records
then more space must be created consider the two records of node l and the
record to be inserted without further concern for which two were already in l and
which is the new record the first step is to split l into two nodes thus a new
node  call it l0  must be created from free store l receives the record with
the least of the three key values l0 receives the greatest of the three the record


$$@@$$PAGE: 382
363

sec 104 23 trees

18 33

12

10

23 30

15

20 21

24

48 52

31

45 47

50

55

figure 1013 a simple nodesplitting insert for a 23 tree the value 55 is added
to the 23 tree of figure 109 this makes the node containing values 50 and 52
split promoting value 52 to the parent node

with the middle of the three key value is passed up to the parent node along with a
pointer to l0  this is called a promotion the promoted key is then inserted into
the parent if the parent currently contains only one record and thus has only two
children then the promoted record and the pointer to l0 are simply added to the
parent node if the parent is full then the splitandpromote process is repeated
figure 1013 illustrates a simple promotion figure 1014 illustrates what happens
when promotions require the root to split adding a new level to the tree in either
case all leaf nodes continue to have equal depth figures 1015 and 1016 present
an implementation for the insertion process
note that inserthelp of figure 1015 takes three parameters the first is
a pointer to the root of the current subtree named rt the second is the key for
the record to be inserted and the third is the record itself the return value for
inserthelp is a pointer to a 23 tree node if rt is unchanged then a pointer to
rt is returned if rt is changed due to the insertion causing the node to split then
a pointer to the new subtree root is returned with the key value and record value in
the leftmost fields and a pointer to the single subtree in the center pointer field
this revised node will then be added to the parent as illustrated in figure 1014
when deleting a record from the 23 tree there are three cases to consider the
simplest occurs when the record is to be removed from a leaf node containing two
records in this case the record is simply removed and no other nodes are affected
the second case occurs when the only record in a leaf node is to be removed the
third case occurs when a record is to be removed from an internal node in both
the second and the third cases the deleted record is replaced with another that can
take its place while maintaining the correct order similar to removing a node from
a bst if the tree is sparse enough there is no such record available that will allow
all nodes to still maintain at least one record in this situation sibling nodes are
merged together the delete operation for the 23 tree is excessively complex and
will not be described further instead a complete discussion of deletion will be
postponed until the next section where it can be generalized for a particular variant
of the btree


$$@@$$PAGE: 383
364

chap 10 indexing

18 33
23

19

20

23 30

20

21

24

31

19

30

21

a

24

31

b

23

18

33

12

10

20

15

19

30

21

24

48

31

45 47

50 52

c

figure 1014 example of inserting a record that causes the 23 tree root to split
a the value 19 is added to the 23 tree of figure 109 this causes the node
containing 20 and 21 to split promoting 20 b this in turn causes the internal
node containing 23 and 30 to split promoting 23 c finally the root node splits
promoting 23 to become the left record in the new root the result is that the tree
becomes one level higher

the 23 tree insert and delete routines do not add new nodes at the bottom of
the tree instead they cause leaf nodes to split or merge possibly causing a ripple
effect moving up the tree to the root if necessary the root will split causing a new
root node to be created and making the tree one level deeper on deletion if the
last two children of the root merge then the root node is removed and the tree will
lose a level in either case all leaf nodes are always at the same level when all
leaf nodes are at the same level we say that a tree is height balanced because the
23 tree is height balanced and every internal node has at least two children we
know that the maximum depth of the tree is log n thus all 23 tree insert find
and delete operations require log n time

105

btrees

this section presents the btree btrees are usually attributed to r bayer and
e mccreight who described the btree in a 1972 paper by 1979 btrees had re


$$@@$$PAGE: 384
365

sec 105 btrees

template typename key typename e
ttnodekeye tttreekey e
inserthelpttnodekeye rt const key k const e e 
ttnodekeye retval
if rt  null  empty tree create a leaf node for root
return new ttnodekeyek e emptykey null
null null null
if rtisleaf  at leaf node insert here
return rtaddnew ttnodekeyek e emptykey null
null null null
 add to internal node
if k  rtlkey 
retval  inserthelprtleft k e
if retval  rtleft return rt
else return rtaddretval

else ifrtrkey  emptykey  k  rtrkey 
retval  inserthelprtcenter k e
if retval  rtcenter return rt
else return rtaddretval

else   insert right
retval  inserthelprtright k e
if retval  rtright return rt
else return rtaddretval


figure 1015 the 23 tree insert routine

placed virtually all largefile access methods other than hashing btrees or some
variant of btrees are the standard file organization for applications requiring insertion deletion and key range searches they are used to implement most modern
file systems btrees address effectively all of the major problems encountered
when implementing diskbased search trees
1 btrees are always height balanced with all leaf nodes at the same level
2 update and search operations affect only a few disk blocks the fewer the
number of disk blocks affected the less disk io is required
3 btrees keep related records that is records with similar key values on the
same disk block which helps to minimize disk io on searches due to locality
of reference
4 btrees guarantee that every node in the tree will be full at least to a certain
minimum percentage this improves space efficiency while reducing the
typical number of disk fetches necessary during a search or update operation
a btree of order m is defined to have the following shape properties
 the root is either a leaf or has at least two children
 each internal node except for the root has between dm2e and m children


$$@@$$PAGE: 385
366

chap 10 indexing

 add a new keyvalue pair to the node there might be a
 subtree associated with the record being added this
 information comes in the form of a 23 tree node with
 one key and a possibly null subtree through the
 center pointer field
template typename key typename e
ttnodekeye ttnodekey eaddttnodekeye it 
if rkey  emptykey   only one key add here
if lkey  itlkey 
rkey  itlkey rval  itlval
right  center center  itcenter

else 
rkey  lkey rval  lval right  center
lkey  itlkey lval  itlval
center  itcenter

return this

else if lkey  itlkey   add left
center  new ttnodekeyerkey rval emptykey null
center right null
rkey  emptykey rval  null right  null
itleft  left left  it
return this

else if rkey  itlkey   add center
itcenter  new ttnodekeyerkey rval emptykey
null itcenter right null
itleft  this
rkey  emptykey rval  null right  null
return it

else   add right
ttnodekeye n1  new ttnodekeyerkey rval
emptykey null this it null
itleft  right
right  null rkey  emptykey rval  null
return n1


figure 1016 the 23 tree node add method


$$@@$$PAGE: 386
367

sec 105 btrees

24

15 20

10 12

18

33 45 48

21 23

30 31

38

47

50 52 60

figure 1017 a btree of order four

 all leaves are at the same level in the tree so the tree is always height balanced
the btree is a generalization of the 23 tree put another way a 23 tree is a
btree of order three normally the size of a node in the btree is chosen to fill a
disk block a btree node implementation typically allows 100 or more children
thus a btree node is equivalent to a disk block and a pointer value stored
in the tree is actually the number of the block containing the child node usually
interpreted as an offset from the beginning of the corresponding disk file in a
typical application the btrees access to the disk file will be managed using a
buffer pool and a blockreplacement scheme such as lru see section 83
figure 1017 shows a btree of order four each node contains up to three keys
and internal nodes have up to four children
search in a btree is a generalization of search in a 23 tree it is an alternating
twostep process beginning with the root node of the btree
1 perform a binary search on the records in the current node if a record with
the search key is found then return that record if the current node is a leaf
node and the key is not found then report an unsuccessful search
2 otherwise follow the proper branch and repeat the process
for example consider a search for the record with key value 47 in the tree of
figure 1017 the root node is examined and the second right branch taken after
examining the node at level 1 the third branch is taken to the next level to arrive at
the leaf node containing a record with key value 47
btree insertion is a generalization of 23 tree insertion the first step is to find
the leaf node that should contain the key to be inserted space permitting if there
is room in this node then insert the key if there is not then split the node into two
and promote the middle key to the parent if the parent becomes full then it is split
in turn and its middle key promoted
note that this insertion process is guaranteed to keep all nodes at least half full
for example when we attempt to insert into a full internal node of a btree of order
four there will now be five children that must be dealt with the node is split into


$$@@$$PAGE: 387
368

chap 10 indexing

two nodes containing two keys each thus retaining the btree property the middle
of the five children is promoted to its parent
1051

b trees

the previous section mentioned that btrees are universally used to implement
largescale diskbased systems actually the btree as described in the previous section is almost never implemented nor is the 23 tree as described in section 104 what is most commonly implemented is a variant of the btree called
the b tree when greater efficiency is required a more complicated variant known
as the b tree is used
when data are static a linear index provides an extremely efficient way to
search the problem is how to handle those pesky inserts and deletes we could try
to keep the core idea of storing a sorted arraybased list but make it more flexible
by breaking the list into manageable chunks that are more easily updated how
might we do that first we need to decide how big the chunks should be since
the data are on disk it seems reasonable to store a chunk that is the size of a disk
block or a small multiple of the disk block size if the next record to be inserted
belongs to a chunk that hasnt filled its block then we can just insert it there the
fact that this might cause other records in that chunk to move a little bit in the array
is not important since this does not cause any extra disk accesses so long as we
move data within that chunk but what if the chunk fills up the entire block that
contains it we could just split it in half what if we want to delete a record we
could just take the deleted record out of the chunk but we might not want a lot of
nearempty chunks so we could put adjacent chunks together if they have only
a small amount of data between them or we could shuffle data between adjacent
chunks that together contain more data the big problem would be how to find
the desired chunk when processing a record with a given key perhaps some sort
of treelike structure could be used to locate the appropriate chunk these ideas
are exactly what motivate the b tree the b tree is essentially a mechanism for
managing a sorted arraybased list where the list is broken into chunks
the most significant difference between the b tree and the bst or the standard btree is that the b tree stores records only at the leaf nodes internal nodes
store key values but these are used solely as placeholders to guide the search this
means that internal nodes are significantly different in structure from leaf nodes
internal nodes store keys to guide the search associating each key with a pointer
to a child b tree node leaf nodes store actual records or else keys and pointers
to actual records in a separate disk file if the b tree is being used purely as an
index depending on the size of a record as compared to the size of a key a leaf
node in a b tree of order m might have enough room to store more or less than
m records the requirement is simply that the leaf nodes store enough records to
remain at least half full the leaf nodes of a b tree are normally linked together


$$@@$$PAGE: 388
369

sec 105 btrees

33

18 23

10 12 15

18 19 20 21 22

48

23 30 31

33 45 47

48 50 52

figure 1018 example of a b tree of order four internal nodes must store
between two and four children for this example the record size is assumed to be
such that leaf nodes store between three and five records

to form a doubly linked list thus the entire collection of records can be traversed
in sorted order by visiting all the leaf nodes on the linked list here is a c like
pseudocode representation for the b tree node class leaf node and internal node
subclasses would implement this base class
 abstract class definition for btrees
template typename key typename eclass bpnode 
public
bpnode lftptr bpnode rghtptr  links to siblings
virtual bpnode   base destructor
virtual bool isleaf const 0  true if node is a leaf
virtual bool isfull const 0  true if node is full
virtual int numrecs const 0  current num of records
virtual key keys const0
 return array of keys


an important implementation detail to note is that while figure 1017 shows
internal nodes containing three keys and four pointers class bpnode is slightly
different in that it stores keypointer pairs figure 1017 shows the b tree as it
is traditionally drawn to simplify implementation in practice nodes really do
associate a key with each pointer each internal node should be assumed to hold
in the leftmost position an additional key that is less than or equal to any possible
key value in the nodes leftmost subtree b tree implementations typically store
an additional dummy record in the leftmost leaf node whose key value is less than
any legal key value
b trees are exceptionally good for range queries once the first record in
the range has been found the rest of the records with keys in the range can be
accessed by sequential processing of the remaining records in the first node and
then continuing down the linked list of leaf nodes as far as necessary figure 1018
illustrates the b tree
search in a b tree is nearly identical to search in a regular btree except that
the search must always continue to the proper leaf node even if the searchkey
value is found in an internal node this is only a placeholder and does not provide


$$@@$$PAGE: 389
370

chap 10 indexing

template typename key typename e
e bptreekey efindhelpbpnodekeye rt const key k
const 
int currec  binarylertkeys rtnumrecs k
if rtisleaf
if bpleafkeyertkeyscurrec  k
return bpleafkeyertrecscurrec
else return null
else
return findhelpbpinternalkeyert
pointerscurrec k

figure 1019 implementation for the b tree search method

access to the actual record to find a record with key value 33 in the b tree of
figure 1018 search begins at the root the value 33 stored in the root merely
serves as a placeholder indicating that keys with values greater than or equal to 33
are found in the second subtree from the second child of the root the first branch
is taken to reach the leaf node containing the actual record or a pointer to the actual
record with key value 33 figure 1019 shows a pseudocode sketch of the b tree
search algorithm
b tree insertion is similar to btree insertion first the leaf l that should
contain the record is found if l is not full then the new record is added and no
other b tree nodes are affected if l is already full split it in two dividing the
records evenly among the two nodes and promote a copy of the leastvalued key
in the newly formed right node as with the 23 tree promotion might cause the
parent to split in turn perhaps eventually leading to splitting the root and causing
the b tree to gain a new level b tree insertion keeps all leaf nodes at equal
depth figure 1020 illustrates the insertion process through several examples figure 1021 shows a c like pseudocode sketch of the b tree insert algorithm
to delete record r from the b tree first locate the leaf l that contains r if l
is more than half full then we need only remove r leaving l still at least half full
this is demonstrated by figure 1022
if deleting a record reduces the number of records in the node below the minimum threshold called an underflow then we must do something to keep the
node sufficiently full the first choice is to look at the nodes adjacent siblings to
determine if they have a spare record that can be used to fill the gap if so then
enough records are transferred from the sibling so that both nodes have about the
same number of records this is done so as to delay as long as possible the next
time when a delete causes this node to underflow again this process might require
that the parent node has its placeholder key value revised to reflect the true first key
value in each node figure 1023 illustrates the process


$$@@$$PAGE: 390
371

sec 105 btrees

33
1012 233348

10 12 23

33 48 50

a

b

18 33 48
1012 15

18 20 2123 31

33 45 47

48 50 52

c

33
18 23
10 12 15

18 20 21

48
23 30 31

33 45 47

48 50 52

d

figure 1020 examples of b tree insertion a a b tree containing five
records b the result of inserting a record with key value 50 into the tree of a
the leaf node splits causing creation of the first internal node c the b tree of
b after further insertions d the result of inserting a record with key value 30
into the tree of c the second leaf node splits which causes the internal node to
split in turn creating a new root

template typename key typename e
bpnodekeye bptreekey einserthelpbpnodekeye rt
const key k const e e 
if rtisleaf  at leaf node insert here
return bpleafkeyertaddk e
 add to internal node
int currec  binarylertkeys rtnumrecs k
bpnodekeye temp  inserthelp
bpinternalkeyerootpointerscurrec k e
if temp  bpinternalkeyertpointerscurrec
return bpinternalkeyert
addk bpinternalkeyetemp
else
return rt

figure 1021 a c like pseudocode sketch of the b tree insert algorithm


$$@@$$PAGE: 391
372

chap 10 indexing

33

18 23

101215

19 2021 22

48

23 30 31

33 45 47

48 50 52

figure 1022 simple deletion from a b tree the record with key value 18 is
removed from the tree of figure 1018 note that even though 18 is also a placeholder used to direct search in the parent node that value need not be removed
from internal nodes even if no record in the tree has key value 18 thus the
leftmost node at level one in this example retains the key with value 18 after the
record with key value 18 has been removed from the second leaf node

33

19 23

101518

19 20 21 22

48

23 30 31

33 45 47

48 50 52

figure 1023 deletion from the b tree of figure 1018 via borrowing from a
sibling the key with value 12 is deleted from the leftmost leaf causing the record
with key value 18 to shift to the leftmost leaf to take its place note that the parent
must be updated to properly indicate the key range within the subtrees in this
example the parent node has its leftmost key value changed to 19

if neither sibling can lend a record to the underfull node call it n  then n
must give its records to a sibling and be removed from the tree there is certainly
room to do this because the sibling is at most half full remember that it had no
records to contribute to the current node and n has become less than half full
because it is underflowing this merge process combines two subtrees of the parent which might cause it to underflow in turn if the last two children of the root
merge together then the tree loses a level figure 1024 illustrates the nodemerge
deletion process figure 1025 shows c like pseudocode for the b tree delete
algorithm
the b tree requires that all nodes be at least half full except for the root
thus the storage utilization must be at least 50 this is satisfactory for many
implementations but note that keeping nodes fuller will result both in less space
required because there is less empty space in the disk file and in more efficient
processing fewer blocks on average will be read into memory because the amount
of information in each block is greater because btrees have become so popular
many algorithm designers have tried to improve btree performance one method


$$@@$$PAGE: 392
373

sec 105 btrees

48
45 4748 50 52
a

23
18
101215

33

18 19 20 21 22

23 30 31

45 47 48 50 52

b

figure 1024 deleting the record with key value 33 from the b tree of figure 1018 via collapsing siblings a the two leftmost leaf nodes merge together
to form a single leaf unfortunately the parent node now has only one child
b because the left subtree has a spare leaf node that node is passed to the right
subtree the placeholder values of the root and the right internal node are updated
to reflect the changes value 23 moves to the root and old root value 33 moves to
the rightmost internal node

 delete a record with the given key value and
return true if the root underflows 
template typename key typename e
bool bptreekey eremovehelpbpnodekeye rt
const key k 
int currec  binarylertkeys rtnumrecs k
if rtisleaf
if bpleafkeyertkeyscurrec  k
return bpleafkeyertdelcurrec
else return false
else  process internal node
if removehelpbpinternalkeyert
pointerscurrec k
 child will merge if necessary
return bpinternalkeyertunderflowcurrec
else return false

figure 1025 c like pseudocode for the b tree delete algorithm


$$@@$$PAGE: 393
374

chap 10 indexing

for doing so is to use the b tree variant known as the b tree the b tree is
identical to the b tree except for the rules used to split and merge nodes instead
of splitting a node in half when it overflows the b tree gives some records to its
neighboring sibling if possible if the sibling is also full then these two nodes split
into three similarly when a node underflows it is combined with its two siblings
and the total reduced to two nodes thus the nodes are always at least two thirds
full2
1052

btree analysis

the asymptotic cost of search insertion and deletion of records from btrees
b trees and b trees is log n where n is the total number of records in the
tree however the base of the log is the average branching factor of the tree
typical database applications use extremely high branching factors perhaps 100 or
more thus in practice the btree and its variants are extremely shallow
as an illustration consider a b tree of order 100 and leaf nodes that contain
up to 100 records a b tree with height one that is just a single leaf node can
have at most 100 records a b tree with height two a root internal node whose
children are leaves must have at least 100 records 2 leaves with 50 records each
it has at most 10000 records 100 leaves with 100 records each a b tree with
height three must have at least 5000 records two secondlevel nodes with 50 children containing 50 records each and at most one million records 100 secondlevel
nodes with 100 full children each a b tree with height four must have at least
250000 records and at most 100 million records thus it would require an extremely large database to generate a b tree of more than height four
the b tree split and insert rules guarantee that every node except perhaps the
root is at least half full so they are on average about 34 full but the internal
nodes are purely overhead since the keys stored there are used only by the tree to
direct search rather than store actual data does this overhead amount to a significant use of space no because once again the high fanout rate of the tree structure
means that the vast majority of nodes are leaf nodes recall from section 64 that
a full kary tree has approximately 1k of its nodes as internal nodes this means
that while half of a full binary trees nodes are internal nodes in a b tree of order
100 probably only about 175 of its nodes are internal nodes this means that the
overhead associated with internal nodes is very low
we can reduce the number of disk fetches required for the btree even more
by using the following methods first the upper levels of the tree can be stored in
2

this concept can be extended further if higher space utilization is required however the update
routines become much more complicated i once worked on a project where we implemented 3for4
node split and merge routines this gave better performance than the 2for3 node split and merge
routines of the b tree however the spitting and merging routines were so complicated that even
their author could no longer understand them once they were completed


$$@@$$PAGE: 394
sec 106 further reading

375

main memory at all times because the tree branches so quickly the top two levels
levels 0 and 1 require relatively little space if the btree is only height four then
at most two disk fetches internal nodes at level two and leaves at level three are
required to reach the pointer to any given record
a buffer pool could be used to manage nodes of the btree several nodes of
the tree would typically be in main memory at one time the most straightforward
approach is to use a standard method such as lru to do node replacement however sometimes it might be desirable to lock certain nodes such as the root into
the buffer pool in general if the buffer pool is even of modest size say at least
twice the depth of the tree no special techniques for node replacement will be
required because the upperlevel nodes will naturally be accessed frequently

106

further reading

for an expanded discussion of the issues touched on in this chapter see a general file processing text such as file structures a conceptual toolkit by folk and
zoellick fz98 in particular folk and zoellick provide a good discussion of
the relationship between primary and secondary indices the most thorough discussion on various implementations for the btree is the survey article by comer
com79 also see sal88 for further details on implementing btrees see shaffer and brown sb93 for a discussion of buffer pool management strategies for
b treelike data structures

107

exercises

101 assume that a computer system has disk blocks of 1024 bytes and that you
are storing records that have 4byte keys and 4byte data fields the records
are sorted and packed sequentially into the disk file
a assume that a linear index uses 4 bytes to store the key and 4 bytes
to store the block id for the associated records what is the greatest
number of records that can be stored in the file if a linear index of size
256kb is used
b what is the greatest number of records that can be stored in the file if
the linear index is also stored on disk and thus its size is limited only
by the secondlevel index when using a secondlevel index of 1024
bytes ie 256 key values as illustrated by figure 102 each element
of the secondlevel index references the smallest key value for a disk
block of the linear index
102 assume that a computer system has disk blocks of 4096 bytes and that you
are storing records that have 4byte keys and 64byte data fields the records
are sorted and packed sequentially into the disk file


$$@@$$PAGE: 395
376

chap 10 indexing

a assume that a linear index uses 4 bytes to store the key and 4 bytes
to store the block id for the associated records what is the greatest
number of records that can be stored in the file if a linear index of size
2mb is used
b what is the greatest number of records that can be stored in the file if
the linear index is also stored on disk and thus its size is limited only by
the secondlevel index when using a secondlevel index of 4096 bytes
ie 1024 key values as illustrated by figure 102 each element of
the secondlevel index references the smallest key value for a disk block
of the linear index
103 modify the function binary of section 35 so as to support variablelength
records with fixedlength keys indexed by a simple linear index as illustrated
by figure 101
104 assume that a database stores records consisting of a 2byte integer key and
a variablelength data field consisting of a string show the linear index as
illustrated by figure 101 for the following collection of records
397
82
1038
1037
42
2222

hello world
xyz
this string is rather long
this is shorter
abc
hello new world

105 each of the following series of records consists of a fourdigit primary key
with no duplicates and a fourcharacter secondary key with many duplicates
3456
2398
2926
9737
7739
9279
1111
8133
7183
7186

deer
deer
duck
deer
goat
duck
frog
deer
duck
frog

a show the inverted list as illustrated by figure 104 for this collection
of records
b show the improved inverted list as illustrated by figure 105 for this
collection of records


$$@@$$PAGE: 396
sec 108 projects

377

106 under what conditions will isam be more efficient than a b tree implementation
107 prove that the number of leaf nodes in a 23 tree with height k is between
2k1 and 3k1 
108 show the result of inserting the values 55 and 46 into the 23 tree of figure 109
109 you are given a series of records whose keys are letters the records arrive
in the following order c s d t a m p i b w n g u r k e h o
l j show the 23 tree that results from inserting these records
1010 you are given a series of records whose keys are letters the records are
inserted in the following order c s d t a m p i b w n g u r k
e h o l j show the tree that results from inserting these records when
the 23 tree is modified to be a 23 tree that is the internal nodes act only
as placeholders assume that the leaf nodes are capable of holding up to two
records
1011 show the result of inserting the value 55 into the btree of figure 1017
1012 show the result of inserting the values 1 2 3 4 5 and 6 in that order into
the b tree of figure 1018
1013 show the result of deleting the values 18 19 and 20 in that order from the
b tree of figure 1024b
1014 you are given a series of records whose keys are letters the records are
inserted in the following order c s d t a m p i b w n g u r
k e h o l j show the b tree of order four that results from inserting
these records assume that the leaf nodes are capable of storing up to three
records
1015 assume that you have a b tree whose internal nodes can store up to 100
children and whose leaf nodes can store up to 15 records what are the
minimum and maximum number of records that can be stored by the b tree
with heights 1 2 3 4 and 5
1016 assume that you have a b tree whose internal nodes can store up to 50
children and whose leaf nodes can store up to 50 records what are the
minimum and maximum number of records that can be stored by the b tree
with heights 1 2 3 4 and 5

108

projects

101 implement a twolevel linear index for variablelength records as illustrated
by figures 101 and 102 assume that disk blocks are 1024 bytes in length
records in the database file should typically range between 20 and 200 bytes
including a 4byte key value each record of the index file should store a
key value and the byte offset in the database file for the first byte of the


$$@@$$PAGE: 397
378

chap 10 indexing

corresponding record the toplevel index stored in memory should be a
simple array storing the lowest key value on the corresponding block in the
index file
102 implement the 23 tree that is a 23 tree where the internal nodes act only
as placeholders your 23 tree should implement the dictionary interface of
section 44
103 implement the dictionary adt of section 44 for a large file stored on disk
by means of the b tree of section 105 assume that disk blocks are
1024 bytes and thus both leaf nodes and internal nodes are also 1024 bytes
records should store a 4byte int key value and a 60byte data field internal nodes should store key valuepointer pairs where the pointer is actually
the block number on disk for the child node both internal nodes and leaf
nodes will need room to store various information such as a count of the
records stored on that node and a pointer to the next node on that level
thus leaf nodes will store 15 records and internal nodes will have room to
store about 120 to 125 children depending on how you implement them use
a buffer pool section 83 to manage access to the nodes stored on disk


$$@@$$PAGE: 398
part iv
advanced data structures

379


$$@@$$PAGE: 399

$$@@$$PAGE: 400
11
graphs

graphs provide the ultimate in data structure flexibility graphs can model both
realworld systems and abstract problems so they are used in hundreds of applications here is a small sampling of the range of problems that graphs are routinely
applied to
1 modeling connectivity in computer and communications networks
2 representing a map as a set of locations with distances between locations
used to compute shortest routes between locations
3 modeling flow capacities in transportation networks
4 finding a path from a starting condition to a goal condition for example in
artificial intelligence problem solving
5 modeling computer algorithms showing transitions from one program state
to another
6 finding an acceptable order for finishing subtasks in a complex activity such
as constructing large buildings
7 modeling relationships such as family trees business or military organizations and scientific taxonomies
we begin in section 111 with some basic graph terminology and then define
two fundamental representations for graphs the adjacency matrix and adjacency
list section 112 presents a graph adt and simple implementations based on the
adjacency matrix and adjacency list section 113 presents the two most commonly
used graph traversal algorithms called depthfirst and breadthfirst search with
application to topological sorting section 114 presents algorithms for solving
some problems related to finding shortest routes in a graph finally section 115
presents algorithms for finding the minimumcost spanning tree useful for determining lowestcost connectivity in a network besides being useful and interesting
in their own right these algorithms illustrate the use of some data structures presented in earlier chapters
381


$$@@$$PAGE: 401
382

chap 11 graphs

0

2
4

3

1
4

7

1
1
a

b

2

3

c

figure 111 examples of graphs and terminology a a graph b a directed
graph digraph c a labeled directed graph with weights associated with the
edges in this example there is a simple path from vertex 0 to vertex 3 containing
vertices 0 1 and 3 vertices 0 1 3 2 4 and 1 also form a path but not a simple
path because vertex 1 appears twice vertices 1 3 2 4 and 1 form a simple cycle

111

terminology and representations

a graph g  v e consists of a set of vertices v and a set of edges e such
that each edge in e is a connection between a pair of vertices in v1 the number
of vertices is written v and the number of edges is written e e can range
from zero to a maximum of v2  v a graph with relatively few edges is called
sparse while a graph with many edges is called dense a graph containing all
possible edges is said to be complete
a graph with edges directed from one vertex to another as in figure 111b
is called a directed graph or digraph a graph whose edges are not directed is
called an undirected graph as illustrated by figure 111a a graph with labels
associated with its vertices as in figure 111c is called a labeled graph two
vertices are adjacent if they are joined by an edge such vertices are also called
neighbors an edge connecting vertices u and v is written u v such an edge
is said to be incident on vertices u and v associated with each edge may be a
cost or weight graphs whose edges have weights as in figure 111c are said to
be weighted
a sequence of vertices v1  v2   vn forms a path of length n  1 if there exist
edges from vi to vi1 for 1  i  n a path is simple if all vertices on the path are
distinct the length of a path is the number of edges it contains a cycle is a path
of length three or more that connects some vertex v1 to itself a cycle is simple if
the path is simple except for the first and last vertices being the same
1
some graph applications require that a given pair of vertices can have multiple or parallel edges
connecting them or that a vertex can have an edge to itself however the applications discussed
in this book do not require either of these special cases so for simplicity we will assume that they
cannot occur


$$@@$$PAGE: 402
383

sec 111 terminology and representations

0

2

6

3

5

7

4

1

figure 112 an undirected graph with three connected components vertices 0
1 2 3 and 4 form one connected component vertices 5 and 6 form a second
connected component vertex 7 by itself forms a third connected component

a subgraph s is formed from graph g by selecting a subset vs of gs vertices
and a subset es of gs edges such that for every edge e in es  both of es vertices
are in vs 
an undirected graph is connected if there is at least one path from any vertex
to any other the maximally connected subgraphs of an undirected graph are called
connected components for example figure 112 shows an undirected graph with
three connected components
a graph without cycles is called acyclic thus a directed graph without cycles
is called a directed acyclic graph or dag
a free tree is a connected undirected graph with no simple cycles an equivalent definition is that a free tree is connected and has v  1 edges
there are two commonly used methods for representing graphs the adjacency matrix is illustrated by figure 113b the adjacency matrix for a graph
is a v  v array assume that v  n and that the vertices are labeled from
v0 through vn1  row i of the adjacency matrix contains entries for vertex vi 
column j in row i is marked if there is an edge from vi to vj and is not marked otherwise thus the adjacency matrix requires one bit at each position alternatively
if we wish to associate a number with each edge such as the weight or distance
between two vertices then each matrix position must store that number in either
case the space requirements for the adjacency matrix are v2 
the second common representation for graphs is the adjacency list illustrated
by figure 113c the adjacency list is an array of linked lists the array is
v items long with position i storing a pointer to the linked list of edges for vertex vi  this linked list represents the edges by the vertices that are adjacent to
vertex vi  the adjacency list is therefore a generalization of the list of children
representation for trees described in section 631
example 111 the entry for vertex 0 in figure 113c stores 1 and 4
because there are two edges in the graph leaving vertex 0 with one going


$$@@$$PAGE: 403
384

chap 11 graphs

0
0

2

0

1

2

1

2

1

3
1

3

4
1

1
4

3

1

1

4

1

a

b

0

1

1

3

2

4

3

2

4

1

4

c

figure 113 two graph representations a a directed graph b the adjacency
matrix for the graph of a c the adjacency list for the graph of a

to vertex 1 and one going to vertex 4 the list for vertex 2 stores an entry
for vertex 4 because there is an edge from vertex 2 to vertex 4 but no entry
for vertex 3 because this edge comes into vertex 2 rather than going out
the storage requirements for the adjacency list depend on both the number of
edges and the number of vertices in the graph there must be an array entry for
each vertex even if the vertex is not adjacent to any other vertex and thus has no
elements on its linked list and each edge must appear on one of the lists thus
the cost is v  e
both the adjacency matrix and the adjacency list can be used to store directed
or undirected graphs each edge of an undirected graph connecting vertices u
and v is represented by two directed edges one from u to v and one from v to
u figure 114 illustrates the use of the adjacency matrix and the adjacency list for
undirected graphs
which graph representation is more space efficient depends on the number of
edges in the graph the adjacency list stores information only for those edges that
actually appear in the graph while the adjacency matrix requires space for each
potential edge whether it exists or not however the adjacency matrix requires
no overhead for pointers which can be a substantial cost especially if the only


$$@@$$PAGE: 404
385

sec 111 terminology and representations

0
0

2

0
1

4

2

4

1

1

1

1

1

1

a

4
1

2

3

3

1

3
1

1

1

1

1

1

b

0

1

4

1

0

3

2

3

4

3

1

2

4

0

1

4

2

c

figure 114 using the graph representations for undirected graphs a an undirected graph b the adjacency matrix for the graph of a c the adjacency list
for the graph of a

information stored for an edge is one bit to indicate its existence as the graph becomes denser the adjacency matrix becomes relatively more space efficient sparse
graphs are likely to have their adjacency list representation be more space efficient
example 112 assume that a vertex index requires two bytes a pointer
requires four bytes and an edge weight requires two bytes then the adjacency matrix for the graph of figure 113 requires 2v2   50 bytes while
the adjacency list requires 4v  6e  56 bytes for the graph of figure 114 the adjacency matrix requires the same space as before while the
adjacency list requires 4v  6e  92 bytes because there are now 12
edges instead of 6
the adjacency matrix often requires a higher asymptotic cost for an algorithm
than would result if the adjacency list were used the reason is that it is common
for a graph algorithm to visit each neighbor of each vertex using the adjacency list
only the actual edges connecting a vertex to its neighbors are examined however
the adjacency matrix must look at each of its v potential edges yielding a total
cost of v2  time when the algorithm might otherwise require only ve


$$@@$$PAGE: 405
386

chap 11 graphs

time this is a considerable disadvantage when the graph is sparse but not when
the graph is closer to full

112

graph implementations

we next turn to the problem of implementing a generalpurpose graph class figure 115 shows an abstract class defining an adt for graphs vertices are defined
by an integer index value in other words there is a vertex 0 vertex 1 and so
on we can assume that a graph application stores any additional information of
interest about a given vertex elsewhere such as a name or applicationdependent
value note that this adt is not implemented using a template because it is the
graph class users responsibility to maintain information related to the vertices
themselves the graph class need have no knowledge of the type or content of
the information associated with a vertex only the index number for that vertex
abstract class graph has methods to return the number of vertices and edges
methods n and e respectively function weight returns the weight of a given
edge with that edge identified by its two incident vertices for example calling
weight0 4 on the graph of figure 111 c would return 4 if no such edge
exists the weight is defined to be 0 so calling weight0 2 on the graph of
figure 111 c would return 0
functions setedge and deledge set the weight of an edge and remove an
edge from the graph respectively again an edge is identified by its two incident
vertices setedge does not permit the user to set the weight to be 0 because this
value is used to indicate a nonexistent edge nor are negative edge weights permitted functions getmark and setmark get and set respectively a requested
value in the mark array described below for vertex v
nearly every graph algorithm presented in this chapter will require visits to all
neighbors of a given vertex two methods are provided to support this they work
in a manner similar to linked list access functions function first takes as input
a vertex v and returns the edge to the first neighbor for v we assume the neighbor
list is sorted by vertex number function next takes as input vertices v1 and v2
and returns the index for the vertex forming the next edge with v1 after v2 on v1s
edge list function next will return a value of n  v once the end of the edge
list for v1 has been reached the following line appears in many graph algorithms
for w  gfirstv w  gn w  gnextvw

this for loop gets the first neighbor of v then works through the remaining neighbors of v until a value equal to gn is returned signaling that all neighbors
of v have been visited for example first1 in figure 114 would return 0
next1 0 would return 3 next0 3 would return 4 next1 4
would return 5 which is not a vertex in the graph


$$@@$$PAGE: 406
sec 112 graph implementations

387

 graph abstract class this adt assumes that the number
 of vertices is fixed when the graph is created
class graph 
private
void operator const graph 
 protect assignment
graphconst graph 
 protect copy constructor
public
graph 
 default constructor
virtual graph   base destructor
 initialize a graph of n vertices
virtual void initint n 0
 return the number of vertices and edges
virtual int n 0
virtual int e 0
 return vs first neighbor
virtual int firstint v 0
 return vs next neighbor
virtual int nextint v int w 0
 set the weight for an edge
 i j the vertices
 wgt edge weight
virtual void setedgeint v1 int v2 int wght 0
 delete an edge
 i j the vertices
virtual void deledgeint v1 int v2 0
 determine if an edge is in the graph
 i j the vertices
 return true if edge ij has nonzero weight
virtual bool isedgeint i int j 0
 return an edges weight
 i j the vertices
 return the weight of edge ij or zero
virtual int weightint v1 int v2 0
 get and set the mark value for a vertex
 v the vertex
 val the value to set
virtual int getmarkint v 0
virtual void setmarkint v int val 0

figure 115 a graph adt this adt assumes that the number of vertices is
fixed when the graph is created but that edges can be added and removed it also
supports a mark array to aid graph traversal algorithms


$$@@$$PAGE: 407
388

chap 11 graphs

it is reasonably straightforward to implement our graph and edge adts using
either the adjacency list or adjacency matrix the sample implementations presented here do not address the issue of how the graph is actually created the user
of these implementations must add functionality for this purpose perhaps reading
the graph description from a file the graph can be built up by using the setedge
function provided by the adt
figure 116 shows an implementation for the adjacency matrix array mark
stores the information manipulated by the setmark and getmark functions the
edge matrix is implemented as an integer array of size n  n for a graph of n vertices position i j in the matrix stores the weight for edge i j if it exists a
weight of zero for edge i j is used to indicate that no edge connects vertices i
and j
given a vertex v function first locates the position in matrix of the first
edge if any of v by beginning with edge v 0 and scanning through row v until
an edge is found if no edge is incident on v then first returns n
function next locates the edge following edge i j if any by continuing
down the row of vertex i starting at position j  1 looking for an edge if no
such edge exists next returns n functions setedge and deledge adjust the
appropriate value in the array function weight returns the value stored in the
appropriate position in the array
figure 117 presents an implementation of the adjacency list representation for
graphs its main data structure is an array of linked lists one linked list for each
vertex these linked lists store objects of type edge which merely stores the index
for the vertex pointed to by the edge along with the weight of the edge because
the edge class is assumed to be private to the graphl class its data members
have been made public for convenience
 edge class for adjacency list graph representation
class edge 
int vert wt
public
edge  vert  1 wt  1 
edgeint v int w  vert  v wt  w 
int vertex  return vert 
int weight  return wt 


implementation for graphl member functions is straightforward in principle
with the key functions being setedge deledge and weight they simply
start at the beginning of the adjacency list and move along it until the desired vertex
has been found note that isedge checks to see if j is already the current neighbor
in is adjacency list since this will often be true when processing the neighbors of
each vertex in turn


$$@@$$PAGE: 408
sec 112 graph implementations

389

 implementation for the adjacency matrix representation
class graphm  public graph 
private
int numvertex numedge  store number of vertices edges
int matrix
 pointer to adjacency matrix
int mark
 pointer to mark array
public
graphmint numvert
 constructor
 initnumvert 
graphm 
 destructor
delete  mark  return dynamically allocated memory
for int i0 inumvertex i
delete  matrixi
delete  matrix

void initint n   initialize the graph
int i
numvertex  n
numedge  0
mark  new intn
 initialize mark array
for i0 inumvertex i
marki  unvisited
matrix  int new intnumvertex  make matrix
for i0 inumvertex i
matrixi  new intnumvertex
for i0 i numvertex i  initialize to 0 weights
for int j0 jnumvertex j
matrixij  0

int n  return numvertex   number of vertices
int e  return numedge 
 number of edges
 return first neighbor of v
int firstint v 
for int i0 inumvertex i
if matrixvi  0 return i
return numvertex
 return n if none

 return vs next neighbor after w
int nextint v int w 
forint iw1 inumvertex i
if matrixvi  0
return i
return numvertex
 return n if none

figure 116 an implementation for the adjacency matrix implementation


$$@@$$PAGE: 409
390

chap 11 graphs

 set edge v1 v2 to wt
void setedgeint v1 int v2 int wt 
assertwt0 illegal weight value
if matrixv1v2  0 numedge
matrixv1v2  wt

void deledgeint v1 int v2   delete edge v1 v2
if matrixv1v2  0 numedge
matrixv1v2  0

bool isedgeint i int j  is i j an edge
 return matrixij  0 
int weightint v1 int v2  return matrixv1v2 
int getmarkint v  return markv 
void setmarkint v int val  markv  val 

figure 116 continued

113

graph traversals

often it is useful to visit the vertices of a graph in some specific order based on the
graphs topology this is known as a graph traversal and is similar in concept to
a tree traversal recall that tree traversals visit every node exactly once in some
specified order such as preorder inorder or postorder multiple tree traversals exist
because various applications require the nodes to be visited in a particular order
for example to print a bsts nodes in ascending order requires an inorder traversal as opposed to some other traversal standard graph traversal orders also exist
each is appropriate for solving certain problems for example many problems in
artificial intelligence programming are modeled using graphs the problem domain
may consist of a large collection of states with connections between various pairs
of states solving the problem may require getting from a specified start state to a
specified goal state by moving between states only through the connections typically the start and goal states are not directly connected to solve this problem the
vertices of the graph must be searched in some organized manner
graph traversal algorithms typically begin with a start vertex and attempt to
visit the remaining vertices from there graph traversals must deal with a number
of troublesome cases first it may not be possible to reach all vertices from the
start vertex this occurs when the graph is not connected second the graph may
contain cycles and we must make sure that cycles do not cause the algorithm to go
into an infinite loop


$$@@$$PAGE: 410
sec 113 graph traversals

391

class graphl  public graph 
private
listedge vertex
 list headers
int numvertex numedge
 number of vertices edges
int mark
 pointer to mark array
public
graphlint numvert
 initnumvert 
graphl 
 destructor
delete  mark  return dynamically allocated memory
for int i0 inumvertex i delete  vertexi
delete  vertex

void initint n 
int i
numvertex  n
numedge  0
mark  new intn  initialize mark array
for i0 inumvertex i marki  unvisited
 create and initialize adjacency lists
vertex  listedge new listedgenumvertex
for i0 inumvertex i
vertexi  new llistedge

int n  return numvertex   number of vertices
int e  return numedge 
 number of edges
int firstint v   return first neighbor of v
if vertexvlength  0
return numvertex
 no neighbor
vertexvmovetostart
edge it  vertexvgetvalue
return itvertex

 get vs next neighbor after w
int nextint v int w 
edge it
if isedgev w 
if vertexvcurrpos1  vertexvlength 
vertexvnext
it  vertexvgetvalue
return itvertex


return n  no neighbor

figure 117 an implementation for the adjacency list


$$@@$$PAGE: 411
392

chap 11 graphs

 set edge i j to weight
void setedgeint i int j int weight 
assertweight0 may not set weight to 0
edge curredgej weight
if isedgei j   edge already exists in graph
vertexiremove
vertexiinsertcurredge

else   keep neighbors sorted by vertex index
numedge
for verteximovetostart
vertexicurrpos  vertexilength
vertexinext 
edge temp  vertexigetvalue
if tempvertex  j break

vertexiinsertcurredge


void deledgeint i int j 
if isedgeij 
vertexiremove
numedge



 delete edge i j

bool isedgeint i int j   is ij an edge
edge it
for verteximovetostart
vertexicurrpos  vertexilength
vertexinext 
 check whole list
edge temp  vertexigetvalue
if tempvertex  j return true

return false

int weightint i int j   return weight of i j
edge curr
if isedgei j 
curr  vertexigetvalue
return currweight

else return 0

int getmarkint v  return markv 
void setmarkint v int val  markv  val 

figure 117 continued


$$@@$$PAGE: 412
sec 113 graph traversals

393

graph traversal algorithms can solve both of these problems by maintaining a
mark bit for each vertex on the graph at the beginning of the algorithm the mark
bit for all vertices is cleared the mark bit for a vertex is set when the vertex is first
visited during the traversal if a marked vertex is encountered during traversal it is
not visited a second time this keeps the program from going into an infinite loop
when it encounters a cycle
once the traversal algorithm completes we can check to see if all vertices have
been processed by checking the mark bit array if not all vertices are marked we
can continue the traversal from another unmarked vertex note that this process
works regardless of whether the graph is directed or undirected to ensure visiting
all vertices graphtraverse could be called as follows on a graph g
void graphtraversegraph g 
int v
for v0 vgn v
gsetmarkv unvisited  initialize mark bits
for v0 vgn v
if ggetmarkv  unvisited
dotraverseg v


function dotraverse might be implemented by using one of the graph traversals described in this section
1131

depthfirst search

the first method of organized graph traversal is called depthfirst search dfs
whenever a vertex v is visited during the search dfs will recursively visit all
of vs unvisited neighbors equivalently dfs will add all edges leading out of v
to a stack the next vertex to be visited is determined by popping the stack and
following that edge the effect is to follow one branch through the graph to its
conclusion then it will back up and follow another branch and so on the dfs
process can be used to define a depthfirst search tree this tree is composed of
the edges that were followed to any new unvisited vertex during the traversal and
leaves out the edges that lead to already visited vertices dfs can be applied to
directed or undirected graphs here is an implementation for the dfs algorithm
void dfsgraph g int v   depth first search
previsitg v
 take appropriate action
gsetmarkv visited
for int wgfirstv wgn w  gnextvw
if ggetmarkw  unvisited
dfsg w
postvisitg v
 take appropriate action


this implementation contains calls to functions previsit and postvisit
these functions specify what activity should take place during the search just


$$@@$$PAGE: 413
394

chap 11 graphs

a

b

a

b

c

c

d

d
f

e

f
e

a

b

figure 118 a a graph b the depthfirst search tree for the graph when
starting at vertex a

as a preorder tree traversal requires action before the subtrees are visited some
graph traversals require that a vertex be processed before ones further along in the
dfs alternatively some applications require activity after the remaining vertices
are processed hence the call to function postvisit this would be a natural
opportunity to make use of the visitor design pattern described in section 132
figure 118 shows a graph and its corresponding depthfirst search tree figure 119 illustrates the dfs process for the graph of figure 118a
dfs processes each edge once in a directed graph in an undirected graph
dfs processes each edge from both directions each vertex must be visited but
only once so the total cost is v  e
1132

breadthfirst search

our second graph traversal algorithm is known as a breadthfirst search bfs
bfs examines all vertices connected to the start vertex before visiting vertices further away bfs is implemented similarly to dfs except that a queue replaces
the recursion stack note that if the graph is a tree and the start vertex is at the
root bfs is equivalent to visiting vertices level by level from top to bottom figure 1110 provides an implementation for the bfs algorithm figure 1111 shows
a graph and the corresponding breadthfirst search tree figure 1112 illustrates the
bfs process for the graph of figure 1111a
1133

topological sort

assume that we need to schedule a series of tasks such as classes or construction
jobs where we cannot start one task until after its prerequisites are completed we
wish to organize the tasks into a linear order that allows us to complete them one


$$@@$$PAGE: 414
395

sec 113 graph traversals

a call dfs on a

mark a
c process a c
print a c and
a
call dfs on c

mark c
process c a
c process c b
print c b and
a
call dfs on b
b

d
f

f

mark b
b
process b c
c process b f
print b f and
a
call dfs on f

mark f
process f b
b
process f c
c process f d
print f d and
a
call dfs on d

f
b

mark d
c process d c
process d f
a
pop d

e
f

f

b

b

c process f e
print f e and
a
call dfs on e

mark e
c process e a
process e f
a
pop e

c
a

done with b
pop b

a

continue with c
process c e
process c f
pop c

b
c
a

done with f
pop f

continue with a
process a e
pop a
dfs complete

figure 119 a detailed illustration of the dfs process for the graph of figure 118a starting at vertex a the steps leading to each change in the recursion
stack are described


$$@@$$PAGE: 415
396

chap 11 graphs

void bfsgraph g int start queueint q 
int v w
qenqueuestart
 initialize q
gsetmarkstart visited
while qlength  0   process all vertices on q
v  qdequeue
previsitg v
 take appropriate action
for wgfirstv wgn w  gnextvw
if ggetmarkw  unvisited 
gsetmarkw visited
qenqueuew



figure 1110 implementation for the breadthfirst graph traversal algorithm

a

b

a

b

c

c

d

d
f

e

f
e

a

b

figure 1111 a a graph b the breadthfirst search tree for the graph when
starting at vertex a

at a time without violating any prerequisites we can model the problem using a
dag the graph is directed because one task is a prerequisite of another  the
vertices have a directed relationship it is acyclic because a cycle would indicate
a conflicting series of prerequisites that could not be completed without violating
at least one prerequisite the process of laying out the vertices of a dag in a
linear order to meet the prerequisite rules is called a topological sort figure 1114
illustrates the problem an acceptable topological sort for this example is j1 j2
j3 j4 j5 j6 j7
a topological sort may be found by performing a dfs on the graph when a
vertex is visited no action is taken ie function previsit does nothing when
the recursion pops back to that vertex function postvisit prints the vertex this
yields a topological sort in reverse order it does not matter where the sort starts as
long as all vertices are visited in the end figure 1113 shows an implementation
for the dfsbased algorithm


$$@@$$PAGE: 416
397

sec 113 graph traversals

a

c

initial call to bfs on a
mark a and put on the queue

e

b

d

f

dequeue c
process c a ignore
process c b
mark and enqueue b print c b
process c d
mark and enqueue d print c d
process c f
mark and enqueue f print c f
d

f

dequeue b
process b c ignore
process b f ignore

e

dequeue a
process a c
mark and enqueue c print a c
process a e
mark and enqueue e printa e
b

d

f

dequeue e
process e a ignore
process e f ignore

f
dequeue d
process d c ignore
process d f ignore

dequeue f
process f b ignore
process f c ignore
process f d ignore
bfs is complete
figure 1112 a detailed illustration of the bfs process for the graph of figure 1111a starting at vertex a the steps leading to each change in the queue
are described


$$@@$$PAGE: 417
398

chap 11 graphs

void topsortgraph g 
 topological sort recursive
int i
for i0 ign i  initialize mark array
gsetmarki unvisited
for i0 ign i  process all vertices
if ggetmarki  unvisited
tophelpg i
 call recursive helper function

void tophelpgraph g int v   process vertex v
gsetmarkv visited
for int wgfirstv wgn w  gnextvw
if ggetmarkw  unvisited
tophelpg w
printoutv
 postvisit for vertex v

figure 1113 implementation for the recursive topological sort

j6
j1

j2

j3

j5

j7

j4

figure 1114 an example graph for topological sort seven tasks have dependencies as shown by the directed graph

using this algorithm starting at j1 and visiting adjacent neighbors in alphabetic
order vertices of the graph in figure 1114 are printed out in the order j7 j5 j4
j6 j2 j3 j1 reversing this yields the topological sort j1 j3 j2 j6 j4 j5 j7
we can implement topological sort using a queue instead of recursion as follows first visit all edges counting the number of edges that lead to each vertex
ie count the number of prerequisites for each vertex all vertices with no prerequisites are placed on the queue we then begin processing the queue when
vertex v is taken off of the queue it is printed and all neighbors of v that is all
vertices that have v as a prerequisite have their counts decremented by one place
on the queue any neighbor whose count becomes zero if the queue becomes empty
without printing all of the vertices then the graph contains a cycle ie there is no
possible ordering for the tasks that does not violate some prerequisite the printed
order for the vertices of the graph in figure 1114 using the queue version of topological sort is j1 j2 j3 j6 j4 j5 j7 figure 1115 shows an implementation for
the algorithm


$$@@$$PAGE: 418
sec 114 shortestpaths problems

399

 topological sort queue
void topsortgraph g queueint q 
int countgn
int v w
for v0 vgn v countv  0  initialize
for v0 vgn v
 process every edge
for wgfirstv wgn w  gnextvw
countw
 add to vs prereq count
for v0 vgn v
 initialize queue
if countv  0
 vertex has no prerequisites
qenqueuev
while qlength  0   process the vertices
v  qdequeue
printoutv
 previsit for v
for wgfirstv wgn w  gnextvw 
countw
 one less prerequisite
if countw  0
 this vertex is now free
qenqueuew



figure 1115 a queuebased topological sort algorithm

114

shortestpaths problems

on a road map a road connecting two towns is typically labeled with its distance
we can model a road network as a directed graph whose edges are labeled with
real numbers these numbers represent the distance or other cost metric such as
travel time between two vertices these labels may be called weights costs or
distances depending on the application given such a graph a typical problem
is to find the total length of the shortest path between two specified vertices this
is not a trivial problem because the shortest path may not be along the edge if
any connecting two vertices but rather may be along a path involving one or more
intermediate vertices for example in figure 1116 the cost of the path from a to
b to d is 15 the cost of the edge directly from a to d is 20 the cost of the path
from a to c to b to d is 10 thus the shortest path from a to d is 10 not along
the edge connecting a to d we use the notation da d  10 to indicate that the
shortest distance from a to d is 10 in figure 1116 there is no path from e to b so
we set de b   we define wa d  20 to be the weight of edge a d that
is the weight of the direct connection from a to d because there is no edge from
e to b we b   note that wd a   because the graph of figure 1116
is directed we assume that all weights are positive


$$@@$$PAGE: 419
400

chap 11 graphs

b

5

10

d

20

a
11

2

3
c

15

e

figure 1116 example graph for shortestpath definitions

1141

singlesource shortest paths

this section presents an algorithm to solve the singlesource shortestpaths problem given vertex s in graph g find a shortest path from s to every other vertex
in g we might want only the shortest path between two vertices s and t however in the worst case while finding the shortest path from s to t we might find
the shortest paths from s to every other vertex as well so there is no better algorithm in the worst case for finding the shortest path to a single vertex than to find
shortest paths to all vertices the algorithm described here will only compute the
distance to every such vertex rather than recording the actual path recording the
path requires modifications to the algorithm that are left as an exercise
computer networks provide an application for the singlesource shortestpaths
problem the goal is to find the cheapest way for one computer to broadcast a
message to all other computers on the network the network can be modeled by a
graph with edge weights indicating time or cost to send a message to a neighboring
computer
for unweighted graphs or whenever all edges have the same cost the singlesource shortest paths can be found using a simple breadthfirst search when
weights are added bfs will not give the correct answer
one approach to solving this problem when the edges have differing weights
might be to process the vertices in a fixed order label the vertices v0 to vn1  with
s  v0  when processing vertex v1  we take the edge connecting v0 and v1  when
processing v2  we consider the shortest distance from v0 to v2 and compare that to
the shortest distance from v0 to v1 to v2  when processing vertex vi  we consider
the shortest path for vertices v0 through vi1 that have already been processed
unfortunately the true shortest path to vi might go through vertex vj for j  i
such a path will not be considered by this algorithm however the problem would
not occur if we process the vertices in order of distance from s assume that we
have processed in order of distance from s to the first i  1 vertices that are closest
to s call this set of vertices s we are now about to process the ith closest vertex


$$@@$$PAGE: 420
sec 114 shortestpaths problems

401

 compute shortest path distances from s
 return these distances in d
void dijkstragraph g int d int s 
int i v w
for int i0 ign i
 initialize
di  infinity
d0  0
for i0 ign i 
 process the vertices
v  minvertexg d
if dv  infinity return  unreachable vertices
gsetmarkv visited
for wgfirstv wgn w  gnextvw
if dw  dv  gweightv w
dw  dv  gweightv w


figure 1117 an implementation for dijkstras algorithm

call it x a shortest path from s to x must have its nexttolast vertex in s thus
ds x  min ds u  wu x
us
in other words the shortest path from s to x is the minimum over all paths that go
from s to u then have an edge from u to x where u is some vertex in s
this solution is usually referred to as dijkstras algorithm it works by maintaining a distance estimate dx for all vertices x in v the elements of d are initialized to the value infinite vertices are processed in order of distance from
s whenever a vertex v is processed dx is updated for every neighbor x of v
figure 1117 shows an implementation for dijkstras algorithm at the end array d
will contain the shortest distance values
there are two reasonable solutions to the key issue of finding the unvisited
vertex with minimum distance value during each pass through the main for loop
the first method is simply to scan through the list of v vertices searching for the
minimum value as follows
int minvertexgraph g int d   find min cost vertex
int i v  1
 initialize v to some unvisited vertex
for i0 ign i
if ggetmarki  unvisited  v  i break 
for i ign i  now find smallest d value
if ggetmarki  unvisited  di  dv
v  i
return v


because this scan is done v times and because each edge requires a constanttime update to d the total cost for this approach is v2  e  v2 
because e is in ov2 


$$@@$$PAGE: 421
402

chap 11 graphs

the second method is to store unprocessed vertices in a minheap ordered by
distance values the nextclosest vertex can be found in the heap in log v
time every time we modify dx we could reorder x in the heap by deleting
and reinserting it this is an example of a priority queue with priority update as
described in section 55 to implement true priority updating we would need to
store with each vertex its array index within the heap a simpler approach is to
add the new smaller distance value for a given vertex as a new record in the heap
the smallest value for a given vertex currently in the heap will be found first and
greater distance values found later will be ignored because the vertex will already
be marked as visited the only disadvantage to repeatedly inserting distance
values is that it will raise the number of elements in the heap from v to e
in the worst case the time complexity is v  e log e because for each
edge we must reorder the heap because the objects stored on the heap need to
know both their vertex number and their distance we create a simple class for the
purpose called dijkelem as follows dijkelem is quite similar to the edge
class used by the adjacency list representation
class dijkelem 
public
int vertex distance
dijkelem  vertex  1 distance  1 
dijkelemint v int d  vertex  v distance  d 


figure 1118 shows an implementation for dijkstras algorithm using the priority queue
using minvertex to scan the vertex list for the minimum value is more efficient when the graph is dense that is when e approaches v2  using a priority queue is more efficient when the graph is sparse because its cost is v 
e log e however when the graph is dense this cost can become as great as
v2 log e  v 2 log v 
figure 1119 illustrates dijkstras algorithm the start vertex is a all vertices
except a have an initial value of  after processing vertex a its neighbors have
their d estimates updated to be the direct distance from a after processing c
the closest vertex to a vertices b and e are updated to reflect the shortest path
through c the remaining vertices are processed in order b d and e

115

minimumcost spanning trees

the minimumcost spanning tree mst problem takes as input a connected
undirected graph g where each edge has a distance or weight measure attached
the mst is the graph containing the vertices of g along with the subset of gs
edges that 1 has minimum total cost as measured by summing the values for all of


$$@@$$PAGE: 422
403

sec 115 minimumcost spanning trees

 dijkstras shortest paths algorithm with priority queue
void dijkstragraph g int d int s 
int i v w
 v is current vertex
dijkelem temp
dijkelem ege
 heap array with lots of space
tempdistance  0 tempvertex  s
e0  temp
 initialize heap array
heapdijkelem ddcomp he 1 ge  create heap
for int i0 ign i
 initialize
di  infinity
d0  0
for i0 ign i 
 now get distances
do 
if hsize  0 return  nothing to remove
temp  hremovefirst
v  tempvertex
 while ggetmarkv  visited
gsetmarkv visited
if dv  infinity return
 unreachable vertices
for wgfirstv wgn w  gnextvw
if dw  dv  gweightv w   update d
dw  dv  gweightv w
tempdistance  dw tempvertex  w
hinserttemp
 insert new distance in heap



figure 1118 an implementation for dijkstras algorithm using a priority queue

initial
process a
process c
process b
process d
process e

a b c d e
0    
0 10 3 20 
0 5 3 20 18
0 5 3 10 18
0 5 3 10 18
0 5 3 10 18

figure 1119 a listing for the progress of dijkstras algorithm operating on the
graph of figure 1116 the start vertex is a


$$@@$$PAGE: 423
404

chap 11 graphs

a

b
7

5
c

9

1

6

2
d

2
f

e

1

figure 1120 a graph and its mst all edges appear in the original graph
those edges drawn with heavy lines indicate the subset making up the mst note
that edge c f could be replaced with edge d f to form a different mst with
equal cost

the edges in the subset and 2 keeps the vertices connected applications where a
solution to this problem is useful include soldering the shortest set of wires needed
to connect a set of terminals on a circuit board and connecting a set of cities by
telephone lines in such a way as to require the least amount of cable
the mst contains no cycles if a proposed mst did have a cycle a cheaper
mst could be had by removing any one of the edges in the cycle thus the mst
is a free tree with v  1 edges the name minimumcost spanning tree comes
from the fact that the required set of edges forms a tree it spans the vertices ie
it connects them together and it has minimum cost figure 1120 shows the mst
for an example graph
1151

prims algorithm

the first of our two algorithms for finding msts is commonly referred to as prims
algorithm prims algorithm is very simple start with any vertex n in the graph
setting the mst to be n initially pick the leastcost edge connected to n this
edge connects n to another vertex call this m add vertex m and edge n m to
the mst next pick the leastcost edge coming from either n or m to any other
vertex in the graph add this edge and the new vertex it reaches to the mst this
process continues at each step expanding the mst by selecting the leastcost edge
from a vertex currently in the mst to a vertex not currently in the mst
prims algorithm is quite similar to dijkstras algorithm for finding the singlesource shortest paths the primary difference is that we are seeking not the next
closest vertex to the start vertex but rather the next closest vertex to any vertex
currently in the mst thus we replace the lines
if dw  dv  gweightv w
dw  dv  gweightv w


$$@@$$PAGE: 424
sec 115 minimumcost spanning trees

405

void primgraph g int d int s   prims mst algorithm
int vgn
 store closest vertex
int i w
for int i0 ign i
 initialize
di  infinity
d0  0
for i0 ign i 
 process the vertices
int v  minvertexg d
gsetmarkv visited
if v  s
addedgetomstvv v
 add edge to mst
if dv  infinity return
 unreachable vertices
for wgfirstv wgn w  gnextvw
if dw  gweightvw 
dw  gweightvw
 update distance
vw  v
 where it came from



figure 1121 an implementation for prims algorithm

in djikstras algorithm with the lines
if dw  gweightv w
dw  gweightv w

in prims algorithm
figure 1121 shows an implementation for prims algorithm that searches the
distance matrix for the next closest vertex for each vertex i when i is processed
by prims algorithm an edge going to i is added to the mst that we are building
array vi stores the previously visited vertex that is closest to vertex i this
information lets us know which edge goes into the mst when vertex i is processed
the implementation of figure 1121 also contains calls to addedgetomst to
indicate which edges are actually added to the mst
alternatively we can implement prims algorithm using a priority queue to find
the next closest vertex as shown in figure 1122 as with the priority queue version
of dijkstras algorithm the heaps elem type stores a dijkelem object
prims algorithm is an example of a greedy algorithm at each step in the
for loop we select the leastcost edge that connects some marked vertex to some
unmarked vertex the algorithm does not otherwise check that the mst really
should include this leastcost edge this leads to an important question does
prims algorithm work correctly clearly it generates a spanning tree because
each pass through the for loop adds one edge and one unmarked vertex to the
spanning tree until all vertices have been added but does this tree have minimum
cost
theorem 111 prims algorithm produces a minimumcost spanning tree


$$@@$$PAGE: 425
406

chap 11 graphs

 prims mst algorithm priority queue version
void primgraph g int d int s 
int i v w
 v is current vertex
int vgn
 vi stores is closest neighbor
dijkelem temp
dijkelem ege
 heap array with lots of space
tempdistance  0 tempvertex  s
e0  temp
 initialize heap array
heapdijkelem ddcomp he 1 ge  create heap
for int i0 ign i
 initialize
di  infinity
d0  0
for i0 ign i 
 now build mst
do 
ifhsize  0 return  nothing to remove
temp  hremovefirst
v  tempvertex
 while ggetmarkv  visited
gsetmarkv visited
if v  s addedgetomstvv v  add edge to mst
if dv  infinity return
 ureachable vertex
for wgfirstv wgn w  gnextvw
if dw  gweightv w 
 update d
dw  gweightv w
vw  v
 update who it came from
tempdistance  dw tempvertex  w
hinserttemp  insert new distance in heap



figure 1122 an implementation of prims algorithm using a priority queue

proof we will use a proof by contradiction let g  v e be a graph for which
prims algorithm does not generate an mst define an ordering on the vertices
according to the order in which they were added by prims algorithm to the mst
v0  v1   vn1  let edge ei connect vx  vi  for some x  i and i  1 let ej be the
lowest numbered first edge added by prims algorithm such that the set of edges
selected so far cannot be extended to form an mst for g in other words ej is the
first edge where prims algorithm went wrong let t be the true mst call vp
p  j the vertex connected by edge ej  that is ej  vp  vj 
because t is a tree there exists some path in t connecting vp and vj  there
must be some edge e0 in this path connecting vertices vu and vw  with u  j and
w  j because ej is not part of t adding edge ej to t forms a cycle edge e0 must
be of lower cost than edge ej  because prims algorithm did not generate an mst
this situation is illustrated in figure 1123 however prims algorithm would have
selected the leastcost edge available it would have selected e0  not ej  thus it is a
contradiction that prims algorithm would have selected the wrong edge and thus
prims algorithm must be correct
2


$$@@$$PAGE: 426
407

sec 115 minimumcost spanning trees

marked
vertices vi  i  j
vu

unmarked
vertices vi  i  j
correct edge
e

vp
ej
prims edge

vu

vj

figure 1123 prims mst algorithm proof the left oval contains that portion of
the graph where prims mst and the true mst t agree the right oval contains
the rest of the graph the two portions of the graph are connected by at least
edges ej selected by prims algorithm to be in the mst and e0 the correct
edge to be placed in the mst note that the path from vw to vj cannot include
any marked vertex vi  i  j because to do so would form a cycle

example 113 for the graph of figure 1120 assume that we begin by
marking vertex a from a the leastcost edge leads to vertex c vertex c
and edge a c are added to the mst at this point our candidate edges
connecting the mst vertices a and c with the rest of the graph are a e
c b c d and c f from these choices the leastcost edge from the
mst is c d so we add vertex d to the mst for the next iteration our
edge choices are a e c b c f and d f because edges c f
and d f happen to have equal cost it is an arbitrary decision as to which
gets selected say we pick c f the next step marks vertex e and adds
edge f e to the mst following in this manner vertex b through edge
c b is marked at this point the algorithm terminates

1152

kruskals algorithm

our next mst algorithm is commonly referred to as kruskals algorithm kruskals
algorithm is also a simple greedy algorithm first partition the set of vertices into
v equivalence classes see section 62 each consisting of one vertex then process the edges in order of weight an edge is added to the mst and two equivalence classes combined if the edge connects two vertices in different equivalence
classes this process is repeated until only one equivalence class remains


$$@@$$PAGE: 427
408

chap 11 graphs

example 114 figure 1124 shows the first three steps of kruskals algorithm for the graph of figure 1120 edge c d has the least cost and
because c and d are currently in separate msts they are combined we
next select edge e f to process and combine these vertices into a single
mst the third edge we process is c f which causes the mst containing vertices c and d to merge with the mst containing vertices e and f
the next edge to process is d f but because vertices d and f are currently in the same mst this edge is rejected the algorithm will continue
on to accept edges b c and a c into the mst
the edges can be processed in order of weight by using a minheap this is
generally faster than sorting the edges first because in practice we need only visit
a small fraction of the edges before completing the mst this is an example of
finding only a few smallest elements in a list as discussed in section 76
the only tricky part to this algorithm is determining if two vertices belong to
the same equivalence class fortunately the ideal algorithm is available for the
purpose  the unionfind algorithm based on the parent pointer representation
for trees described in section 62 figure 1125 shows an implementation for the
algorithm class kruskalelem is used to store the edges on the minheap
kruskals algorithm is dominated by the time required to process the edges
the differ and union functions are nearly constant in time if path compression
and weighted union is used thus the total cost of the algorithm is e log e
in the worst case when nearly all edges must be processed before all the edges of
the spanning tree are found and the algorithm can stop more often the edges of the
spanning tree are the shorter onesand only about v edges must be processed if
so the cost is often close to v log e in the average case

116

further reading

many interesting properties of graphs can be investigated by playing with the programs in the stanford graphbase this is a collection of benchmark databases and
graph processing programs the stanford graphbase is documented in knu94

117

exercises

111 prove by induction that a graph with n vertices has at most nn12 edges
112 prove the following implications regarding free trees
a if an undirected graph is connected and has no simple cycles then
the graph has v  1 edges


$$@@$$PAGE: 428
409

sec 117 exercises

initial

a

b

step 1

a

b

c

d

e

e

f

f

c
1
process edge c d
d
c
step 2

a

b
1

f
e

1

process edge e f
d
c
step 3

a

b
1

2

process edge c f
d
f
e

1

figure 1124 illustration of the first three steps of kruskals mst algorithm as
applied to the graph of figure 1120

b if an undirected graph has v  1 edges and no cycles then the
graph is connected
113

a draw the adjacency matrix representation for the graph of figure 1126
b draw the adjacency list representation for the same graph
c if a pointer requires four bytes a vertex label requires two bytes and
an edge weight requires two bytes which representation requires more
space for this graph
d if a pointer requires four bytes a vertex label requires one byte and
an edge weight requires two bytes which representation requires more
space for this graph
114 show the dfs tree for the graph of figure 1126 starting at vertex 1
115 write a pseudocode algorithm to create a dfs tree for an undirected connected graph starting at a specified vertex v


$$@@$$PAGE: 429
410
class kruskelem
public
int from to
kruskelem 
kruskelemint
 from  f


chap 11 graphs



 an element for the heap

distance  the edge being stored
from  1 to  1 distance  1 
f int t int d
to  t distance  d 

void kruskelgraph g 
 kruskals mst algorithm
parptrtree agn
 equivalence class array
kruskelem ege
 array of edges for minheap
int i
int edgecnt  0
for i0 ign i  put the edges on the array
for int wgfirsti wgn w  gnextiw 
eedgecntdistance  gweighti w
eedgecntfrom  i
eedgecntto  w

 heapify the edges
heapkruskelem comp he edgecnt edgecnt
int nummst  gn
 initially n equiv classes
for i0 nummst1 i   combine equiv classes
kruskelem temp
temp  hremovefirst  get next cheapest edge
int v  tempfrom int u  tempto
if adifferv u   if in different equiv classes
aunionv u
 combine equiv classes
addedgetomsttempfrom tempto  add edge to mst
nummst
 one less mst



figure 1125 an implementation for kruskals algorithm

116 show the bfs tree for the graph of figure 1126 starting at vertex 1
117 write a pseudocode algorithm to create a bfs tree for an undirected connected graph starting at a specified vertex v
118 the bfs topological sort algorithm can report the existence of a cycle if one
is encountered modify this algorithm to print the vertices possibly appearing
in cycles if there are any cycles
119 explain why in the worst case dijkstras algorithm is asymptotically as
efficient as any algorithm for finding the shortest path from some vertex i to
another vertex j
1110 show the shortest paths generated by running dijkstras shortestpaths algorithm on the graph of figure 1126 beginning at vertex 4 show the d
values as each vertex is processed as in figure 1119
1111 modify the algorithm for singlesource shortest paths to actually store and
return the shortest paths rather than just compute the distances


$$@@$$PAGE: 430
411

sec 117 exercises

2

5

10

4

20

1

10

3
6

2

11
3

3
15

5

figure 1126 example graph for chapter 11 exercises

1112 the root of a dag is a vertex r such that every vertex of the dag can be
reached by a directed path from r write an algorithm that takes a directed
graph as input and determines the root if there is one for the graph the
running time of your algorithm should be v  e
1113 write an algorithm to find the longest path in a dag where the length of
the path is measured by the number of edges that it contains what is the
asymptotic complexity of your algorithm
1114 write an algorithm to determine whether a directed graph of v vertices
contains a cycle your algorithm should run in v  e time
1115 write an algorithm to determine whether an undirected graph of v vertices
contains a cycle your algorithm should run in v time
1116 the singledestination shortestpaths problem for a directed graph is to find
the shortest path from every vertex to a specified vertex v write an algorithm
to solve the singledestination shortestpaths problem
1117 list the order in which the edges of the graph in figure 1126 are visited
when running prims mst algorithm starting at vertex 3 show the final
mst
1118 list the order in which the edges of the graph in figure 1126 are visited
when running kruskals mst algorithm each time an edge is added to the
mst show the result on the equivalence array eg show the array as in
figure 67
1119 write an algorithm to find a maximum cost spanning tree that is the spanning tree with highest possible cost
1120 when can prims and kruskals algorithms yield different msts
1121 prove that if the costs for the edges of graph g are distinct then only one
mst exists for g
1122 does either prims or kruskals algorithm work if there are negative edge
weights
1123 consider the collection of edges selected by dijkstras algorithm as the shortest paths to the graphs vertices from the start vertex do these edges form


$$@@$$PAGE: 431
412

chap 11 graphs

a spanning tree not necessarily of minimum cost do these edges form an
mst explain why or why not
1124 prove that a tree is a bipartite graph
1125 prove that any tree ie a connected undirected graph with no cycles can
be twocolored a graph can be two colored if every vertex can be assigned
one of two colors such that no adjacent vertices have the same color
1126 write an algorithm that determines if an arbitrary undirected graph is a bipartite graph if the graph is bipartite then your algorithm should also identify
the vertices as to which of the two partitions each belongs to

118

projects

111 design a format for storing graphs in files then implement two functions
one to read a graph from a file and the other to write a graph to a file test
your functions by implementing a complete mst program that reads an undirected graph in from a file constructs the mst and then writes to a second
file the graph representing the mst
112 an undirected graph need not explicitly store two separate directed edges to
represent a single undirected edge an alternative would be to store only a
single undirected edge i j to connect vertices i and j however what if the
user asks for edge j i we can solve this problem by consistently storing
the edge such that the lesser of i and j always comes first thus if we have
an edge connecting vertices 5 and 3 requests for edge 5 3 and 3 5 both
map to 3 5 because 3  5
looking at the adjacency matrix we notice that only the lower triangle of
the array is used thus we could cut the space required by the adjacency
matrix from v2 positions to vv  12 positions read section 122 on
triangular matrices the reimplement the adjacency matrix representation
of figure 116 to implement undirected graphs using a triangular array
113 while the underlying implementation whether adjacency matrix or adjacency list is hidden behind the graph adt these two implementations can
have an impact on the efficiency of the resulting program for dijkstras
shortest paths algorithm two different implementations were given in section 1141 that provide different ways for determining the next closest vertex
at each iteration of the algorithm the relative costs of these two variants
depend on who sparse or dense the graph is they might also depend on
whether the graph is implemented using an adjacency list or adjacency matrix
design and implement a study to compare the effects on performance for
three variables i the two graph representations adjacency list and adjacency matrix ii the two implementations for djikstras shortest paths algorithm searching the table of vertex distances or using a priority queue to


$$@@$$PAGE: 432
sec 118 projects

413

track the distances and iii sparse versus dense graphs be sure to test your
implementations on a variety of graphs that are sufficiently large to generate
meaningful times
114 the example implementations for dfs and bfs show calls to functions
previsit and postvisit reimplement the bfs and dfs functions
to make use of the visitor design pattern to handle the prepost visit functionality
115 write a program to label the connected components for an undirected graph
in other words all vertices of the first component are given the first components label all vertices of the second component are given the second
components label and so on your algorithm should work by defining any
two vertices connected by an edge to be members of the same equivalence
class once all of the edges have been processed all vertices in a given equivalence class will be connected use the unionfind implementation from
section 62 to implement equivalence classes


$$@@$$PAGE: 433

$$@@$$PAGE: 434
12
lists and arrays revisited

simple lists and arrays are the right tools for the many applications other situations require support for operations that cannot be implemented efficiently by the
standard list representations of chapter 4 this chapter presents a range of topics
whose unifying thread is that the data structures included are all list or arraylike
these structures overcome some of the problems of simple linked list and contiguous array representations this chapter also seeks to reinforce the concept of
logical representation versus physical implementation as some of the list implementations have quite different organizations internally
section 121 describes a series of representations for multilists which are lists
that may contain sublists section 122 discusses representations for implementing
sparse matrices large matrices where most of the elements have zero values section 123 discusses memory management techniques which are essentially a way
of allocating variablelength sections from a large array

121

multilists

recall from chapter 4 that a list is a finite ordered sequence of items of the form
hx0  x1   xn1 i where n  0 we can represent the empty list by null or hi
in chapter 4 we assumed that all list elements had the same data type in this
section we extend the definition of lists to allow elements to be arbitrary in nature
in general list elements are one of two types
1 an atom which is a data record of some type such as a number symbol or
string
2 another list which is called a sublist
a list containing sublists will be written as
hx1 hy1 ha1 a2i y3i hz1 z2i x4i
415


$$@@$$PAGE: 435
416

chap 12 lists and arrays revisited

x1

x4
y1

y3

z1 z2

a1 a2
figure 121 example of a multilist represented by a tree

l2
l1

l3
c

a

d

e

b

figure 122 example of a reentrant multilist the shape of the structure is a
dag all edges point downward

in this example the list has four elements the second element is the sublist
hy1 ha1 a2i y3i and the third is the sublist hz1 z2i the sublist hy1 ha1 a2i y3i
itself contains a sublist if a list l has one or more sublists we call l a multilist lists with no sublists are often referred to as linear lists or chains note that
this definition for multilist fits well with our definition of sets from definition 21
where a sets members can be either primitive elements or sets
we can restrict the sublists of a multilist in various ways depending on whether
the multilist should have the form of a tree a dag or a generic graph a pure list
is a list structure whose graph corresponds to a tree such as in figure 121 in other
words there is exactly one path from the root to any node which is equivalent to
saying that no object may appear more than once in the list in the pure list each
pair of angle brackets corresponds to an internal node of the tree the members of
the list correspond to the children for the node atoms on the list correspond to leaf
nodes
a reentrant list is a list structure whose graph corresponds to a dag nodes
might be accessible from the root by more than one path which is equivalent to
saying that objects including sublists may appear multiple times in the list as long
as no cycles are formed all edges point downward from the node representing a
list or sublist to its elements figure 122 illustrates a reentrant list to write out
this list in bracket notation we can duplicate nodes as necessary thus the bracket
notation for the list of figure 122 could be written
hhha bii hha bi ci hc d ei heii
for convenience we will adopt a convention of allowing sublists and atoms to be
labeled such as l1 whenever a label is repeated the element corresponding to


$$@@$$PAGE: 436
417

sec 121 multilists

l4
l1

l3
l2
b

c

d

a
figure 123 example of a cyclic list the shape of the structure is a directed
graph

that label will be substituted when we write out the list thus the bracket notation
for the list of figure 122 could be written
hhl1  ha bii hl1 l2  ci hl2 d l3  ei hl3ii
a cyclic list is a list structure whose graph corresponds to any directed graph
possibly containing cycles figure 123 illustrates such a list labels are required to
write this in bracket notation here is the bracket notation for the list of figure 123
hl1  hl2  ha l1ii hl2 l3  bi hl3 c di l4  hl4ii
multilists can be implemented in a number of ways most of these should be
familiar from implementations suggested earlier in the book for list tree and graph
data structures
one simple approach is to use a simple array to represent the list this works
well for chains with fixedlength elements equivalent to the simple arraybased list
of chapter 4 we can view nested sublists as variablelength elements to use this
approach we require some indication of the beginning and end of each sublist in
essence we are using a sequential tree implementation as discussed in section 65
this should be no surprise because the pure list is equivalent to a general tree
structure unfortunately as with any sequential representation access to the nth
sublist must be done sequentially from the beginning of the list
because pure lists are equivalent to trees we can also use linked allocation
methods to support direct access to the list of children simple linear lists are
represented by linked lists pure lists can be represented as linked lists with an
additional tag field to indicate whether the node is an atom or a sublist if it is a
sublist the data field points to the first element on the sublist this is illustrated by
figure 124
another approach is to represent all list elements with link nodes storing two
pointer fields except for atoms atoms just contain data this is the system used by
the programming language lisp figure 125 illustrates this representation either
the pointer contains a tag bit to identify what it points to or the object being pointed
to stores a tag bit to identify itself tags distinguish atoms from list nodes this


$$@@$$PAGE: 437
418

chap 12 lists and arrays revisited

root
 x1




 y1

 y3


 a1

 x4
 z1

 z2

 a2

figure 124 linked representation for the pure list of figure 121 the first field
in each link node stores a tag bit if the tag bit stores  then the data field stores
an atom if the tag bit stores  then the data field stores a pointer to a sublist

root

b

c

d

a

figure 125 lisplike linked representation for the cyclic multilist of figure 123 each link node stores two pointers a pointer either points to an atom
or to another link node link nodes are represented by two boxes and atoms by
circles

implementation can easily support reentrant and cyclic lists because nonatoms
can point to any other node

122

matrix representations

sometimes we need to represent a large twodimensional matrix where many of
the elements have a value of zero one example is the lower triangular matrix that
results from solving systems of simultaneous equations a lower triangular matrix
stores zero values at all positions r c such that r  c as shown in figure 126a
thus the upperright triangle of the matrix is always zero another example is
representing undirected graphs in an adjacency matrix see project 112 because
all edges between vertices i and j go in both directions there is no need to store
both instead we can just store one edge going from the higherindexed vertex to


$$@@$$PAGE: 438
419

sec 122 matrix representations

a00
a10
a20
a30

0
a11
a21
a31

0
0
a22
a32

0
0
0
a33

a00
0
0
0

a01
a11
0
0

a

a02
a12
a22
0

a03
a13
a23
a33

b

figure 126 triangular matrices a a lower triangular matrix b an upper
triangular matrix

the lowerindexed vertex in this case only the lower triangle of the matrix can
have nonzero values
we can take advantage of this fact to save space instead of storing nn  12
pieces of information in an n  n array it would save space to use a list of length
nn  12 this is only practical if some means can be found to locate within the
list the element that would correspond to position r c in the original matrix
we will derive an equation to convert position r c to a position in a onedimensional list to store the lower triangular matrix note that row 0 of the matrix
has one nonzero value row 1 has twopnonzero values and so on thus row r
is preceded by r rows with a total of rk1 k  r2  r2 nonzero elements
adding c to reach the cth position in the rth row yields the following equation to
convert position r c in the original matrix to the correct position in the list
matrixr c  listr2  r2  c
a similar equation can be used to convert coordinates in an upper triangular matrix
that is a matrix with zero values at positions r c such that r  c as shown in
figure 126b for an n  n upper triangular matrix the equation to convert from
matrix coordinates to list positions would be
matrixr c  listrn  r2  r2  c
a more difficult situation arises when the vast majority of values stored in an
n  m matrix are zero but there is no restriction on which positions are zero and
which are nonzero this is known as a sparse matrix
one approach to representing a sparse matrix is to concatenate or otherwise
combine the row and column coordinates into a single value and use this as a key
in a hash table thus if we want to know the value of a particular position in the
matrix we search the hash table for the appropriate key if a value for this position
is not found it is assumed to be zero this is an ideal approach when all queries to
the matrix are in terms of access by specified position however if we wish to find
the first nonzero element in a given row or the next nonzero element below the
current one in a given column then the hash table requires us to check sequentially
through all possible positions in some row or column


$$@@$$PAGE: 439
420

chap 12 lists and arrays revisited

another approach is to implement the matrix as an orthogonal list consider
the following sparse matrix
10
45
0
0
40
0
0
0

23
5
0
0
0
0
0
32

0
0
0
0
0
0
0
0

0
93
0
0
0
0
0
12

0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0

19
0
0
0
0
0
0
7

the corresponding orthogonal array is shown in figure 127 here we have a
list of row headers each of which contains a pointer to a list of matrix records
a second list of column headers also contains pointers to matrix records each
nonzero matrix element stores pointers to its nonzero neighbors in the row both
following and preceding it each nonzero element also stores pointers to its nonzero neighbors following and preceding it in the column thus each nonzero
element stores its own value its position within the matrix and four pointers nonzero elements are found by traversing a row or column list note that the first
nonzero element in a given row could be in any column likewise the neighboring
nonzero element in any row or column list could be at any higher row or column
in the array thus each nonzero element must also store its row and column
position explicitly
to find if a particular position in the matrix contains a nonzero element we
traverse the appropriate row or column list for example when looking for the
element at row 7 and column 1 we can traverse the list either for row 7 or for
column 1 when traversing a row or column list if we come to an element with
the correct position then its value is nonzero if we encounter an element with
a higher position then the element we are looking for is not in the sparse matrix
in this case the elements value is zero for example when traversing the list
for row 7 in the matrix of figure 127 we first reach the element at row 7 and
column 1 if this is what we are looking for then the search can stop if we are
looking for the element at row 7 and column 2 then the search proceeds along the
row 7 list to next reach the element at column 3 at this point we know that no
element at row 7 and column 2 is stored in the sparse matrix
insertion and deletion can be performed by working in a similar way to insert
or delete elements within the appropriate row and column lists
each nonzero element stored in the sparse matrix representation takes much
more space than an element stored in a simple n  n matrix when is the sparse
matrix more space efficient than the standard representation to calculate this we
need to determine how much space the standard matrix requires and how much


$$@@$$PAGE: 440
421

sec 122 matrix representations

cols

0

1

3

6

rows

0

1

4

7

00

01

06

10

23

19

03

11

13

45

5

93

04

40

71

73

76

32

12

7

figure 127 the orthogonal list sparse matrix representation

the sparse matrix requires the size of the sparse matrix depends on the number
of nonzero elements we will refer to this value as nnz while the size of the
standard matrix representation does not vary we need to know the relative sizes
of a pointer and a data value for simplicity our calculation will ignore the space
taken up by the row and column header which is not much affected by the number
of elements in the sparse array
as an example assume that a data value a row or column index and a pointer
each require four bytes an n  m matrix requires 4nm bytes the sparse matrix
requires 28 bytes per nonzero element four pointers two array indices and one
data value if we set x to be the percentage of nonzero elements we can solve
for the value of x below which the sparse matrix representation is more space
efficient using the equation
28x  4mn
and solving for x we find that the sparse matrix using this implementation is more
space efficient when x  17 that is when less than about 14 of the elements


$$@@$$PAGE: 441
422

chap 12 lists and arrays revisited

are nonzero different values for the relative sizes of data values pointers or
matrix indices can lead to a different breakeven point for the two implementations
the time required to process a sparse matrix should ideally depend on nnz
when searching for an element the cost is the number of elements preceding the
desired element on its row or column list the cost for operations such as adding
two matrices should be n  m in the worst case when the one matrix stores n
nonzero elements and the other stores m nonzero elements
another representation for sparse matrices is sometimes called the yale representation matlab uses a similar representation with a primary difference being
that the matlab representation uses columnmajor order1 the matlab representation stores the sparse matrix using three lists the first is simply all of the nonzero
element values in columnmajor order the second list stores the start position
within the first list for each column the third list stores the row positions for each
of the corresponding nonzero values in the yale representation the matrix of
figure 127 would appear as
values
column starts
row positions

10 45 40 23 5 32 93 12 19 7
03557777
0140171707

if the matrix has c columns then the total space required will be proportional to
c  2n n z this is good in terms of space it allows fairly quick access to any
column and allows for easy processing of the nonzero values along a column
however it does not do a good job of providing access to the values along a row
and is terrible when values need to be added or removed from the representation
fortunately when doing computations such as adding or multiplying two sparse
matrices the processing of the input matrices and construction of the output matrix
can be done reasonably efficiently

123

memory management

most data structures are designed to store and access objects of uniform size a
typical example would be an integer stored in a list or a queue some applications
require the ability to store variablelength records such as a string of arbitrary
length one solution is to store in the list or queue fixedlength pointers to the
variablelength strings this is fine for data structures stored in main memory
but if the collection of strings is meant to be stored on disk then we might need
to worry about where exactly these strings are stored and even when stored in
main memory something has to figure out where there are available bytes to hold
the string we could easily store variablesize records in a queue or stack where
1

scientific packages tend to prefer columnoriented representations for matrices since this the
dominant access need for the operations to be performed


$$@@$$PAGE: 442
sec 123 memory management

423

 memory manager abstract class
class memmanager 
public
virtual memmanager   base destructor
 store a record and return a handle to it
virtual memhandle insertvoid info int length 0
 get back a copy of a stored record
virtual int getvoid info memhandle h 0
 release the space associated with a record
virtual void releasememhandle h 0

figure 128 a simple adt for a memory manager

the restricted order of insertions and deletions makes this easy to deal with but
in a language like c or java programmers can allocate and deallocate space in
complex ways through use of new where does this space come from this section
discusses memory management techniques for the general problem of handling
space requests of variable size
the basic model for memory management is that we have a large block of
contiguous memory locations which we will call the memory pool periodically
memory requests are issued for some amount of space in the pool the memory
manager has the job of finding a contiguous block of locations of at least the requested size from somewhere within the memory pool honoring such a request
is called a memory allocation the memory manager will typically return some
piece of information that the requester can hold on to so that later it can recover
the record that was just stored by the memory manager this piece of information
is called a handle at some point space that has been requested might no longer
be needed and this space can be returned to the memory manager so that it can be
reused this is called a memory deallocation the memory manager should then
be able to reuse this space to satisfy later memory requests we can define an adt
for the memory manager as shown in figure 128
the user of the memmanager adt provides a pointer in parameter info to
space that holds some record or message to be stored or retrieved this is similar to
the c basic file readwrite methods presented in section 84 the fundamental
idea is that the client gives messages to the memory manager for safe keeping the
memory manager returns a receipt for the message in the form of a memhandle
object of course to be practical a memhandle must be much smaller than the
typical message to be stored the client holds the memhandle object until it
wishes to get the message back
method insert lets the client tell the memory manager the length and contents of the message to be stored this adt assumes that the memory manager will


$$@@$$PAGE: 443
424

chap 12 lists and arrays revisited

figure 129 dynamic storage allocation model memory is made up of a series
of variablesize blocks some allocated and some free in this example shaded
areas represent memory currently allocated and unshaded areas represent unused
memory available for future allocation

remember the length of the message associated with a given handle perhaps in the
handle itself thus method get does not include a length parameter but instead
returns the length of the message actually stored method release allows the
client to tell the memory manager to release the space that stores a given message
when all inserts and releases follow a simple pattern such as last requested
first released stack order or first requested first released queue order memory
management is fairly easy we are concerned here with the general case where
blocks of any size might be requested and released in any order this is known
as dynamic storage allocation one example of dynamic storage allocation is
managing free store for a compilers runtime environment such as the systemlevel
new and delete operations in c  another example is managing main memory
in a multitasking operating system here a program might require a certain amount
of space and the memory manager must keep track of which programs are using
which parts of the main memory yet another example is the file manager for a
disk drive when a disk file is created expanded or deleted the file manager must
allocate or deallocate disk space
a block of memory or disk space managed in this way is sometimes referred to
as a heap the term heap is being used here in a different way than the heap data
structure discussed in section 55 here heap refers to the memory controlled by
a dynamic memory management scheme
in the rest of this section we first study techniques for dynamic memory management we then tackle the issue of what to do when no single block of memory
in the memory pool is large enough to honor a given request
1231

dynamic storage allocation

for the purpose of dynamic storage allocation we view memory as a single array
which after a series of memory requests and releases tends to become broken into
a series of variablesize blocks where some of the blocks are free and some are
reserved or already allocated to store messages the memory manager typically
uses a linked list to keep track of the free blocks called the freelist which is used
for servicing future memory requests figure 129 illustrates the situation that can
arise after a series of memory allocations and deallocations


$$@@$$PAGE: 444
sec 123 memory management

425

small block external fragmentation

unused space in allocated block internal fragmentation
figure 1210 an illustration of internal and external fragmentation the small
white block labeled external fragmentation is too small to satisfy typical memory requests the small grey block labeled internal fragmentation was allocated
as part of the grey block to its left but it does not actually store information

when a memory request is received by the memory manager some block on
the freelist must be found that is large enough to service the request if no such
block is found then the memory manager must resort to a failure policy such as
discussed in section 1232
if there is a request for m words and no block exists of exactly size m then
a larger block must be used instead one possibility in this case is that the entire
block is given away to the memory allocation request this might be desirable
when the size of the block is only slightly larger than the request this is because
saving a tiny block that is too small to be useful for a future memory request might
not be worthwhile alternatively for a free block of size k with k  m up to
k  m space may be retained by the memory manager to form a new free block
while the rest is used to service the request
memory managers can suffer from two types of fragmentation which refers to
unused space that is too small to be useful external fragmentation occurs when
a series of memory requests and releases results in small free blocks internal
fragmentation occurs when more than m words are allocated to a request for m
words wasting free storage this is equivalent to the internal fragmentation that
occurs when files are allocated in multiples of the cluster size the difference
between internal and external fragmentation is illustrated by figure 1210
some memory management schemes sacrifice space to internal fragmentation
to make memory management easier and perhaps reduce external fragmentation
for example external fragmentation does not happen in file management systems
that allocate file space in clusters another example of sacrificing space to internal fragmentation so as to simplify memory management is the buddy method
described later in this section
the process of searching the memory pool for a block large enough to service
the request possibly reserving the remaining space as a free block is referred to as
a sequential fit method


$$@@$$PAGE: 445
426

chap 12 lists and arrays revisited

figure 1211 a doubly linked list of free blocks as seen by the memory manager
shaded areas represent allocated memory unshaded areas are part of the freelist

sequential fit methods
sequentialfit methods attempt to find a good block to service a storage request
the three sequentialfit methods described here assume that the free blocks are
organized into a doubly linked list as illustrated by figure 1211
there are two basic approaches to implementing the freelist the simpler approach is to store the freelist separately from the memory pool in other words
a simple linkedlist implementation such as described in chapter 4 can be used
where each node of the linked list contains a pointer to a single free block in the
memory pool this is fine if there is space available for the linked list itself separate from the memory pool
the second approach to storing the freelist is more complicated but saves space
because the free space is free it can be used by the memory manager to help it do
its job that is the memory manager can temporarily borrow space within the
free blocks to maintain its doubly linked list to do so each unallocated block must
be large enough to hold these pointers in addition it is usually worthwhile to let
the memory manager add a few bytes of space to each reserved block for its own
purposes in other words a request for m bytes of space might result in slightly
more than m bytes being allocated by the memory manager with the extra bytes
used by the memory manager itself rather than the requester we will assume that
all memory blocks are organized as shown in figure 1212 with space for tags and
linked list pointers here free and reserved blocks are distinguished by a tag bit
at both the beginning and the end of the block for reasons that will be explained
in addition both free and reserved blocks have a size indicator immediately after
the tag bit at the beginning of the block to indicate how large the block is free
blocks have a second size indicator immediately preceding the tag bit at the end of
the block finally free blocks have left and right pointers to their neighbors in the
free block list
the information fields associated with each block permit the memory manager
to allocate and deallocate blocks as needed when a request comes in for m words
of storage the memory manager searches the linked list of free blocks until it finds
a suitable block for allocation how it determines which block is suitable will
be discussed below if the block contains exactly m words plus space for the tag
and size fields then it is removed from the freelist if the block of size k is large


$$@@$$PAGE: 446
427

sec 123 memory management

tag sizellink rlink


tag size


k

k

k





size tag
a

tag
b

figure 1212 blocks as seen by the memory manager each block includes
additional information such as freelist link pointers start and end tags and a size
field a the layout for a free block the beginning of the block contains the tag
bit field the block size field and two pointers for the freelist the end of the block
contains a second tag field and a second block size field b a reserved block of
k bytes the memory manager adds to these k bytes an additional tag bit field and
block size field at the beginning of the block and a second tag field at the end of
the block

enough then the remaining k  m words are reserved as a block on the freelist in
the current location
when a block f is freed it must be merged into the freelist if we do not
care about merging adjacent free blocks then this is a simple insertion into the
doubly linked list of free blocks however we would like to merge adjacent blocks
because this allows the memory manager to serve requests of the largest possible
size merging is easily done due to the tag and size fields stored at the ends of each
block as illustrated by figure 1213 here the memory manager first checks the
unit of memory immediately preceding block f to see if the preceding block call
it p is also free if it is then the memory unit before ps tag bit stores the size
of p thus indicating the position for the beginning of the block in memory p can
then simply have its size extended to include block f if block p is not free then
we just add block f to the freelist finally we also check the bit following the end
of block f if this bit indicates that the following block call it s is free then s is
removed from the freelist and the size of f is extended appropriately
we now consider how a suitable free block is selected to service a memory
request to illustrate the process assume that we have a memory pool with 200
units of storage after some series of allocation requests and releases we have
reached a point where there are four free blocks on the freelist of sizes 25 35 32
and 45 in that order assume that a request is made for 30 units of storage for
our examples we ignore the overhead imposed for the tag link and size fields
discussed above


$$@@$$PAGE: 447
428

chap 12 lists and arrays revisited

p

f

s

 k

k





figure 1213 adding block f to the freelist the word immediately preceding
the start of f in the memory pool stores the tag bit of the preceding block p if p
is free merge f into p we find the end of f by using fs size field the word
following the end of f is the tag field for block s if s is free merge it into f

the simplest method for selecting a block would be to move down the free
block list until a block of size at least 30 is found any remaining space in this
block is left on the freelist if we begin at the beginning of the list and work down
to the first free block at least as large as 30 we select the block of size 35 30 units
of storage will be allocated leaving a free block with 5 units of space because this
approach selects the first block with enough space it is called first fit a simple
variation that will improve performance is instead of always beginning at the head
of the freelist remember the last position reached in the previous search and start
from there when the end of the freelist is reached search begins again at the
head of the freelist this modification reduces the number of unnecessary searches
through small blocks that were passed over by previous requests
there is a potential disadvantage to first fit it might waste larger blocks
by breaking them up and so they will not be available for large requests later
a strategy that avoids using large blocks unnecessarily is called best fit best fit
looks at the entire list and picks the smallest block that is at least as large as the
request ie the best or closest fit to the request continuing with the preceding
example the best fit for a request of 30 units is the block of size 32 leaving a
remainder of size 2 best fit has the disadvantage that it requires that the entire list
be searched another problem is that the remaining portion of the bestfit block is
likely to be small and thus useless for future requests in other words best fit tends
to maximize problems of external fragmentation while it minimizes the chance of
not being able to service an occasional large request
a strategy contrary to best fit might make sense because it tends to minimize the
effects of external fragmentation this is called worst fit which always allocates
the largest block on the list hoping that the remainder of the block will be useful
for servicing a future request in our example the worst fit is the block of size
45 leaving a remainder of size 15 if there are a few unusually large requests
this approach will have less chance of servicing them if requests generally tend


$$@@$$PAGE: 448
sec 123 memory management

429

to be of the same size then this might be an effective strategy like best fit worst
fit requires searching the entire freelist at each memory request to find the largest
block alternatively the freelist can be ordered from largest to smallest free block
possibly by using a priority queue implementation
which strategy is best it depends on the expected types of memory requests
if the requests are of widely ranging size best fit might work well if the requests
tend to be of similar size with rare large and small requests first or worst fit might
work well unfortunately there are always request patterns that one of the three
sequential fit methods will service but which the other two will not be able to
service for example if the series of requests 600 650 900 500 100 is made to
a freelist containing blocks 500 700 650 900 in that order the requests can all
be serviced by first fit but not by best fit alternatively the series of requests 600
500 700 900 can be serviced by best fit but not by first fit on this same freelist
buddy methods
sequentialfit methods rely on a linked list of free blocks which must be searched
for a suitable block at each memory request thus the time to find a suitable free
block would be n in the worst case for a freelist containing n blocks merging
adjacent free blocks is somewhat complicated finally we must either use additional space for the linked list or use space within the memory pool to support the
memory manager operations in the second option both free and reserved blocks
require tag and size fields fields in free blocks do not cost any space because they
are stored in memory that is not otherwise being used but fields in reserved blocks
create additional overhead
the buddy system solves most of these problems searching for a block of
the proper size is efficient merging adjacent free blocks is simple and no tag or
other information fields need be stored within reserved blocks the buddy system
assumes that memory is of size 2n for some integer n  both free and reserved
blocks will always be of size 2k for k  n  at any given time there might be both
free and reserved blocks of various sizes the buddy system keeps a separate list
for free blocks of each size there can be at most n such lists because there can
only be n distinct block sizes
when a request comes in for m words we first determine the smallest value of
k such that 2k  m a block of size 2k is selected from the free list for that block
size if one exists the buddy system does not worry about internal fragmentation
the entire block of size 2k is allocated
if no block of size 2k exists the next larger block is located this block is split
in half repeatedly if necessary until the desired block of size 2k is created any
other blocks generated as a byproduct of this splitting process are placed on the
appropriate freelists


$$@@$$PAGE: 449
430

chap 12 lists and arrays revisited

0000

0000
buddies
buddies

0100

1000

1000
buddies

1100

a

b

figure 1214 example of the buddy system a blocks of size 8 b blocks of
size 4

the disadvantage of the buddy system is that it allows internal fragmentation
for example a request for 257 words will require a block of size 512 the primary
advantages of the buddy system are 1 there is less external fragmentation 2
search for a block of the right size is cheaper than say best fit because we need
only find the first available block on the block list for blocks of size 2k  and 3
merging adjacent free blocks is easy
the reason why this method is called the buddy system is because of the way
that merging takes place the buddy for any block of size 2k is another block
of the same size and with the same address ie the byte position in memory
read as a binary value except that the kth bit is reversed for example the block
of size 8 with beginning address 0000 in figure 1214a has buddy with address
1000 likewise in figure 1214b the block of size 4 with address 0000 has
buddy 0100 if free blocks are sorted by address value the buddy can be found by
searching the correct blocksize list merging simply requires that the address for
the combined buddies be moved to the freelist for the next larger block size
other memory allocation methods
in addition to sequentialfit and buddy methods there are many ad hoc approaches
to memory management if the application is sufficiently complex it might be
desirable to break available memory into several memory zones each with a different memory management scheme for example some zones might have a simple
memory access pattern of firstin firstout this zone can therefore be managed efficiently by using a simple queue another zone might allocate only records of fixed
size and so can be managed with a simple freelist as described in section 412
other zones might need one of the generalpurpose memory allocation methods
discussed in this section the advantage of zones is that some portions of memory


$$@@$$PAGE: 450
sec 123 memory management

431

can be managed more efficiently the disadvantage is that one zone might fill up
while other zones have excess free memory if the zone sizes are chosen poorly
another approach to memory management is to impose a standard size on all
memory requests we have seen an example of this concept already in disk file
management where all files are allocated in multiples of the cluster size this
approach leads to internal fragmentation but managing files composed of clusters
is easier than managing arbitrarily sized files the cluster scheme also allows us
to relax the restriction that the memory request be serviced by a contiguous block
of memory most disk file managers and operating system main memory managers
work on a cluster or page system block management is usually done with a buffer
pool to allocate available blocks in main memory efficiently
1232

failure policies and garbage collection

at some point when processing a series of requests a memory manager could encounter a request for memory that it cannot satisfy in some situations there might
be nothing that can be done there simply might not be enough free memory to
service the request and the application may require that the request be serviced immediately in this case the memory manager has no option but to return an error
which could in turn lead to a failure of the application program however in many
cases there are alternatives to simply returning an error the possible options are
referred to collectively as failure policies
in some cases there might be sufficient free memory to satisfy the request
but it is scattered among small blocks this can happen when using a sequentialfit memory allocation method where external fragmentation has led to a series of
small blocks that collectively could service the request in this case it might be
possible to compact memory by moving the reserved blocks around so that the
free space is collected into a single block a problem with this approach is that
the application must somehow be able to deal with the fact that its data have now
been moved to different locations if the application program relies on the absolute
positions of the data in any way this would be disastrous one approach for dealing
with this problem involves the handles returned by the memory manager a handle
works as a second level of indirection to a memory location the memory allocation
routine does not return a pointer to the block of storage but rather a pointer to a the
handle that in turn gives access to the storage the handle never moves its position
but the position of the block might be moved and the value of the handle updated
of course this requires that the memory manager keep track of the handles and
how they associate with the stored messages figure 1215 illustrates the concept
another failure policy that might work in some applications is to defer the
memory request until sufficient memory becomes available for example a multitasking operating system could adopt the strategy of not allowing a process to run
until there is sufficient memory available while such a delay might be annoying


$$@@$$PAGE: 451
432

chap 12 lists and arrays revisited

handle

memory block

figure 1215 using handles for dynamic memory management the memory
manager returns the address of the handle in response to a memory request the
handle stores the address of the actual memory block in this way the memory
block might be moved with its address updated in the handle without disrupting
the application program

to the user it is better than halting the entire system the assumption here is that
other processes will eventually terminate freeing memory
another option might be to allocate more memory to the memory manager in
a zoned memory allocation system where the memory manager is part of a larger
system this might be a viable option in a c program that implements its own
memory manager it might be possible to get more memory from the systemlevel
new operator such as is done by the freelist of section 412
the last failure policy that we will consider is garbage collection consider
the following series of statements
int p  new int5
int q  new int10
p  q

while in java this would be no problem due to automatic garbage collection in
languages such as c  this would be considered bad form because the original
space allocated to p is lost as a result of the third assignment this space cannot
be used again by the program such lost memory is referred to as garbage also
known as a memory leak when no program variable points to a block of space
no future access to that space is possible of course if another variable had first
been assigned to point to ps space then reassigning p would not create garbage
some programming languages take a different view towards garbage in particular the lisp programming language uses the multilist representation of figure 125 and all storage is in the form either of internal nodes with two pointers
or atoms figure 1216 shows a typical collection of lisp structures headed by
variables named a b and c along with a freelist
in lisp list objects are constantly being put together in various ways as temporary variables and then all reference to them is lost when the object is no longer
needed thus garbage is normal in lisp and in fact cannot be avoided during
routine program behavior when lisp runs out of memory it resorts to a garbage
collection process to recover the space tied up in garbage garbage collection consists of examining the managed memory pool to determine which parts are still


$$@@$$PAGE: 452
433

sec 123 memory management

a

a

c

b

d
c

e

f
g

h

freelist
figure 1216 example of lisp list variables including the system freelist

being used and which parts are garbage in particular a list is kept of all program
variables and any memory locations not reachable from one of these variables are
considered to be garbage when the garbage collector executes all unused memory
locations are placed in free store for future access this approach has the advantage
that it allows for easy collection of garbage it has the disadvantage from a users
point of view that every so often the system must halt while it performs garbage
collection for example garbage collection is noticeable in the emacs text editor which is normally implemented in lisp occasionally the user must wait for a
moment while the memory management system performs garbage collection
the java programming language also makes use of garbage collection as in
lisp it is common practice in java to allocate dynamic memory as needed and to
later drop all references to that memory the garbage collector is responsible for
reclaiming such unused space as necessary this might require extra time when
running the program but it makes life considerably easier for the programmer in
contrast many large applications written in c even commonly used commercial
software contain memory leaks that will in time cause the program to fail
several algorithms have been used for garbage collection one is the reference
count algorithm here every dynamically allocated memory block includes space
for a count field whenever a pointer is directed to a memory block the reference
count is increased whenever a pointer is directed away from a memory block
the reference count is decreased if the count ever becomes zero then the memory
block is considered garbage and is immediately placed in free store this approach
has the advantage that it does not require an explicit garbage collection phase because information is put in free store immediately when it becomes garbage


$$@@$$PAGE: 453
434

chap 12 lists and arrays revisited

g

h

figure 1217 garbage cycle example all memory elements in the cycle have
nonzero reference counts because each element has one pointer to it even though
the entire cycle is garbage ie no static variable in the program points to it

reference counts are used by the unix file system files can have multiple
names called links the file system keeps a count of the number of links to each
file whenever a file is deleted in actuality its link field is simply reduced by
one if there is another link to the file then no space is recovered by the file system
when the number of links goes to zero the files space becomes available for reuse
reference counts have several major disadvantages first a reference count
must be maintained for each memory object this works well when the objects are
large such as a file however it will not work well in a system such as lisp where
the memory objects typically consist of two pointers or a value an atom another
major problem occurs when garbage contains cycles consider figure 1217 here
each memory object is pointed to once but the collection of objects is still garbage
because no pointer points to the collection thus reference counts only work when
the memory objects are linked together without cycles such as the unix file system where files can only be organized as a dag
another approach to garbage collection is the marksweep strategy here each
memory object needs only a single mark bit rather than a reference counter field
when free store is exhausted a separate garbage collection phase takes place as
follows
1 clear all mark bits
2 perform depthfirst search dfs following pointers beginning with each
variable on the systems list of static variables each memory element encountered during the dfs has its mark bit turned on
3 a sweep is made through the memory pool visiting all elements unmarked elements are considered garbage and placed in free store
the advantages of the marksweep approach are that it needs less space than is
necessary for reference counts and it works for cycles however there is a major
disadvantage this is a hidden space requirement needed to do the processing
dfs is a recursive algorithm either it must be implemented recursively in which
case the compilers runtime system maintains a stack or else the memory manager
can maintain its own stack what happens if all memory is contained in a single
linked list then the depth of the recursion or the size of the stack is the number
of memory cells unfortunately the space for the dfs stack must be available at
the worst conceivable time that is when free memory has been exhausted


$$@@$$PAGE: 454
435

sec 124 further reading

4

3
b

1
a
2

c

e

5

a

6
4

3 b

1
a

prev

5
2

c

e
6
curr

b

figure 1218 example of the deutschschorrwaite garbage collection algorithm a the initial multilist structure b the multilist structure of a at
the instant when link node 5 is being processed by the garbage collection algorithm a chain of pointers stretching from variable prev to the head node of the
structure has been temporarily created by the garbage collection algorithm

fortunately a clever technique allows dfs to be performed without requiring
additional space for a stack instead the structure being traversed is used to hold
the stack at each step deeper into the traversal instead of storing a pointer on the
stack we borrow the pointer being followed this pointer is set to point back
to the node we just came from in the previous step as illustrated by figure 1218
each borrowed pointer stores an additional bit to tell us whether we came down
the left branch or the right branch of the link node being pointed to at any given
instant we have passed down only one path from the root and we can follow the
trail of pointers back up as we return equivalent to popping the recursion stack
we set the pointer back to its original position so as to return the structure to its
original condition this is known as the deutschschorrwaite garbage collection
algorithm

124

further reading

for information on lisp see the little lisper by friedman and felleisen ff89
another good lisp reference is common lisp the language by guy l steele
ste90 for information on emacs which is both an excellent text editor and
a programming environment see the gnu emacs manual by richard stallman


$$@@$$PAGE: 455
436

chap 12 lists and arrays revisited

l4
l1
a

d

e

l2

l3

b
l1

c

a

a

a

b

b

c

d

c

figure 1219 some example multilists

sta11b you can get more information about javas garbage collection system
from the java programming language by ken arnold and james gosling ag06
for more details on sparse matrix representations the yale representation is described by eisenstat schultz and sherman ess81 the matlab sparse matrix
representation is described by gilbert moler and schreiber gms91
an introductory text on operating systems covers many topics relating to memory management issues including layout of files on disk and caching of information
in main memory all of the topics covered here on memory management buffer
pools and paging are relevant to operating system implementation for example
see operating systems by william stallingssta11a

125

exercises

121 for each of the following bracket notation descriptions draw the equivalent
multilist in graphical form such as shown in figure 122
a ha b hc d ei hf hgi hii
b ha b hc d l1  ei l1i
c hl1  a l1 hl2  bi l2 hl1ii
122 a show the bracket notation for the list of figure 1219a
b show the bracket notation for the list of figure 1219b
c show the bracket notation for the list of figure 1219c
123 given the linked representation of a pure list such as
hx1  hy1  y2  hz1  z2 i y4 i hw1  w2 i x4 i
write an inplace reversal algorithm to reverse the sublists at all levels including the topmost level for this example the result would be a linked
representation corresponding to
hx4  hw2  w1 i hy4  hz2  z1 i y2  y1 i x1 i
124 what fraction of the values in a matrix must be zero for the sparse matrix
representation of section 122 to be more space efficient than the standard
twodimensional matrix representation when data values require eight bytes
array indices require two bytes and pointers require four bytes


$$@@$$PAGE: 456
sec 126 projects

437

125 write a function to add an element at a given position to the sparse matrix
representation of section 122
126 write a function to delete an element from a given position in the sparse
matrix representation of section 122
127 write a function to transpose a sparse matrix as represented in section 122
128 write a function to add two sparse matrices as represented in section 122
129 write memory manager allocation and deallocation routines for the situation
where all requests and releases follow a lastrequested firstreleased stack
order
1210 write memory manager allocation and deallocation routines for the situation
where all requests and releases follow a lastrequested lastreleased queue
order
1211 show the result of allocating the following blocks from a memory pool of
size 1000 using first fit for each series of block requests state if a given
request cannot be satisfied
a take 300 call this block a take 500 release a take 200 take 300
b take 200 call this block a take 500 release a take 200 take 300
c take 500 call this block a take 300 release a take 300 take 200
1212 show the result of allocating the following blocks from a memory pool of
size 1000 using best fit for each series of block requests state if a given
request cannot be satisfied
a take 300 call this block a take 500 release a take 200 take 300
b take 200 call this block a take 500 release a take 200 take 300
c take 500 call this block a take 300 release a take 300 take 200
1213 show the result of allocating the following blocks from a memory pool of
size 1000 using worst fit for each series of block requests state if a given
request cannot be satisfied
a take 300 call this block a take 500 release a take 200 take 300
b take 200 call this block a take 500 release a take 200 take 300
c take 500 call this block a take 300 release a take 300 take 200
1214 assume that the memory pool contains three blocks of free storage their
sizes are 1300 2000 and 1000 give examples of storage requests for which
a firstfit allocation will work but not best fit or worst fit
b bestfit allocation will work but not first fit or worst fit
c worstfit allocation will work but not first fit or best fit

126

projects

121 implement the orthogonal list sparse matrix representation of section 122
your implementation should support the following operations on the matrix


$$@@$$PAGE: 457
438

122

123

124

125

126

chap 12 lists and arrays revisited

 insert an element at a given position
 delete an element from a given position
 return the value of the element at a given position
 take the transpose of a matrix
 add two matrices and
 multiply two matrices
implement the yale model for sparse matrices described at the end of section 122 your implementation should support the following operations on
the matrix
 insert an element at a given position
 delete an element from a given position
 return the value of the element at a given position
 take the transpose of a matrix
 add two matrices and
 multiply two matrices
implement the memmanager adt shown at the beginning of section 123
use a separate linked list to implement the freelist your implementation
should work for any of the three sequentialfit methods first fit best fit and
worst fit test your system empirically to determine under what conditions
each method performs well
implement the memmanager adt shown at the beginning of section 123
do not use separate memory for the free list but instead embed the free list
into the memory pool as shown in figure 1212 your implementation should
work for any of the three sequentialfit methods first fit best fit and worst
fit test your system empirically to determine under what conditions each
method performs well
implement the memmanager adt shown at the beginning of section 123
using the buddy method of section 1231 your system should support
requests for blocks of a specified size and release of previously requested
blocks
implement the deutschschorrwaite garbage collection algorithm that is illustrated by figure 1218


$$@@$$PAGE: 458
13
advanced tree structures

this chapter introduces several tree structures designed for use in specialized applications the trie of section 131 is commonly used to store and retrieve strings
it also serves to illustrate the concept of a key space decomposition the avl
tree and splay tree of section 132 are variants on the bst they are examples of
selfbalancing search trees and have guaranteed good performance regardless of the
insertion order for records an introduction to several spatial data structures used
to organize point data by xycoordinates is presented in section 133
descriptions of the fundamental operations are given for each data structure
one purpose for this chapter is to provide opportunities for class programming
projects so detailed implementations are left to the reader

131

tries

recall that the shape of a bst is determined by the order in which its data records
are inserted one permutation of the records might yield a balanced tree while
another might yield an unbalanced tree with the extreme case becoming the shape
of a linked list the reason is that the value of the key stored in the root node splits
the key range into two parts those key values less than the roots key value and
those key values greater than the roots key value depending on the relationship
between the root nodes key value and the distribution of the key values for the
other records in the the tree the resulting bst might be balanced or unbalanced
thus the bst is an example of a data structure whose organization is based on an
object space decomposition so called because the decomposition of the key range
is driven by the objects ie the key values of the data records stored in the tree
the alternative to object space decomposition is to predefine the splitting position within the key range for each node in the tree in other words the root could be
predefined to split the key range into two equal halves regardless of the particular
values or order of insertion for the data records those records with keys in the
lower half of the key range will be stored in the left subtree while those records
439


$$@@$$PAGE: 459
440

chap 13 advanced tree structures

with keys in the upper half of the key range will be stored in the right subtree
while such a decomposition rule will not necessarily result in a balanced tree the
tree will be unbalanced if the records are not well distributed within the key range
at least the shape of the tree will not depend on the order of key insertion furthermore the depth of the tree will be limited by the resolution of the key range that
is the depth of the tree can never be greater than the number of bits required to
store a key value for example if the keys are integers in the range 0 to 1023 then
the resolution for the key is ten bits thus two keys can be identical only until the
tenth bit in the worst case two keys will follow the same path in the tree only until
the tenth branch as a result the tree will never be more than ten levels deep in
contrast a bst containing n records could be as much as n levels deep
splitting based on predetermined subdivisions of the key range is called key
space decomposition in computer graphics the technique is known as image
space decomposition and this term is sometimes used to describe the process for
data structures as well a data structure based on key space decomposition is called
a trie folklore has it that trie comes from retrieval unfortunately that would
imply that the word is pronounced tree which would lead to confusion with regular use of the word tree trie is actually pronounced as try
like the b tree a trie stores data records only in leaf nodes internal nodes
serve as placeholders to direct the search process but since the split points are predetermined internal nodes need not store trafficdirecting key values figure 131
illustrates the trie concept upper and lower bounds must be imposed on the key
values so that we can compute the middle of the key range because the largest
value inserted in this example is 120 a range from 0 to 127 is assumed as 128 is
the smallest power of two greater than 120 the binary value of the key determines
whether to select the left or right branch at any given point during the search the
most significant bit determines the branch direction at the root figure 131 shows
a binary trie so called because in this example the trie structure is based on the
value of the key interpreted as a binary number which results in a binary tree
the huffman coding tree of section 56 is another example of a binary trie all
data values in the huffman tree are at the leaves and each branch splits the range
of possible letter codes in half the huffman codes are actually reconstructed from
the letter positions within the trie
these are examples of binary tries but tries can be built with any branching
factor normally the branching factor is determined by the alphabet used for
binary numbers the alphabet is 0 1 and a binary trie results other alphabets
lead to other branching factors
one application for tries is to store a dictionary of words such a trie will be
referred to as an alphabet trie for simplicity our examples will ignore case in
letters we add a special character  to the 26 standard english letters the 
character is used to represent the end of a string thus the branching factor for


$$@@$$PAGE: 460
441

sec 131 tries

1

0
1

0
0
0
0
2

120
0

1
24

1
7

0
32

0

1

1

0

37

0

1

40
42
figure 131 the binary trie for the collection of values 2 7 24 31 37 40 42
120 all data values are stored in the leaf nodes edges are labeled with the value
of the bit used to determine the branching direction of each node the binary
form of the key value determines the path to the record assuming that each key is
represented as a 7bit value representing a number in the range 0 to 127

each node is up to 27 once constructed the alphabet trie is used to determine
if a given word is in the dictionary consider searching for a word in the alphabet
trie of figure 132 the first letter of the search word determines which branch
to take from the root the second letter determines which branch to take at the
next level and so on only the letters that lead to a word are shown as branches
in figure 132b the leaf nodes of the trie store a copy of the actual words while
in figure 132a the word is built up from the letters associated with each branch
one way to implement a node of the alphabet trie is as an array of 27 pointers
indexed by letter because most nodes have branches to only a small fraction of the
possible letters in the alphabet an alternate implementation is to use a linked list of
pointers to the child nodes as in figure 69
the depth of a leaf node in the alphabet trie of figure 132b has little to do
with the number of nodes in the trie or even with the length of the corresponding
string rather a nodes depth depends on the number of characters required to
distinguish this nodes word from any other for example if the words anteater
and antelope are both stored in the trie it is not until the fifth letter that the two
words can be distinguished thus these words must be stored at least as deep as
level five in general the limiting factor on the depth of nodes in the alphabet trie
is the length of the words stored
poor balance and clumping can result when certain prefixes are heavily used
for example an alphabet trie storing the common words in the english language
would have many words in the th branch of the tree but none in the zq branch
any multiway branching trie can be replaced with a binary trie by replacing the
original tries alphabet with an equivalent binary code alternatively we can use
the techniques of section 634 for converting a general tree to a binary tree without
modifying the alphabet


$$@@$$PAGE: 461
442

chap 13 advanced tree structures

d

c

a

e

g

h
o

o

l o

r

n

h

t

i

e

c

c

r

k

t

d

s

s

k







f

e

e







e
a

l

u
a

t

o

e

i

e

p

n

s

r

e



h






a

a
n

c
chicken

t

ant

d
e

g

o

u

deer

duck

e
a

h

a

l

horse
o

goat goldfish goose
l

anteater antelope
b

figure 132 two variations on the alphabet trie representation for a set of ten
words a each node contains a set of links corresponding to single letters and
each letter in the set of words has a corresponding link  is used to indicate
the end of a word internal nodes direct the search and also spell out the word
one letter per link the word need not be stored explicitly  is needed to
recognize the existence of words that are prefixes to other words such as ant in
this example b here the trie extends only far enough to discriminate between the
words leaf nodes of the trie each store a complete word internal nodes merely
direct the search


$$@@$$PAGE: 462
443

sec 131 tries

0
1xxxxxx

0xxxxxx

120

1
00xxxxx

01xxxxx

2

3
0101xxx

000xxxx
24

4
2

7

4
32

5
37 40

010101x
42

figure 133 the pat trie for the collection of values 2 7 24 32 37 40 42
120 contrast this with the binary trie of figure 131 in the pat trie all data
values are stored in the leaf nodes while internal nodes store the bit position used
to determine the branching decision assuming that each key is represented as a 7bit value representing a number in the range 0 to 127 some of the branches in this
pat trie have been labeled to indicate the binary representation for all values in
that subtree for example all values in the left subtree of the node labeled 0 must
have value 0xxxxxx where x means that bit can be either a 0 or a 1 all nodes in
the right subtree of the node labeled 3 must have value 0101xxx however we can
skip branching on bit 2 for this subtree because all values currently stored have a
value of 0 for that bit

the trie implementations illustrated by figures 131 and 132 are potentially
quite inefficient as certain key sets might lead to a large number of nodes with only
a single child a variant on trie implementation is known as patricia which
stands for practical algorithm to retrieve information coded in alphanumeric
in the case of a binary alphabet a patricia trie referred to hereafter as a pat
trie is a full binary tree that stores data records in the leaf nodes internal nodes
store only the position within the keys bit pattern that is used to decide on the next
branching point in this way internal nodes with single children equivalently bit
positions within the key that do not distinguish any of the keys within the current
subtree are eliminated a pat trie corresponding to the values of figure 131 is
shown in figure 133
example 131 when searching for the value 7 0000111 in binary in
the pat trie of figure 133 the root node indicates that bit position 0 the
leftmost bit is checked first because the 0th bit for value 7 is 0 take the
left branch at level 1 branch depending on the value of bit 1 which again
is 0 at level 2 branch depending on the value of bit 2 which again is 0 at
level 3 the index stored in the node is 4 this means that bit 4 of the key is
checked next the value of bit 3 is irrelevant because all values stored in
that subtree have the same value at bit position 3 thus the single branch
that extends from the equivalent node in figure 131 is just skipped for
key value 7 bit 4 has value 1 so the rightmost branch is taken because


$$@@$$PAGE: 463
444

chap 13 advanced tree structures

this leads to a leaf node the search key is compared against the key stored
in that node if they match then the desired record has been found
note that during the search process only a single bit of the search key is compared at each internal node this is significant because the search key could be
quite large search in the pat trie requires only a single fullkey comparison
which takes place once a leaf node has been reached
example 132 consider the situation where we need to store a library of
dna sequences a dna sequence is a series of letters usually many thousands of characters long with the string coming from an alphabet of only
four letters that stand for the four amino acids making up a dna strand
similar dna sequences might have long sections of their string that are
identical the pat trie would avoid making multiple full key comparisons
when searching for a specific sequence

132

balanced trees

we have noted several times that the bst has a high risk of becoming unbalanced
resulting in excessively expensive search and update operations one solution to
this problem is to adopt another search tree structure such as the 23 tree or the
binary trie an alternative is to modify the bst access functions in some way to
guarantee that the tree performs well this is an appealing concept and it works
well for heaps whose access functions maintain the heap in the shape of a complete
binary tree unfortunately requiring that the bst always be in the shape of a
complete binary tree requires excessive modification to the tree during update as
discussed in section 103
if we are willing to weaken the balance requirements we can come up with
alternative update routines that perform well both in terms of cost for the update
and in balance for the resulting tree structure the avl tree works in this way
using insertion and deletion routines altered from those of the bst to ensure that
for every node the depths of the left and right subtrees differ by at most one the
avl tree is described in section 1321
a different approach to improving the performance of the bst is to not require
that the tree always be balanced but rather to expend some effort toward making
the bst more balanced every time it is accessed this is a little like the idea of path
compression used by the unionfind algorithm presented in section 62 one
example of such a compromise is called the splay tree the splay tree is described
in section 1322


$$@@$$PAGE: 464
445

sec 132 balanced trees

37

37

24
7

42
32

40

24
42
120

2

7

42
32

2

40

42
120

5
figure 134 example of an insert operation that violates the avl tree balance
property prior to the insert operation all nodes of the tree are balanced ie the
depths of the left and right subtrees for every node differ by at most one after
inserting the node with value 5 the nodes with values 7 and 24 are no longer
balanced

1321

the avl tree

the avl tree named for its inventors adelsonvelskii and landis should be
viewed as a bst with the following additional property for every node the heights
of its left and right subtrees differ by at most 1 as long as the tree maintains this
property if the tree contains n nodes then it has a depth of at most olog n as
a result search for any node will cost olog n and if the updates can be done in
time proportional to the depth of the node inserted or deleted then updates will also
cost olog n even in the worst case
the key to making the avl tree work is to alter the insert and delete routines
so as to maintain the balance property of course to be practical we must be able
to implement the revised update routines in log n time
consider what happens when we insert a node with key value 5 as shown in
figure 134 the tree on the left meets the avl tree balance requirements after
the insertion two nodes no longer meet the requirements because the original tree
met the balance requirement nodes in the new tree can only be unbalanced by a
difference of at most 2 in the subtrees for the bottommost unbalanced node call
it s there are 4 cases
1
2
3
4

the extra node is in the left child of the left child of s
the extra node is in the right child of the left child of s
the extra node is in the left child of the right child of s
the extra node is in the right child of the right child of s

cases 1 and 4 are symmetrical as are cases 2 and 3 note also that the unbalanced
nodes must be on the path from the root to the newly inserted node
our problem now is how to balance the tree in olog n time it turns out that
we can do this using a series of local operations known as rotations cases 1 and


$$@@$$PAGE: 465
446

chap 13 advanced tree structures

s

x

x

s

c
a
b

b

c

a

b

a

figure 135 a single rotation in an avl tree this operation occurs when the
excess node in subtree a is in the left child of the left child of the unbalanced
node labeled s by rearranging the nodes as shown we preserve the bst property
as well as rebalance the tree to preserve the avl tree balance property the case
where the excess node is in the right child of the right child of the unbalanced
node is handled in the same way

s

x

y
d
y

a

x

c
b

a

s

a

c
b

d

b

figure 136 a double rotation in an avl tree this operation occurs when the
excess node in subtree b is in the right child of the left child of the unbalanced
node labeled s by rearranging the nodes as shown we preserve the bst property
as well as rebalance the tree to preserve the avl tree balance property the case
where the excess node is in the left child of the right child of s is handled in the
same way

4 can be fixed using a single rotation as shown in figure 135 cases 2 and 3 can
be fixed using a double rotation as shown in figure 136
the avl tree insert algorithm begins with a normal bst insert then as the
recursion unwinds up the tree we perform the appropriate rotation on any node


$$@@$$PAGE: 466
sec 132 balanced trees

447

that is found to be unbalanced deletion is similar however consideration for
unbalanced nodes must begin at the level of the deletemin operation
example 133 in figure 134 b the bottommost unbalanced node has
value 7 the excess node with value 5 is in the right subtree of the left
child of 7 so we have an example of case 2 this requires a double rotation
to fix after the rotation 5 becomes the left child of 24 2 becomes the left
child of 5 and 7 becomes the right child of 5

1322

the splay tree

like the avl tree the splay tree is not actually a distinct data structure but rather
reimplements the bst insert delete and search methods to improve the performance of a bst the goal of these revised methods is to provide guarantees on the
time required by a series of operations thereby avoiding the worstcase linear time
behavior of standard bst operations no single operation in the splay tree is guaranteed to be efficient instead the splay tree access rules guarantee that a series
of m operations will take om log n time for a tree of n nodes whenever m  n
thus a single insert or search operation could take on time however m such
operations are guaranteed to require a total of om log n time for an average cost
of olog n per access operation this is a desirable performance guarantee for any
searchtree structure
unlike the avl tree the splay tree is not guaranteed to be height balanced
what is guaranteed is that the total cost of the entire series of accesses will be
cheap ultimately it is the cost of the series of operations that matters not whether
the tree is balanced maintaining balance is really done only for the sake of reaching
this time efficiency goal
the splay tree access functions operate in a manner reminiscent of the movetofront rule for selforganizing lists from section 92 and of the path compression technique for managing parentpointer trees from section 62 these access
functions tend to make the tree more balanced but an individual access will not
necessarily result in a more balanced tree
whenever a node s is accessed eg when s is inserted deleted or is the goal
of a search the splay tree performs a process called splaying splaying moves s
to the root of the bst when s is being deleted splaying moves the parent of s to
the root as in the avl tree a splay of node s consists of a series of rotations
a rotation moves s higher in the tree by adjusting its position with respect to its
parent and grandparent a side effect of the rotations is a tendency to balance the
tree there are three types of rotation
a single rotation is performed only if s is a child of the root node the single
rotation is illustrated by figure 137 it basically switches s with its parent in a


$$@@$$PAGE: 467
448

chap 13 advanced tree structures

p

s

a

s

c

p

a

b
a

b

c

b

figure 137 splay tree single rotation this rotation takes place only when
the node being splayed is a child of the root here node s is promoted to the
root rotating with node p because the value of s is less than the value of p
p must become ss right child the positions of subtrees a b and c are altered
as appropriate to maintain the bst property but the contents of these subtrees
remains unchanged a the original tree with p as the parent b the tree after
a rotation takes place performing a single rotation a second time will return the
tree to its original shape equivalently if b is the initial configuration of the tree
ie s is at the root and p is its right child then a shows the result of a single
rotation to splay p to the root

way that retains the bst property while figure 137 is slightly different from
figure 135 in fact the splay tree single rotation is identical to the avl tree single
rotation
unlike the avl tree the splay tree requires two types of double rotation double rotations involve s its parent call it p and ss grandparent call it g the
effect of a double rotation is to move s up two levels in the tree
the first double rotation is called a zigzag rotation it takes place when either
of the following two conditions are met
1 s is the left child of p and p is the right child of g
2 s is the right child of p and p is the left child of g
in other words a zigzag rotation is used when g p and s form a zigzag the
zigzag rotation is illustrated by figure 138
the other double rotation is known as a zigzig rotation a zigzig rotation takes
place when either of the following two conditions are met
1 s is the left child of p which is in turn the left child of g
2 s is the right child of p which is in turn the right child of g
thus a zigzig rotation takes place in those situations where a zigzag rotation is not
appropriate the zigzig rotation is illustrated by figure 139 while figure 139
appears somewhat different from figure 136 in fact the zigzig rotation is identical
to the avl tree double rotation


$$@@$$PAGE: 468
449

sec 132 balanced trees

g

p

s

d
p

s

a

c

b

a

g

b

a

c

d

b

figure 138 splay tree zigzag rotation a the original tree with s p and g
in zigzag formation b the tree after the rotation takes place the positions of
subtrees a b c and d are altered as appropriate to maintain the bst property

g

p

d

s

a

s

c

p

a

g

b

b

c
a

d

b

figure 139 splay tree zigzig rotation a the original tree with s p and g
in zigzig formation b the tree after the rotation takes place the positions of
subtrees a b c and d are altered as appropriate to maintain the bst property

note that zigzag rotations tend to make the tree more balanced because they
bring subtrees b and c up one level while moving subtree d down one level the
result is often a reduction of the trees height by one zigzig promotions and single
rotations do not typically reduce the height of the tree they merely bring the newly
accessed record toward the root
splaying node s involves a series of double rotations until s reaches either the
root or the child of the root then if necessary a single rotation makes s the root
this process tends to rebalance the tree regardless of balance splaying will make
frequently accessed nodes stay near the top of the tree resulting in reduced access
cost proof that the splay tree meets the guarantee of om log n is beyond the
scope of this book such a proof can be found in the references in section 134


$$@@$$PAGE: 469
450

chap 13 advanced tree structures

example 134 consider a search for value 89 in the splay tree of figure 1310a the splay trees search operation is identical to searching in
a bst however once the value has been found it is splayed to the root
three rotations are required in this example the first is a zigzig rotation
whose result is shown in figure 1310b the second is a zigzag rotation
whose result is shown in figure 1310c the final step is a single rotation
resulting in the tree of figure 1310d notice that the splaying process has
made the tree shallower

133

spatial data structures

all of the search trees discussed so far  bsts avl trees splay trees 23 trees
btrees and tries  are designed for searching on a onedimensional key a typical
example is an integer key whose onedimensional range can be visualized as a
number line these various tree structures can be viewed as dividing this onedimensional number line into pieces
some databases require support for multiple keys in other words records can
be searched for using any one of several key fields such as name or id number
typically each such key has its own onedimensional index and any given search
query searches one of these independent indices as appropriate
a multidimensional search key presents a rather different concept imagine
that we have a database of city records where each city has a name and an xycoordinate a bst or splay tree provides good performance for searches on city
name which is a onedimensional key separate bsts could be used to index the xand ycoordinates this would allow us to insert and delete cities and locate them
by name or by one coordinate however search on one of the two coordinates is
not a natural way to view search in a twodimensional space another option is to
combine the xycoordinates into a single key say by concatenating the two coordinates and index cities by the resulting key in a bst that would allow search by
coordinate but would not allow for efficient twodimensional range queries such
as searching for all cities within a given distance of a specified point the problem
is that the bst only works well for onedimensional keys while a coordinate is a
twodimensional key where neither dimension is more important than the other
multidimensional range queries are the defining feature of a spatial application because a coordinate gives a position in space it is called a spatial attribute
to implement spatial applications efficiently requires the use of spatial data structures spatial data structures store data objects organized by position and are an
important class of data structures used in geographic information systems computer graphics robotics and many other fields


$$@@$$PAGE: 470
451

sec 133 spatial data structures

17

17
92

92 g

25
18

99

25 p
18

42 g
72 p

99

89 s
72

89 s

42

75

75
a

b

17 p

89
89 s

25
18

92

92
72

42

17
25
99

75
c

18

99
72

42

75
d

figure 1310 example of splaying after performing a search in a splay tree
after finding the node with key value 89 that node is splayed to the root by performing three rotations a the original splay tree b the result of performing
a zigzig rotation on the node with key value 89 in the tree of a c the result
of performing a zigzag rotation on the node with key value 89 in the tree of b
d the result of performing a single rotation on the node with key value 89 in the
tree of c if the search had been for 91 the search would have been unsuccessful
with the node storing key value 89 being that last one visited in that case the
same splay operations would take place


$$@@$$PAGE: 471
452

chap 13 advanced tree structures

this section presents two spatial data structures for storing point data in two or
more dimensions they are the kd tree and the pr quadtree the kd tree is a
natural extension of the bst to multiple dimensions it is a binary tree whose splitting decisions alternate among the key dimensions like the bst the kd tree uses
object space decomposition the pr quadtree uses key space decomposition and so
is a form of trie it is a binary tree only for onedimensional keys in which case it
is a trie with a binary alphabet for d dimensions it has 2d branches thus in two
dimensions the pr quadtree has four branches hence the name quadtree splitting space into four equalsized quadrants at each branch section 1333 briefly
mentions two other variations on these data structures the bintree and the point
quadtree these four structures cover all four combinations of object versus key
space decomposition on the one hand and multilevel binary versus 2d way branching on the other section 1334 briefly discusses spatial data structures for storing
other types of spatial data
1331

the kd tree

the kd tree is a modification to the bst that allows for efficient processing of
multidimensional keys the kd tree differs from the bst in that each level of
the kd tree makes branching decisions based on a particular search key associated
with that level called the discriminator in principle the kd tree could be used to
unify key searching across any arbitrary set of keys such as name and zipcode but
in practice it is nearly always used to support search on multidimensional coordinates such as locations in 2d or 3d space we define the discriminator at level i
to be i mod k for k dimensions for example assume that we store data organized
by xycoordinates in this case k is 2 there are two coordinates with the xcoordinate field arbitrarily designated key 0 and the ycoordinate field designated
key 1 at each level the discriminator alternates between x and y thus a node n
at level 0 the root would have in its left subtree only nodes whose x values are less
than nx because x is search key 0 and 0 mod 2  0 the right subtree would
contain nodes whose x values are greater than nx  a node m at level 1 would
have in its left subtree only nodes whose y values are less than my  there is no restriction on the relative values of mx and the x values of ms descendants because
branching decisions made at m are based solely on the y coordinate figure 1311
shows an example of how a collection of twodimensional points would be stored
in a kd tree
in figure 1311 the region containing the points is arbitrarily restricted to a
128  128 square and each internal node splits the search space each split is
shown by a line vertical for nodes with x discriminators and horizontal for nodes
with y discriminators the root node splits the space into two parts its children
further subdivide the space into smaller parts the childrens split lines do not
cross the roots split line thus each node in the kd tree helps to decompose the


$$@@$$PAGE: 472
453

sec 133 spatial data structures

c
a 40 45

x
a

d

y

c 70 10
b 15 70

x

b
e

a

d 69 50

y
f

e 66 85

f 85 90

b

figure 1311 example of a kd tree a the kd tree decomposition for a 128 
128unit region containing seven data points b the kd tree for the region of a

space into rectangles that show the extent of where nodes can fall in the various
subtrees
searching a kd tree for the record with a specified xycoordinate is like searching a bst except that each level of the kd tree is associated with a particular discriminator
example 135 consider searching the kd tree for a record located at p 
69 50 first compare p with the point stored at the root record a in
figure 1311 if p matches the location of a then the search is successful
in this example the positions do not match as location 40 45 is not
the same as 69 50 so the search must continue the x value of a is
compared with that of p to determine in which direction to branch because
ax s value of 40 is less than ps x value of 69 we branch to the right subtree
all cities with x value greater than or equal to 40 are in the right subtree
ay does not affect the decision on which way to branch at this level at the
second level p does not match record cs position so another branch must
be taken however at this level we branch based on the relative y values
of point p and record c because 1 mod 2  1 which corresponds to the
ycoordinate because cy s value of 10 is less than py s value of 50 we
branch to the right at this point p is compared against the position of d
a match is made and the search is successful
if the search process reaches a null pointer then that point is not contained
in the tree here is a kd tree search implementation equivalent to the findhelp
function of the bst class kd class private member d stores the keys dimension


$$@@$$PAGE: 473
454

chap 13 advanced tree structures

 find the record with the given coordinates
bool findhelpbinnodee root int coord
e e int discrim const 
 member coord of a node is an integer array storing
 the nodes coordinates
if root  null return false
 empty tree
int currcoord  rootvalcoord
if equalcoordcurrcoord coord   found it
e  rootval
return true

if currcoorddiscrim  coorddiscrim
return findhelprootleftcoordediscrim1d
else
return findhelprootrightcoordediscrim1d


inserting a new node into the kd tree is similar to bst insertion the kd tree
search procedure is followed until a null pointer is found indicating the proper
place to insert the new node
example 136 inserting a record at location 10 50 in the kd tree of
figure 1311 first requires a search to the node containing record b at this
point the new record is inserted into bs left subtree
deleting a node from a kd tree is similar to deleting from a bst but slightly
harder as with deleting from a bst the first step is to find the node call it n
to be deleted it is then necessary to find a descendant of n which can be used to
replace n in the tree if n has no children then n is replaced with a null pointer
note that if n has one child that in turn has children we cannot simply assign ns
parent to point to ns child as would be done in the bst to do so would change the
level of all nodes in the subtree and thus the discriminator used for a search would
also change the result is that the subtree would no longer be a kd tree because a
nodes children might now violate the bst property for that discriminator
similar to bst deletion the record stored in n should be replaced either by the
record in ns right subtree with the least value of ns discriminator or by the record
in ns left subtree with the greatest value for this discriminator assume that n was
at an odd level and therefore y is the discriminator n could then be replaced by the
record in its right subtree with the least y value call it ymin  the problem is that
ymin is not necessarily the leftmost node as it would be in the bst a modified
search procedure to find the least y value in the left subtree must be used to find it
instead the implementation for findmin is shown in figure 1312 a recursive
call to the delete routine will then remove ymin from the tree finally ymin s record
is substituted for the record in node n
note that we can replace the node to be deleted with the leastvalued node
from the right subtree only if the right subtree exists if it does not then a suitable


$$@@$$PAGE: 474
sec 133 spatial data structures

455

 return a pointer to the node with the least value in root
 for the selected descriminator
binnodee findminbinnodee root
int discrim int currdis const 
 discrim discriminator key used for minimum search
 currdis current level mod d
if root  null return null
binnodee minnode  findminrootleft discrim
currdis1d
if discrim  currdis   if not at descrims level
 we must search both subtrees
binnodee rightmin 
findminrootright discrim currdis1d
 check if right side has smaller key value
minnode  minminnode rightmin discrim
  now minnode has the smallest value in children
return minminnode root discrim

figure 1312 the kd tree findmin method on levels using the minimum
values discriminator branching is to the left on other levels both childrens
subtrees must be visited helper function min takes two nodes and a discriminator
as input and returns the node with the smaller value in that discriminator

replacement must be found in the left subtree unfortunately it is not satisfactory
to replace ns record with the record having the greatest value for the discriminator
in the left subtree because this new value might be duplicated if so then we
would have equal values for the discriminator in ns left subtree which violates
the ordering rules for the kd tree fortunately there is a simple solution to the
problem we first move the left subtree of node n to become the right subtree ie
we simply swap the values of ns left and right child pointers at this point we
proceed with the normal deletion process replacing the record of n to be deleted
with the record containing the least value of the discriminator from what is now
ns right subtree
assume that we want to print out a list of all records that are within a certain
distance d of a given point p we will use euclidean distance that is point p is
defined to be within distance d of point n if1
q
px  nx 2  py  ny 2  d
if the search process reaches a node whose key value for the discriminator is
more than d above the corresponding value in the search key then it is not possible
that any record in the right subtree can be within distance d of the search key because all key values in that dimension are always too great similarly if the current
nodes key value in the discriminator is d less than that for the search key value
a more efficient computation is px  nx 2  py  ny 2  d2  this avoids performing a
square root function
1


$$@@$$PAGE: 475
456

chap 13 advanced tree structures

a

c

figure 1313 function incircle must check the euclidean distance between
a record and the query point it is possible for a record a to have x and ycoordinates each within the query distance of the query point c yet have a itself
lie outside the query circle

then no record in the left subtree can be within the radius in such cases the subtree in question need not be searched potentially saving much time in the average
case the number of nodes that must be visited during a range query is linear on the
number of data records that fall within the query circle
example 137 we will now find all cities in the kd tree of figure 1314
within 25 units of the point 25 65 the search begins with the root node
which contains record a because 40 45 is exactly 25 units from the
search point it will be reported the search procedure then determines
which branches of the tree to take the search circle extends to both the
left and the right of as vertical dividing line so both branches of the
tree must be searched the left subtree is processed first here record b is
checked and found to fall within the search circle because the node storing
b has no children processing of the left subtree is complete processing of
as right subtree now begins the coordinates of record c are checked
and found not to fall within the circle thus it should not be reported
however it is possible that cities within cs subtrees could fall within the
search circle even if c does not as c is at level 1 the discriminator at
this level is the ycoordinate because 65  25  10 no record in cs left
subtree ie records above c could possibly be in the search circle thus
cs left subtree if it had one need not be searched however cities in cs
right subtree could fall within the circle thus search proceeds to the node
containing record d again d is outside the search circle because 25 
25  69 no record in ds right subtree could be within the search circle
thus only ds left subtree need be searched this leads to comparing
record es coordinates against the search circle record e falls outside the
search circle and processing is complete so we see that we only search
subtrees whose rectangles fall within the search circle


$$@@$$PAGE: 476
457

sec 133 spatial data structures

c

a

d

b
e

f

figure 1314 searching in the kd treeof figure 1311 a the kd tree decomposition for a 128  128unit region containing seven data points b the kd tree
for the region of a
 print all points within distance rad of coord
void regionhelpbinnodee root int coord
int rad int discrim const 
if root  null return
 empty tree
 check if record at root is in circle
if incirclerootvalcoord coord rad
cout  rootval  endl  do what is appropriate
int currcoord  rootvalcoord
if currcoorddiscrim  coorddiscrim  rad
regionhelprootleft coord rad discrim1d
if currcoorddiscrim  coorddiscrim  rad
regionhelprootright coord rad discrim1d

figure 1315 the kd tree region search method

figure 1315 shows an implementation for the region search method when
a node is visited function incircle is used to check the euclidean distance
between the nodes record and the query point it is not enough to simply check
that the differences between the x and ycoordinates are each less than the query
distances because the the record could still be outside the search circle as illustrated
by figure 1313
1332

the pr quadtree

in the pointregion quadtree hereafter referred to as the pr quadtree each node
either has exactly four children or is a leaf that is the pr quadtree is a full fourway branching 4ary tree in shape the pr quadtree represents a collection of
data points in two dimensions by decomposing the region containing the data points
into four equal quadrants subquadrants and so on until no leaf node contains more


$$@@$$PAGE: 477
458

chap 13 advanced tree structures

0

127

0
c

nw
a

a

se

ne sw

4045

b
1570

d
c

b
e

f

d
70 10 6950
f
e
558080 90

127

a

b

figure 1316 example of a pr quadtree a a map of data points we define the region to be square with origin at the upperlefthand corner and sides of
length 128 b the pr quadtree for the points in a a also shows the block
decomposition imposed by the pr quadtree for this region

than a single point in other words if a region contains zero or one data points then
it is represented by a pr quadtree consisting of a single leaf node if the region contains more than a single data point then the region is split into four equal quadrants
the corresponding pr quadtree then contains an internal node and four subtrees
each subtree representing a single quadrant of the region which might in turn be
split into subquadrants each internal node of a pr quadtree represents a single
split of the twodimensional region the four quadrants of the region or equivalently the corresponding subtrees are designated in order nw ne sw and se
each quadrant containing more than a single point would in turn be recursively divided into subquadrants until each leaf of the corresponding pr quadtree contains
at most one point
for example consider the region of figure 1316a and the corresponding
pr quadtree in figure 1316b the decomposition process demands a fixed key
range in this example the region is assumed to be of size 128  128 note that the
internal nodes of the pr quadtree are used solely to indicate decomposition of the
region internal nodes do not store data records because the decomposition lines
are predetermined ie keyspace decomposition is used the pr quadtree is a trie
search for a record matching point q in the pr quadtree is straightforward
beginning at the root we continuously branch to the quadrant that contains q until
our search reaches a leaf node if the root is a leaf then just check to see if the
nodes data record matches point q if the root is an internal node proceed to
the child that contains the search coordinate for example the nw quadrant of
figure 1316 contains points whose x and y values each fall in the range 0 to 63


$$@@$$PAGE: 478
459

sec 133 spatial data structures

c
a

a

b

b

a

b

figure 1317 pr quadtree insertion example a the initial pr quadtree containing two data points b the result of inserting point c the block containing a
must be decomposed into four subblocks points a and c would still be in the
same block if only one subdivision takes place so a second decomposition is required to separate them

the ne quadrant contains points whose x value falls in the range 64 to 127 and
whose y value falls in the range 0 to 63 if the roots child is a leaf node then that
child is checked to see if q has been found if the child is another internal node the
search process continues through the tree until a leaf node is found if this leaf node
stores a record whose position matches q then the query is successful otherwise q
is not in the tree
inserting record p into the pr quadtree is performed by first locating the leaf
node that contains the location of p if this leaf node is empty then p is stored
at this leaf if the leaf already contains p or a record with ps coordinates then
a duplicate record should be reported if the leaf node already contains another
record then the node must be repeatedly decomposed until the existing record and
p fall into different leaf nodes figure 1317 shows an example of such an insertion
deleting a record p is performed by first locating the node n of the pr quadtree
that contains p node n is then changed to be empty the next step is to look at ns
three siblings n and its siblings must be merged together to form a single node n 0
if only one point is contained among them this merging process continues until
some level is reached at which at least two points are contained in the subtrees represented by node n 0 and its siblings for example if point c is to be deleted from
the pr quadtree representing figure 1317b the resulting node must be merged
with its siblings and that larger node again merged with its siblings to restore the
pr quadtree to the decomposition of figure 1317a
region search is easily performed with the pr quadtree to locate all points
within radius r of query point q begin at the root if the root is an empty leaf node


$$@@$$PAGE: 479
460

chap 13 advanced tree structures

then no data points are found if the root is a leaf containing a data record then the
location of the data point is examined to determine if it falls within the circle if
the root is an internal node then the process is performed recursively but only on
those subtrees containing some part of the search circle
let us now consider how the structure of the pr quadtree affects the design
of its node representation the pr quadtree is actually a trie as defined in section 131 decomposition takes place at the midpoints for internal nodes regardless of where the data points actually fall the placement of the data points does
determine whether a decomposition for a node takes place but not where the decomposition for the node takes place internal nodes of the pr quadtree are quite
different from leaf nodes in that internal nodes have children leaf nodes do not
and leaf nodes have data fields internal nodes do not thus it is likely to be beneficial to represent internal nodes differently from leaf nodes finally there is the
fact that approximately half of the leaf nodes will contain no data field
another issue to consider is how does a routine traversing the pr quadtree
get the coordinates for the square represented by the current pr quadtree node
one possibility is to store with each node its spatial description such as upperleft
corner and width however this will take a lot of space  perhaps as much as the
space needed for the data records depending on what information is being stored
another possibility is to pass in the coordinates when the recursive call is made
for example consider the search process initially the search visits the root node
of the tree which has origin at 0 0 and whose width is the full size of the space
being covered when the appropriate child is visited it is a simple matter for the
search routine to determine the origin for the child and the width of the square is
simply half that of the parent not only does passing in the size and position information for a node save considerable space but avoiding storing such information
in the nodes enables a good design choice for empty leaf nodes as discussed next
how should we represent empty leaf nodes on average half of the leaf nodes
in a pr quadtree are empty ie do not store a data point one implementation
option is to use a null pointer in internal nodes to represent empty nodes this
will solve the problem of excessive space requirements there is an unfortunate
side effect that using a null pointer requires the pr quadtree processing methods to understand this convention in other words you are breaking encapsulation
on the node representation because the tree now must know things about how the
nodes are implemented this is not too horrible for this particular application because the node class can be considered private to the tree class in which case the
node implementation is completely invisible to the outside world however it is
undesirable if there is another reasonable alternative
fortunately there is a good alternative it is called the flyweight design pattern
in the pr quadtree a flyweight is a single empty leaf node that is reused in all places
where an empty leaf node is needed you simply have all of the internal nodes with


$$@@$$PAGE: 480
sec 133 spatial data structures

461

empty leaf children point to the same node object this node object is created once
at the beginning of the program and is never removed the node class recognizes
from the pointer value that the flyweight is being accessed and acts accordingly
note that when using the flyweight design pattern you cannot store coordinates for the node in the node this is an example of the concept of intrinsic versus
extrinsic state intrinsic state for an object is state information stored in the object if you stored the coordinates for a node in the node object those coordinates
would be intrinsic state extrinsic state is state information about an object stored
elsewhere in the environment such as in global variables or passed to the method
if your recursive calls that process the tree pass in the coordinates for the current
node then the coordinates will be extrinsic state a flyweight can have in its intrinsic state only information that is accurate for all instances of the flyweight clearly
coordinates do not qualify because each empty leaf node has its own location so
if you want to use a flyweight you must pass in coordinates
another design choice is who controls the work the node class or the tree
class for example on an insert operation you could have the tree class control
the flow down the tree looking at querying the nodes to see their type and reacting
accordingly this is the approach used by the bst implementation in section 54
an alternate approach is to have the node class do the work that is you have an
insert method for the nodes if the node is internal it passes the city record to the
appropriate child recursively if the node is a flyweight it replaces itself with a
new leaf node if the node is a full node it replaces itself with a subtree this is
an example of the composite design pattern discussed in section 531 use of the
composite design would be difficult if null pointers are used to represent empty
leaf nodes it turns out that the pr quadtree insert and delete methods are easier to
implement when using the composite design
1333

other point data structures

the differences between the kd tree and the pr quadtree illustrate many of the
design choices encountered when creating spatial data structures the kd tree provides an object space decomposition of the region while the pr quadtree provides
a key space decomposition thus it is a trie the kd tree stores records at all
nodes while the pr quadtree stores records only at the leaf nodes finally the two
trees have different structures the kd tree is a binary tree and need not be full
while the pr quadtree is a full tree with 2d branches in the twodimensional case
22  4 consider the extension of this concept to three dimensions a kd tree for
three dimensions would alternate the discriminator through the x y and z dimensions the threedimensional equivalent of the pr quadtree would be a tree with
23 or eight branches such a tree is called an octree
we can also devise a binary trie based on a key space decomposition in each
dimension or a quadtree that uses the twodimensional equivalent to an object


$$@@$$PAGE: 481
462

chap 13 advanced tree structures

x
c
a

y
x

d

y

a

b

x
b

e

f

c

y

d
e

f

b

a

figure 1318 an example of the bintree a binary tree using key space decomposition and discriminators rotating among the dimensions compare this with
the kd tree of figure 1311 and the pr quadtree of figure 1316

127

0
0

a

c
nw
a
c
b

se
d

ne sw
b

d
e
e

f

f

127
a

b

figure 1319 an example of the point quadtree a 4ary tree using object space
decomposition compare this with the pr quadtree of figure 1311

space decomposition the bintree is a binary trie that uses keyspace decomposition
and alternates discriminators at each level in a manner similar to the kd tree the
bintree for the points of figure 1311 is shown in figure 1318 alternatively we
can use a fourway decomposition of space centered on the data points the tree
resulting from such a decomposition is called a point quadtree the point quadtree
for the data points of figure 1311 is shown in figure 1319


$$@@$$PAGE: 482
sec 134 further reading

1334

463

other spatial data structures

this section has barely scratched the surface of the field of spatial data structures
dozens of distinct spatial data structures have been invented many with variations
and alternate implementations spatial data structures exist for storing many forms
of spatial data other than points the most important distinctions between are the
tree structure binary or not regular decompositions or not and the decomposition
rule used to decide when the data contained within a region is so complex that the
region must be subdivided
one such spatial data structure is the region quadtree for storing images where
the pixel values tend to be blocky such as a map of the countries of the world
the region quadtree uses a fourway regular decomposition scheme similar to the
pr quadtree the decomposition rule is simply to divide any node containing pixels
of more than one color or value
spatial data structures can also be used to store line object rectangle object
or objects of arbitrary shape such as polygons in two dimensions or polyhedra in
three dimensions a simple yet effective data structure for storing rectangles or
arbitrary polygonal shapes can be derived from the pr quadtree pick a threshold
value c and subdivide any region into four quadrants if it contains more than c
objects a special case must be dealt with when more than c object intersect
some of the most interesting developments in spatial data structures have to
do with adapting them for diskbased applications however all such diskbased
implementations boil down to storing the spatial data structure within some variant
on either btrees or hashing

134

further reading

patricia tries and other trie implementations are discussed in information retrieval data structures  algorithms frakes and baezayates eds fby92
see knuth knu97 for a discussion of the avl tree for further reading on
splay trees see selfadjusting binary search by sleator and tarjan st85
the world of spatial data structures is rich and rapidly evolving for a good
introduction see foundations of multidimensional and metric data structures by
hanan samet sam06 this is also the best reference for more information on
the pr quadtree the kd tree was invented by john louis bentley for further
information on the kd tree in addition to sam06 see ben75 for information
on using a quadtree to store arbitrary polygonal objects see sh92
for a discussion on the relative space requirements for twoway versus multiway branching see a generalized comparison of quadtree and bintree storage
requirements by shaffer juvvadi and heath sjh93
closely related to spatial data structures are data structures for storing multidimensional data which might not necessarily be spatial in nature a popular


$$@@$$PAGE: 483
464

chap 13 advanced tree structures

data structure for storing such data is the rtree which was originally proposed by
guttman gut84

135

exercises

131 show the binary trie as illustrated by figure 131 for the following collection of values 42 12 100 10 50 31 7 11 99
132 show the pat trie as illustrated by figure 133 for the following collection
of values 42 12 100 10 50 31 7 11 99
133 write the insertion routine for a binary trie as shown in figure 131
134 write the deletion routine for a binary trie as shown in figure 131
135 a show the result including appropriate rotations of inserting the value
39 into the avl tree on the left in figure 134
b show the result including appropriate rotations of inserting the value
300 into the avl tree on the left in figure 134
c show the result including appropriate rotations of inserting the value
50 into the avl tree on the left in figure 134
d show the result including appropriate rotations of inserting the value
1 into the avl tree on the left in figure 134
136 show the splay tree that results from searching for value 75 in the splay tree
of figure 1310d
137 show the splay tree that results from searching for value 18 in the splay tree
of figure 1310d
138 some applications do not permit storing two records with duplicate key values in such a case an attempt to insert a duplicatekeyed record into a tree
structure such as a splay tree should result in a failure on insert what is
the appropriate action to take in a splay tree implementation when the insert
routine is called with a duplicatekeyed record
139 show the result of deleting point a from the kd tree of figure 1311
1310 a show the result of building a kd tree from the following points inserted in the order given a 20 20 b 10 30 c 25 50 d 35
25 e 30 45 f 30 35 g 55 40 h 45 35 i 50 30
b show the result of deleting point a from the tree you built in part a
1311 a show the result of deleting f from the pr quadtree of figure 1316
b show the result of deleting records e and f from the pr quadtree of
figure 1316
1312 a show the result of building a pr quadtree from the following points
inserted in the order given assume the tree is representing a space of
64 by 64 units a 20 20 b 10 30 c 25 50 d 35 25 e 30
45 f 30 35 g 45 25 h 45 30 i 50 30
b show the result of deleting point c from the tree you built in part a


$$@@$$PAGE: 484
465

sec 136 projects

1313
1314

1315

1316

1317

c show the result of deleting point f from the resulting tree in part b
on average how many leaf nodes of a pr quadtree will typically be empty
explain why
when performing a region search on a pr quadtree we need only search
those subtrees of an internal node whose corresponding square falls within
the query circle this is most easily computed by comparing the x and y
ranges of the query circle against the x and y ranges of the square corresponding to the subtree however as illustrated by figure 1313 the x and
y ranges might overlap without the circle actually intersecting the square
write a function that accurately determines if a circle and a square intersect
a show the result of building a bintree from the following points inserted
in the order given assume the tree is representing a space of 64 by 64
units a 20 20 b 10 30 c 25 50 d 35 25 e 30 45 f 30
35 g 45 25 h 45 30 i 50 30
b show the result of deleting point c from the tree you built in part a
c show the result of deleting point f from the resulting tree in part b
compare the trees constructed for exercises 12 and 15 in terms of the number
of internal nodes full leaf nodes empty leaf nodes and total depths of the
two trees
show the result of building a point quadtree from the following points inserted in the order given assume the tree is representing a space of 64 by
64 units a 20 20 b 10 30 c 25 50 d 35 25 e 30 45 f 31
35 g 45 26 h 44 30 i 50 30

136

projects

131 use the trie data structure to devise a program to sort variablelength strings
the programs running time should be proportional to the total number of
letters in all of the strings note that some strings might be very long while
most are short
132 define the set of suffix strings for a string s to be s s without its first character s without its first two characters and so on for example the complete
set of suffix strings for hello would be
hello ello llo lo o
a suffix tree is a pat trie that contains all of the suffix strings for a given
string and associates each suffix with the complete string the advantage
of a suffix tree is that it allows a search for strings using wildcards for
example the search key th means to find all strings with th as the
first two characters this can easily be done with a regular trie searching
for th is not efficient in a regular trie but it is efficient in a suffix tree


$$@@$$PAGE: 485
466

133

134

135

136

137
138

139

1310

chap 13 advanced tree structures

implement the suffix tree for a dictionary of words or phrases with support
for wildcard search
revise the bst class of section 54 to use the avl tree rotations your new
implementation should not modify the original bst class adt compare
your avl tree against an implementation of the standard bst over a wide
variety of input data under what conditions does the splay tree actually save
time
revise the bst class of section 54 to use the splay tree rotations your new
implementation should not modify the original bst class adt compare
your splay tree against an implementation of the standard bst over a wide
variety of input data under what conditions does the splay tree actually save
time
implement a city database using the kd tree each database record contains
the name of the city a string of arbitrary length and the coordinates of the
city expressed as integer x and ycoordinates your database should allow
records to be inserted deleted by name or coordinate and searched by name
or coordinate you should also support region queries that is a request to
print all records within a given distance of a specified point
implement a city database using the pr quadtree each database record contains the name of the city a string of arbitrary length and the coordinates
of the city expressed as integer x and ycoordinates your database should
allow records to be inserted deleted by name or coordinate and searched by
name or coordinate you should also support region queries that is a request
to print all records within a given distance of a specified point
implement and test the pr quadtree using the composite design to implement the insert search and delete operations
implement a city database using the bintree each database record contains
the name of the city a string of arbitrary length and the coordinates of the
city expressed as integer x and ycoordinates your database should allow
records to be inserted deleted by name or coordinate and searched by name
or coordinate you should also support region queries that is a request to
print all records within a given distance of a specified point
implement a city database using the point quadtree each database record
contains the name of the city a string of arbitrary length and the coordinates
of the city expressed as integer x and ycoordinates your database should
allow records to be inserted deleted by name or coordinate and searched by
name or coordinate you should also support region queries that is a request
to print all records within a given distance of a specified point
use the pr quadtree to implement an efficient solution to problem 65 that
is store the set of points in a pr quadtree for each point the pr quadtree
is used to find those points within distance d that should be equivalenced
what is the asymptotic complexity of this solution


$$@@$$PAGE: 486
sec 136 projects

467

1311 select any two of the point representations described in this chapter ie the
kd tree the pr quadtree the bintree and the point quadtree implement
your two choices and compare them over a wide range of data sets describe
which is easier to implement which appears to be more space efficient and
which appears to be more time efficient
1312 implement a representation for a collection of two dimensional rectangles
using a quadtree based on regular decomposition assume that the space
being represented is a square whose width and height are some power of
two rectangles are assumed to have integer coordinates and integer width
and height pick some value c and use as a decomposition rule that a region
is subdivided into four equalsized regions whenever it contains more that c
rectangles a special case occurs if all of these rectangles intersect at some
point within the current region because decomposing such a node would
never reach termination in this situation the node simply stores pointers
to more than c rectangles try your representation on data sets of rectangles
with varying values of c


$$@@$$PAGE: 487

$$@@$$PAGE: 488
part v
theory of algorithms

469


$$@@$$PAGE: 489

$$@@$$PAGE: 490
14
analysis techniques

often it is easy to invent an equation to model the behavior of an algorithm or
data structure often it is easy to derive a closedform solution for the equation
should it contain a recurrence or summation but sometimes analysis proves more
difficult it may take a clever insight to derive the right model such as the snowplow argument for analyzing the average run length resulting from replacement
selection section 852 in this example once the snowplow argument is understood the resulting equations follow naturally sometimes developing the model
is straightforward but analyzing the resulting equations is not an example is the
averagecase analysis for quicksort the equation given in section 75 simply enumerates all possible cases for the pivot position summing corresponding costs for
the recursive calls to quicksort however deriving a closedform solution for the
resulting recurrence relation is not as easy
many analyses of iterative algorithms use a summation to model the cost of a
loop techniques for finding closedform solutions to summations are presented in
section 141 the cost for many algorithms based on recursion are best modeled
by recurrence relations a discussion of techniques for solving recurrences is provided in section 142 these sections build on the introduction to summations and
recurrences provided in section 24 so the reader should already be familiar with
that material
section 143 provides an introduction to the topic of amortized analysis amortized analysis deals with the cost of a series of operations perhaps a single
operation in the series has high cost but as a result the cost of the remaining operations is limited amortized analysis has been used successfully to analyze several
of the algorithms presented in previous sections including the cost of a series of
unionfind operations section 62 the cost of partition in quicksort section 75 the cost of a series of splay tree operations section 132 and the cost of
a series of operations on selforganizing lists section 92 section 143 discusses
the topic in more detail
471


$$@@$$PAGE: 491
472

141

chap 14 analysis techniques

summation techniques

consider the following simple summation
n
x

i

i1

in section 263 it was proved by induction that this summation has the wellknown
closed form nn  12 but while induction is a good technique for proving that
a proposed closedform expression is correct how do we find a candidate closedform expression to test in the first place let us try to think through this problem
from first principles as though we had never seen it before
a good place to begin analyzing a summation it is to give an estimate of its
value for a given n observe that the biggest term for this summation is n and
there are n terms being summed up so the total must be less than n2  actually
most terms are much less than n and the sizes of the terms grows linearly if we
were to draw a picture with bars for the size of the terms their heights would form a
line and we could enclose them in a box n units wide and n units high it is easy to
see from this that a closer estimate for the summation is about n2 2 having this
estimate in hand helps us when trying to determine an exact closedform solution
because we will hopefully recognize if our proposed solution is badly wrong
let us now consider some ways that we might hit upon an exact equation for
the closed form solution to this summation one particularly clever approach we
can take is to observe that we can pair up the first and last terms the second and
n  1th terms and so on each pair sums to n  1 the number of pairs is n2
thus the solution is nn  12 this is pretty and there is no doubt about it being
correct the problem is that it is not a useful technique for solving many other
summations
now let us try to do something a bit more general we already recognized
that because the largest term is n and there are n terms the summation is less
than n2  if we are lucky the closed form solution is a polynomial using that as
a working assumption we can invoke a technique called guessandtest we will
guess that the closedform solution for this summation is a polynomial of the form
c1 n2  c2 n  c3 for some constants c1  c2  and c3  if this is true then we can plug
in the answers to small cases of the summation to solve for the coefficients for
this example substituting 0 1 and 2 for n leads to three simultaneous equations
because the summation when n  0 is just 0 c3 must be 0 for n  1 and n  2
we get the two equations
c1  c2  1
4c1  2c2  3


$$@@$$PAGE: 492
473

sec 141 summation techniques

which in turn yield c1  12 and c2  12 thus if the closedform solution for
the summation is a polynomial it can only be
12n2  12n  0
which is more commonly written
nn  1

2
at this point we still must do the test part of the guessandtest approach we
can use an induction proof to verify whether our candidate closedform solution is
correct in this case it is indeed correct as shown by example 211 the induction proof is necessary because our initial assumption that the solution is a simple
polynomial could be wrong for example it might have been that the true solution
includes a logarithmic term such as c1 n2  c2 n log n the process shown here is
essentially fitting a curve to a fixed number of points because there is always an
ndegree polynomial that fits n  1 points we have not done enough work to be
sure that we to know the true equation without the induction proof
guessandtest is useful whenever the solution is ap
polynomial expression in
n
2
particular
similar
reasoning
can
be
used
to
solve
for
i1 i  or more generally
pn c
i1 i for c any positive integer why is this not a universal approach to solving
summations because many summations do not have a polynomial as their closed
form solution
a more general approach is based on the subtractandguess or divideandguess strategies one form of subtractandguess is known as the shifting method
the shifting method subtracts the summation from a variation on the summation
the variation selected for the subtraction should be one that makes most of the
terms cancel out to solve sum f  we pick a known function g and find a pattern in
terms of f n  gn or f ngn
p
example 141 find the closed form solution for ni1 i using the divideandguess approach we will try two example functions to illustrate the
divideandguess method dividing by n and dividing by f n  1 our
goal is to find patterns that we can use to guess a closedform expression as
our candidate for testing with an induction proof to aid us in finding such
patterns we can construct a table showing the first few numbers of each
function and the result of dividing one by the other as follows
1
n
f n
1
n
1
f nn 22
f n1
0
f nf n1

2
3
2
32
1
31

3
6
3
42
3
42

4
10
4
52
6
53

5
15
5
62
10
64

6
21
6
72
15
75

7
28
7
82
21
86

8
36
8
92
28
97

9
46
9
102
36
108

10
57
10
112
46
119


$$@@$$PAGE: 493
474

chap 14 analysis techniques

dividing by both n and f n  1 happen to give us useful patterns to
f n
n1
work with f n
 n1
n
2  and f n1  n1  of course lots of other
guesses for function g do not work for example f n  n  f n 
1 knowing that f n  f n  1  n is not useful for determining the
closed form solution to this summation or consider f n  f n  1  n
again knowing that f n  f n  1  n is not useful finding the right
combination of equations can be like finding a needle in a haystack
in our first example we can see directly what the closedform solution
n1
should be since f n
n  2  obviously f n  nn  12
dividing f n by f n  1 does not give so obvious a result but it
provides another useful illustration
f n
n1

f n  1
n1
f nn  1  n  1f n  1
f nn  1  n  1f n  n
nf n  f n  nf n  f n  n2  n
2f n  n2  n  nn  1
nn  1
f n 
2
once again we still do not have a proof that f n  nn  12 why
because we did not prove that f nn  n  12 nor that f nf n 
1  n  1n  1 we merely hypothesized patterns from looking at a
few terms fortunately it is easy to check our hypothesis with induction

example 142 solve the summation
n
x

12i 

i1

we will begin by writing out a table listing the first few values of the summation to see if we can detect a pattern
n

1

2

3

4

5

6

f n

1
2
1
2

3
4
1
4

7
8
1
8

15
16
1
16

31
32
1
32

63
64
1
64

1  f n


$$@@$$PAGE: 494
475

sec 141 summation techniques

by direct inspection of the second line of the table we might recognize the
n
pattern f n  2 21
n  a simple induction proof can then prove that this
always holds true alternatively consider if we hadnt noticed the pattern
for the form of f n we might observe that f n appears to be reaching
an asymptote at one in which case we might consider looking at the difference between f n and the expected asymptote this result is shown in
the last line of the table which has a clear pattern since the ith entry is of
12i  from this we can easily deduce a guess that f n  1  21n  again
a simple induction proof will verify the guess

example 143 solve the summation
f n 

n
x

ari  a  ar  ar2      arn 

i0

this is called a geometric series our goal is to find some function gn
such that the difference between f n and gn one from the other leaves
us with an easily manipulated equation because the difference between
consecutive terms of the summation is a factor of r we can shift terms if
we multiply the entire expression by r
rf n  r

n
x

ari  ar  ar2  ar3      arn1 

i0

we can now subtract the one equation from the other as follows
f n  rf n  a  ar  ar2  ar3      arn
 ar  ar2  ar3      arn   arn1 
the result leaves only the end terms
f n  rf n 

n
x

ari  r

i0

1  rf n  a  arn1 
thus we get the result
f n 
where r 6 1

a  arn1
1r

n
x
i0

ari 


$$@@$$PAGE: 495
476

chap 14 analysis techniques

example 144 for our second example of the shifting method we solve
f n 

n
x

i2i  1  21  2  22  3  23      n  2n 

i1

we can achieve our goal if we multiply by two
n
x
2f n  2
i2i  1  22  2  23  3  24      n  1  2n  n  2n1 
i1

the ith term of 2f n is i  2i1  while the i  1th term of f n is
i  1  2i1  subtracting one expression from the other yields the summation of 2i and a few noncanceled terms
n
n
x
x
2f n  f n  2
i2i 
i2i


i1
n
x

i1
n
x

i2i1 

i1

i2i 

i1

shift is value in the second summation substituting i  1 for i
n1
n1
x
x
 n2n1 
i2i1 
i  12i1 
i0

i0

break the second summation into two parts
n1
n1
n1
x
x
x
n1
i1
i1
 n2

i2

i2

2i1 
i0

cancel like terms
 n2

n1



n1
x

i0

i0

2i1 

i0

again shift is value in the summation substituting i for i  1
n
x
 n2n1 
2i 
i1

replace the new summation with a solution that
 we already know
 n2n1  2n1  2 
finally reorganize the equation
 n  12n1  2


$$@@$$PAGE: 496
477

sec 142 recurrence relations

142

recurrence relations

recurrence relations are often used to model the cost of recursive functions for
example the standard mergesort section 74 takes a list of size n splits it in half
performs mergesort on each half and finally merges the two sublists in n steps
the cost for this can be modeled as
tn  2tn2  n
in other words the cost of the algorithm on input of size n is two times the cost for
input of size n2 due to the two recursive calls to mergesort plus n the time to
merge the sublists together again
there are many approaches to solving recurrence relations and we briefly consider three here the first is an estimation technique guess the upper and lower
bounds for the recurrence use induction to prove the bounds and tighten as required the second approach is to expand the recurrence to convert it to a summation and then use summation techniques the third approach is to take advantage
of already proven theorems when the recurrence is of a suitable form in particular typical divide and conquer algorithms such as mergesort yield recurrences of a
form that fits a pattern for which we have a ready solution
1421

estimating upper and lower bounds

the first approach to solving recurrences is to guess the answer and then attempt
to prove it correct if a correct upper or lower bound estimate is given an easy
induction proof will verify this fact if the proof is successful then try to tighten
the bound if the induction proof fails then loosen the bound and try again once
the upper and lower bounds match you are finished this is a useful technique
when you are only looking for asymptotic complexities when seeking a precise
closedform solution ie you seek the constants for the expression this method
will probably be too much work
example 145 use the guessing technique to find the asymptotic bounds
for mergesort whose running time is described by the equation
tn  2tn2  n

t2  1

we begin by guessing that this recurrence has an upper bound in on2  to
be more precise assume that
tn  n2 
we prove this guess is correct by induction in this proof we assume that
n is a power of two to make the calculations easy for the base case


$$@@$$PAGE: 497
478

chap 14 analysis techniques

t2  1  22  for the induction step we need to show that tn  n2
implies that t2n  2n2 for n  2n  n  1 the induction hypothesis
is
ti  i2  for all i  n
it follows that
t2n  2tn  2n  2n2  2n  4n2  2n2
which is what we wanted to prove thus tn is in on2 
is on2  a good estimate in the nexttolast step we went from n2 2n
to the much larger 4n2  this suggests that on2  is a high estimate if we
guess something smaller such as tn  cn for some constant c it should
be clear that this cannot work because c2n  2cn and there is no room for
the extra n cost to join the two pieces together thus the true cost must be
somewhere between cn and n2 
let us now try tn  n log n for the base case the definition of the
recurrence sets t2  1  2  log 2  2 assume induction hypothesis
that tn  n log n then
t2n  2tn  2n  2n log n  2n  2nlog n  1  2n log 2n
which is what we seek to prove in similar fashion we can prove that tn
is in n log n thus tn is also n log n

example 146 we know that the factorial function grows exponentially
how does it compare to 2n  to nn  do they all grow equally fast in an
asymptotic sense we can begin by looking at a few initial terms
n
n
2n
nn

1
1
2
1

2
2
4
4

3
6
8
9

4
24
16
256

5
6
7
8
9
120
720
5040
40320
362880
32
64
128
256
512
3125 46656 823543 16777216 387420489

we can also look at these functions in terms of their recurrences

1
n1
n 
nn  1 n  1

n

2 



2
n1
22n1  n  1


$$@@$$PAGE: 498
479

sec 142 recurrence relations

nn 



n
n1
nnn1  n  1

at this point our intuition should be telling us pretty clearly the relative
growth rates of these three functions but how do we prove formally which
grows the fastest and how do we decide if the differences are significant
in an asymptotic sense or just constant factor differences
we can use logarithms to help us get an idea about the relative growth
rates of these functions clearly log 2n  n equally clearly log nn 
n log n we can easily see from this that 2n is onn  that is nn grows
asymptotically faster than 2n 
how does n fit into this we can again take advantage of logarithms
obviously n  nn  so we know that log n is on log n but what about
a lower bound for the factorial function consider the following
n





n  n  1     

n
n
   1      2  1
2
2

n n
n
     1    1  1
2
2
2
n n2
 
2

therefore

n
n
n
log n  log n2    log 
2
2
2
in other words log n is in n log n thus log n  n log n
note that this does not mean that n  nn  because log n2 
2 log n it follows that log n  log n2  but n 6 n2  the log function often works as a flattener when dealing with asymptotics that is
whenever log f n is in olog gn we know that f n is in ogn
but knowing that log f n  log gn does not necessarily mean that
f n  gn

example 147 what is the growth rate of the fibonacci sequence we
define the fibonacci sequence as f n  f n  1  f n  2 for n  2
f 0  f 1  1
in this case it is useful to compare the ratio of f n to f n  1 the
following table shows the first few values
n
1
f n
1
f nf n  1 1

2
2
2

3
3
15

4
5
1666

5
6
7
8
13
21
1625 1615 1619


$$@@$$PAGE: 499
480

chap 14 analysis techniques

if we continue for more terms the ratio appears to converge on a value
slightly greater then 1618 assuming f nf n  1 really does converge
to a fixed value as n grows we can determine what that value must be
f n
f n  1 f n  2


x1
f n  2
f n  2 f n  2
for some value x this follows from the fact that f n  f n  1 
f n  2 we divide by f n  2 to make the second term go away and
we also get something useful in the first term remember that the goal of
such manipulations is to give us an equation that relates f n to something
without recursive calls
for large n we also observe that
f n
f n f n  1

 x2
f n  2
f n  1 f n  2
as n gets big this comes from multiplying f nf n  2 by f n 
1f n  1 and rearranging
if x exists then x2  x  1  0 using the quadratic equation the only
solution greater than one is

1 5
x
 1618
2
this expression also has the name  what does this say about the growth
rate of the fibonacci sequence it is exponential with f n  n 
more precisely f n converges to
n  1  n


5

1422

expanding recurrences

estimating bounds is effective if you only need an approximation to the answer
more precise techniques are required to find an exact solution one approach is
called expanding the recurrence in this method the smaller terms on the right
side of the equation are in turn replaced by their definition this is the expanding
step these terms are again expanded and so on until a full series with no recurrence results this yields a summation and techniques for solving summations can
then be used a couple of simple expansions were shown in section 24 a more
complex example is given below


$$@@$$PAGE: 500
481

sec 142 recurrence relations

example 148 find the solution for
tn  2tn2  5n2 

t1  7

for simplicity we assume that n is a power of two so we will rewrite it as
n  2k  this recurrence can be expanded as follows
tn  2tn2  5n2
 22tn4  5n22   5n2
 222tn8  5n42   5n22   5n2
 n 2
 n 2
 2k t1  2k1  5 k1      2  5
 5n2 
2
2
this last expression can best be represented by a summation as follows
7n  5

k1
x

n2 2i

i0

 7n  5n2

k1
x

12i 

i0

from equation 26 we have


 7n  5n2 2  12k1
 7n  5n2 2  2n
 7n  10n2  10n
 10n2  3n
this is the exact solution to the recurrence for n a power of two at this
point we should use a simple induction proof to verify that our solution is
indeed correct

example 149 our next example models the cost of the algorithm to build
a heap recall from section 55 that to build a heap we first heapify the
two subheaps then push down the root to its proper position the cost is
f n  2f n2  2 log n
let us find a closed form solution for this recurrence we can expand
the recurrence a few times to see that


$$@@$$PAGE: 501
482

chap 14 analysis techniques

f n  2f n2  2 log n
 22f n4  2 log n2  2 log n
 222f n8  2 log n4  2 log n2  2 log n
we can deduce from this expansion that this recurrence is equivalent to
following summation and its derivation

f n 

log
n1
x

2i1 logn2i 

i0

 2

log
n1
x

2i log n  i

i0

 2 log n

log
n1
x

i

2 4

i0

log
n1
x

i2i1

i0

 2n log n  2 log n  2n log n  4n  4
 4n  2 log n  4

1423

divide and conquer recurrences

the third approach to solving recurrences is to take advantage of known theorems
that provide the solution for classes of recurrences of particular practical use is
a theorem that gives the answer for a class known as divide and conquer recurrences these have the form
tn  atnb  cnk 

t1  c

where a b c and k are constants in general this recurrence describes a problem
of size n divided into a subproblems of size nb while cnk is the amount of work
necessary to combine the partial solutions mergesort is an example of a divide and
conquer algorithm and its recurrence fits this form so does binary search we use
the method of expanding recurrences to derive the general solution for any divide
and conquer recurrence assuming that n  bm 
tn  atnb  cnk
 aatnb2   cnbk   cnk
 aaatnb3   cnb2 k   cnbk   cnk


$$@@$$PAGE: 502
483

sec 142 recurrence relations

 am t1  am1 cnbm1 k      acnbk  cnk
 am c  am1 cnbm1 k      acnbk  cnk
m
x
 c
ami bik
i0

 cam

m
x

bk ai 

i0

note that
am  alogb n  nlogb a 

141

the summation is a geometric series whose sum depends on the ratio r  bk a
there are three cases
1 r  1 from equation 24
m
x

ri  11  r a constant

i0

thus
tn  am   nlogb a 
2 r  1 because r  bk a we know that a  bk  from the definition
of logarithms it follows immediately that k  logb a we also note from
equation 141 that m  logb n thus
m
x

r  m  1  logb n  1

i0

because am  n logb a  nk  we have
tn  nlogb a log n  nk log n
3 r  1 from equation 25
m
x
i0

r

rm1  1
 rm 
r1

thus
tn  am rm   am bk am   bkm   nk 
we can summarize the above derivation as the following theorem sometimes
referred to as the master theorem


$$@@$$PAGE: 503
484

chap 14 analysis techniques

theorem 141 the master theorem for any recurrence relation of the form
tn  atnb  cnk  t1  c the following relationships hold

if a  bk
 nlogb a 
k
n log n if a  bk
tn 

nk 
if a  bk 
this theorem may be applied whenever appropriate rather than rederiving the
solution for the recurrence
example 1410 apply the master theorem to solve
tn  3tn5  8n2 
because a  3 b  5 c  8 and k  2 we find that 3  52  applying
case 3 of the theorem tn  n2 

example 1411 use the master theorem to solve the recurrence relation
for mergesort
tn  2tn2  n

t1  1

because a  2 b  2 c  1 and k  1 we find that 2  21  applying
case 2 of the theorem tn  n log n

1424

averagecase analysis of quicksort

in section 75 we determined that the averagecase analysis of quicksort had the
following recurrence
n1

tn  cn 

1x
tk  tn  1  k
n

t0  t1  c

k0

the cn term is an upper bound on the findpivot and partition steps this
equation comes from assuming that the partitioning element is equally likely to
occur in any position k it can be simplified by observing that the two recurrence
terms tk and tn  1  k are equivalent because one simply counts up from
t 0 to t n  1 while the other counts down from t n  1 to t 0 this yields
n1

2x
tn  cn 
tk
n
k0


$$@@$$PAGE: 504
485

sec 142 recurrence relations

this form is known as a recurrence with full history the key to solving such a
recurrence is to cancel out the summation terms the shifting method for summations provides a way to do this multiply both sides by n and subtract the result
from the formula for ntn  1
ntn  cn2  2

n1
x

tk

k1

n  1tn  1  cn  12  2

n
x

tk

k1

subtracting ntn from both sides yields
n  1tn  1  ntn  cn  12  cn2  2tn
n  1tn  1  ntn  c2n  1  2tn
n  1tn  1  c2n  1  n  2tn
c2n  1 n  2

tn
tn  1 
n1
n1
at this point we have eliminated the summation and can now use our normal methods for solving recurrences to get a closedform solution note that c2n1
n1  2c
so we can simplify the result expanding the recurrence we get
n2
tn
n1


n2
n1
2c 
2c 
tn  1
n1
n



n2
n1
n
tn  2
2c 
2c 
2c 
n1
n
n1


n2
4
3
2c      2c  t1
2c 
n1
3
2


n2 n2n1
n2n1
3
2c 1 

  

n1 n1 n
n1 n
2



1
1
1
2c 1  n  2
   
n1 n
2
2c  2cn  2 hn1  1

tn  1  2c 







for hn1  the harmonic series from equation 210 hn1  log n so the
final solution is n log n


$$@@$$PAGE: 505
486

143

chap 14 analysis techniques

amortized analysis

this section presents the concept of amortized analysis which is the analysis for
a series of operations taken as a whole in particular amortized analysis allows us
to deal with the situation where the worstcase cost for n operations is less than
n times the worstcase cost of any one operation rather than focusing on the individual cost of each operation independently and summing them amortized analysis
looks at the cost of the entire series and charges each individual operation with a
share of the total cost
we can apply the technique of amortized analysis in the case of a series of sequential searches in an unsorted array for n random searches the averagecase
cost for each search is n2 and so the expected total cost for the series is n2 2
unfortunately in the worst case all of the searches would be to the last item in the
array in this case each search costs n for a total worstcase cost of n2  compare
this to the cost for a series of n searches such that each item in the array is searched
for precisely once in this situation some of the searches must be expensive but
also some searches must be cheap the total number
in the best avpn of searches
2
erage and worst case for this problem must be ii i  n 2 this is a factor
of two better than the more pessimistic analysis that charges each operation in the
series with its worstcase cost
as another example of amortized analysis consider the process of incrementing a binary counter the algorithm is to move from the lowerorder rightmost
bit toward the highorder leftmost bit changing 1s to 0s until the first 0 is encountered this 0 is changed to a 1 and the increment operation is done below
is c code to implement the increment operation assuming that a binary number
of length n is stored in array a of length n
for i0 in  ai  1 i
ai  0
if i  n
ai  1

if we count from 0 through 2n  1 requiring a counter with at least n bits
what is the average cost for an increment operation in terms of the number of bits
processed naive worstcase analysis says that if all n bits are 1 except for the
highorder bit then n bits need to be processed thus if there are 2n increments
then the cost is n2n  however this is much too high because it is rare for so many
bits to be processed in fact half of the time the loworder bit is 0 and so only
that bit is processed one quarter of the time the loworder two bits are 01 and
so only the loworder two bits are processed another way to view this is that the
loworder bit is always flipped the bit to its left is flipped half the time the next
bit one quarter of the time and so on we can capture this with the summation


$$@@$$PAGE: 506
487

sec 143 amortized analysis

charging costs to bits going from right to left
n1
x
i0

1
 2
2i

in other words the average number of bits flipped on each increment is 2 leading
to a total cost of only 2  2n for a series of 2n increments
a useful concept for amortized analysis is illustrated by a simple variation on
the stack data structure where the pop function is slightly modified to take a second parameter k indicating that k pop operations are to be performed this revised
pop function called multipop might look as follows
 pop k elements from stack
void multipopint k

the local worstcase analysis for multipop is n for n elements in the
stack thus if there are m1 calls to push and m2 calls to multipop then the
naive worstcase cost for the series of operation is m1  m2  n  m1  m2  m1 
this analysis is unreasonably pessimistic clearly it is not really possible to pop
m1 elements each time multipop is called analysis that focuses on single operations cannot deal with this global limit and so we turn to amortized analysis to
model the entire series of operations
the key to an amortized analysis of this problem lies in the concept of potential at any given time a certain number of items may be on the stack the cost for
multipop can be no more than this number of items each call to push places
another item on the stack which can be removed by only a single multipop operation thus each call to push raises the potential of the stack by one item the
sum of costs for all calls to multipop can never be more than the total potential of
the stack aside from a constant time cost associated with each call to multipop
itself
the amortized cost for any series of push and multipop operations is the
sum of three costs first each of the push operations takes constant time second
each multipop operation takes a constant time in overhead regardless of the
number of items popped on that call finally we count the sum of the potentials
expended by all multipop operations which is at most m1  the number of push
operations this total cost can therefore be expressed as
m1  m2  m1   m1  m2 
a similar argument was used in our analysis for the partition function in the
quicksort algorithm section 75 while on any given pass through the while
loop the left or right pointers might move all the way through the remainder of the


$$@@$$PAGE: 507
488

chap 14 analysis techniques

partition doing so would reduce the number of times that the while loop can be
further executed
our final example uses amortized analysis to prove a relationship between the
cost of the movetofront selforganizing list heuristic from section 92 and the cost
for the optimal static ordering of the list
recall that for a series of search operations the minimum cost for a static
list results when the list is sorted by frequency of access to its records this is
the optimal ordering for the records if we never allow the positions of records to
change because the mostfrequently accessed record is first and thus has least
cost followed by the next most frequently accessed record and so on
theorem 142 the total number of comparisons required by any series s of n or
more searches on a selforganizing list of length n using the movetofront heuristic
is never more than twice the total number of comparisons required when series s is
applied to the list stored in its optimal static order
proof each comparison of the search key with a record in the list is either successful or unsuccessful for m searches there must be exactly m successful comparisons for both the selforganizing list and the static list the total number of
unsuccessful comparisons in the selforganizing list is the sum over all pairs of
distinct keys of the number of unsuccessful comparisons made between that pair
consider a particular pair of keys a and b for any sequence of searches s
the total number of unsuccessful comparisons between a and b is identical to the
number of comparisons between a and b required for the subsequence of s made up
only of searches for a or b call this subsequence sab  in other words including
searches for other keys does not affect the relative position of a and b and so does
not affect the relative contribution to the total cost of the unsuccessful comparisons
between a and b
the number of unsuccessful comparisons between a and b made by the movetofront heuristic on subsequence sab is at most twice the number of unsuccessful
comparisons between a and b required when sab is applied to the optimal static
ordering for the list to see this assume that sab contains i as and j bs with i  j
under the optimal static ordering i unsuccessful comparisons are required because
b must appear before a in the list because its access frequency is higher movetofront will yield an unsuccessful comparison whenever the request sequence changes
from a to b or from b to a the total number of such changes possible is 2i because
each change involves an a and each a can be part of at most two changes
because the total number of unsuccessful comparisons required by movetofront for any given pair of keys is at most twice that required by the optimal static
ordering the total number of unsuccessful comparisons required by movetofront
for all pairs of keys is also at most twice as high because the number of successful


$$@@$$PAGE: 508
489

sec 144 further reading

comparisons is the same for both methods the total number of comparisons required by movetofront is less than twice the number of comparisons required by
the optimal static ordering
2

144

further reading

a good introduction to solving recurrence relations appears in applied combinatorics by fred s roberts rob84 for a more advanced treatment see concrete
mathematics by graham knuth and patashnik gkp94
cormen leiserson and rivest provide a good discussion on various methods
for performing amortized analysis in introduction to algorithms clrs09 for
an amortized analysis that the splay tree requires m log n time to perform a series
of m operations on n nodes when m  n see selfadjusting binary search
trees by sleator and tarjan st85 the proof for theorem 142 comes from
amortized analysis of selforganizing sequential search heuristics by bentley
and mcgeoch bm85

145

exercises

141 use the technique of guessing a polynomial and deriving the coefficients to
solve the summation
n
x
i2 
i1

142 use the technique of guessing a polynomial and deriving the coefficients to
solve the summation
n
x
i3 
i1

143 find and prove correct a closedform solution for
b
x

i2 

ia

144 use subtractandguess or divideandguess to find the closed form solution
for the following summation you must first find a pattern from which to
deduce a potential closed form solution and then prove that the proposed
solution is correct
n
x
i1

i2i


$$@@$$PAGE: 509
490

chap 14 analysis techniques

145 use the shifting method to solve the summation
n
x

i2 

i1

146 use the shifting method to solve the summation
n
x

2i 

i1

147 use the shifting method to solve the summation
n
x

i2ni 

i1

148 consider the following code fragment
sum  0 inc  0
for i1 in i
for j1 ji j 
sum  sum  inc
inc


149

1410
1411
1412
1413

a determine a summation that defines the final value for variable sum as
a function of n
b determine a closedform solution for your summation
a chocolate company decides to promote its chocolate bars by including a
coupon with each bar a bar costs a dollar and with c coupons you get a free
bar so depending on the value of c you get more than one bar of chocolate
for a dollar when considering the value of the coupons how much chocolate
is a dollar worth as a function of c
write and solve a recurrence relation to compute the number of times fibr is
called in the fibr function of exercise 211
give and prove the closedform solution for the recurrence relation tn 
tn  1  1 t1  1
give and prove the closedform solution for the recurrence relation tn 
tn  1  c t1  c
prove by induction that the closedform solution for the recurrence relation
tn  2tn2  n
is in n log n

t2  1


$$@@$$PAGE: 510
491

sec 145 exercises

1414 for the following recurrence give a closedform solution you should not
give an exact solution but only an asymptotic solution ie using  notation you may assume that n is a power of 2 prove that your answer is
correct
tn  tn2 



n for n  1

t1  1

1415 using the technique of expanding the recurrence find the exact closedform
solution for the recurrence relation
tn  2tn2  n

t2  2

you may assume that n is a power of 2
1416 section 55 provides an asymptotic analysis for the worstcase cost of function buildheap give an exact worstcase analysis for buildheap
1417 for each of the following recurrences find and then prove using induction
an exact closedform solution when convenient you may assume that n is
a power of 2
a tn  tn  1  n2 for n  1 t1  1
b tn  2tn2  n for n  2 t2  2
1418 use theorem 141 to prove that binary search requires log n time
1419 recall that when a hash table gets to be more than about one half full its
performance quickly degrades one solution to this problem is to reinsert
all elements of the hash table into a new hash table that is twice as large
assuming that the expected average case cost to insert into a hash table is
1 prove that the average cost to insert is still 1 when this reinsertion
policy is used
1420 given a 23 tree with n nodes prove that inserting m additional nodes requires om  n  node splits
1421 one approach to implementing an arraybased list where the list size is unknown is to let the array grow and shrink this is known as a dynamic array
when necessary we can grow or shrink the array by copying the arrays contents to a new array if we are careful about the size of the new array this
copy operation can be done rarely enough so as not to affect the amortized
cost of the operations
a what is the amortized cost of inserting elements into the list if the array
is initially of size 1 and we double the array size whenever the number
of elements that we wish to store exceeds the size of the array assume
that the insert itself cost o1 time per operation and so we are just
concerned with minimizing the copy time to the new array


$$@@$$PAGE: 511
492

chap 14 analysis techniques

b consider an underflow strategy that cuts the array size in half whenever
the array falls below half full give an example where this strategy leads
to a bad amortized cost again we are only interested in measuring the
time of the array copy operations
c give a better underflow strategy than that suggested in part b your
goal is to find a strategy whose amortized analysis shows that array
copy requires on time for a series of n operations
1422 recall that two vertices in an undirected graph are in the same connected
component if there is a path connecting them a good algorithm to find the
connected components of an undirected graph begins by calling a dfs on
the first vertex all vertices reached by the dfs are in the same connected
component and are so marked we then look through the vertex mark array
until an unmarked vertex i is found again calling the dfs on i all vertices
reachable from i are in a second connected component we continue working through the mark array until all vertices have been assigned to some
connected component a sketch of the algorithm is as follows
void dfs componentgraph g int v int component 
gsetmarkv component
for int wgfirstv wgn w  gnextvw
if ggetmarkw  0
dfs componentg w component

void concomgraph g 
int i
int component  1
 counter for current component
for i0 ign i  for n vertices in graph
gsetmarki 0  vertices start in no component
for i0 ign i
if ggetmarki  0  start a new component
dfs componentg i component


use the concept of potential from amortized analysis to explain why the total
cost of this algorithm is v  e note that this will not be a true
amortized analysis because this algorithm does not allow an arbitrary series
of dfs operations but rather is fixed to do a single call to dfs from each
vertex
1423 give a proof similar to that used for theorem 142 to show that the total
number of comparisons required by any series of n or more searches s on a
selforganizing list of length n using the count heuristic is never more than
twice the total number of comparisons required when series s is applied to
the list stored in its optimal static order


$$@@$$PAGE: 512
493

sec 146 projects

1424 use mathematical induction to prove that
n
x

f ibi  f ibn  2  1 for n  1

i1

1425 use mathematical induction to prove that fibi is even if and only if n is
divisible by 3
1426 use mathematical induction to prove that for n  6 f ibn  32n1 
1427 find closed forms for each of the following recurrences
a
b
c
d
e
f

f n  f n  1  3 f 1  2
f n  2f n  1 f 0  1
f n  2f n  1  1 f 1  1
f n  2nf n  1 f 0  1
f n  2n f p
n  1 f 0  1
f n  2  n1
i1 f i f 1  1

1428 find  for each of the following recurrence relations
a t n  2t n2  n2 
b t n  2t n2  5
c t n  4t n2  n
d t n  2t n2  n2 
e t n  4t n2  n3 
f t n  4t n3  n
g t n  4t n3  n2 
h t n  2t n2  log n
i t n  2t n2  n log n

146

projects

141 implement the unionfind algorithm of section 62 using both path compression and the weighted union rule count the total number of node accesses required for various series of equivalences to determine if the actual
performance of the algorithm matches the expected cost of n log n


$$@@$$PAGE: 513

$$@@$$PAGE: 514
15
lower bounds

how do i know if i have a good algorithm to solve a problem if my algorithm runs
in n log n time is that good it would be if i were sorting the records stored
in an array but it would be terrible if i were searching the array for the largest
element the value of an algorithm must be determined in relation to the inherent
complexity of the problem at hand
in section 36 we defined the upper bound for a problem to be the upper bound
of the best algorithm we know for that problem and the lower bound to be the
tightest lower bound that we can prove over all algorithms for that problem while
we usually can recognize the upper bound for a given algorithm finding the tightest
lower bound for all possible algorithms is often difficult especially if that lower
bound is more than the trivial lower bound determined by measuring the amount
of input that must be processed
the benefits of being able to discover a strong lower bound are significant in
particular when we can make the upper and lower bounds for a problem meet this
means that we truly understand our problem in a theoretical sense it also saves
us the effort of attempting to discover more asymptotically efficient algorithms
when no such algorithm can exist
often the most effective way to determine the lower bound for a problem is
to find a reduction to another problem whose lower bound is already known this
is the subject of chapter 17 however this approach does not help us when we
cannot find a suitable similar problem our focus in this chapter is discovering
and proving lower bounds from first principles our most significant example of
a lower bounds argument so far is the proof from section 79 that the problem of
sorting is on log n in the worst case
section 151 reviews the concept of a lower bound for a problem and presents
the basic algorithm for finding a good algorithm section 152 discusses lower
bounds on searching in lists both those that are unordered and those that are ordered section 153 deals with finding the maximum value in a list and presents a
model for selection based on building a partially ordered set section 154 presents
495


$$@@$$PAGE: 515
496

chap 15 lower bounds

the concept of an adversarial lower bounds proof section 155 illustrates the concept of a state space lower bound section 156 presents a linear time worstcase
algorithm for finding the ith biggest element on a list section 157 continues our
discussion of sorting with a quest for the algorithm that requires the absolute fewest
number of comparisons needed to sort a list

151

introduction to lower bounds proofs

the lower bound for the problem is the tightest highest lower bound that we can
prove for all possible algorithms that solve the problem1 this can be a difficult bar
given that we cannot possibly know all algorithms for any problem because there
are theoretically an infinite number however we can often recognize a simple
lower bound based on the amount of input that must be examined for example
we can argue that the lower bound for any algorithm to find the maximumvalued
element in an unsorted list must be n because any algorithm must examine all
of the inputs to be sure that it actually finds the maximum value
in the case of maximum finding the fact that we know of a simple algorithm
that runs in on time combined with the fact that any algorithm needs n time
is significant because our upper and lower bounds meet within a constant factor
we know that we do have a good algorithm for solving the problem it is possible
that someone can develop an implementation that is a little faster than an existing
one by a constant factor but we know that its not possible to develop one that is
asymptotically better
we must be careful about how we interpret this last statement however the
world is certainly better off for the invention of quicksort even though mergesort
was available at the time quicksort is not asymptotically faster than mergesort yet
is not merely a tuning of mergesort either quicksort is a substantially different
approach to sorting so even when our upper and lower bounds for a problem meet
there are still benefits to be gained from a new clever algorithm
so now we have an answer to the question how do i know if i have a good
algorithm to solve a problem an algorithm is good asymptotically speaking if
its upper bound matches the problems lower bound if they match we know to
stop trying to find an asymptotically faster algorithm what if the known upper
bound for our algorithm does not match the known lower bound for the problem
in this case we might not know what to do is our upper bound flawed and the
algorithm is really faster than we can prove is our lower bound weak and the true
lower bound for the problem is greater or is our algorithm simply not the best
1
throughout this discussion it should be understood that any mention of bounds must specify
what class of inputs are being considered do we mean the bound for the worst case input the
average cost over all inputs regardless of which class of inputs we consider all of the issues raised
apply equally


$$@@$$PAGE: 516
sec 151 introduction to lower bounds proofs

497

now we know precisely what we are aiming for when designing an algorithm
we want to find an algorithm whos upper bound matches the lower bound of the
problem putting together all that we know so far about algorithms we can organize
our thinking into the following algorithm for designing algorithms2
if the upper and lower bounds match
then stop
else if the bounds are close or the problem isnt important
then stop
else if the problem definition focuses on the wrong thing
then restate it
else if the algorithm is too slow
then find a faster algorithm
else if lower bound is too weak
then generate a stronger bound
we can repeat this process until we are satisfied or exhausted
this brings us smack up against one of the toughest tasks in analysis lower
bounds proofs are notoriously difficult to construct the problem is coming up with
arguments that truly cover all of the things that any algorithm possibly could do
the most common fallacy is to argue from the point of view of what some good
algorithm actually does do and claim that any algorithm must do the same this
simply is not true and any lower bounds proof that refers to specific behavior that
must take place should be viewed with some suspicion
let us consider the towers of hanoi problem again recall from section 25
that our basic algorithm is to move n  1 disks recursively to the middle pole
move the bottom disk to the third pole and then move n1 disks again recursively
from the middle to the third pole this algorithm generates the recurrence tn 
2tn  1  1  2n  1 so the upper bound for our algorithm is 2n  1 but is
this the best algorithm for the problem what is the lower bound for the problem
for our first try at a lower bounds proof the trivial lower bound is that we
must move every disk at least once for a minimum cost of n slightly better is to
observe that to get the bottom disk to the third pole we must move every other disk
at least twice once to get them off the bottom disk and once to get them over to
the third pole this yields a cost of 2n  1 which still is not a good match for our
algorithm is the problem in the algorithm or in the lower bound
we can get to the correct lower bound by the following reasoning to move the
biggest disk from first to the last pole we must first have all of the other n  1 disks
out of the way and the only way to do that is to move them all to the middle pole
for a cost of at least tn  1 we then must move the bottom disk for a cost of
2

this is a minor reformulation of the algorithm given by gregory je rawlins in his book
compared to what


$$@@$$PAGE: 517
498

chap 15 lower bounds

at least one after that we must move the n  1 remaining disks from the middle
pole to the third pole for a cost of at least tn  1 thus no possible algorithm
can solve the problem in less than 2n  1 steps thus our algorithm is optimal3
of course there are variations to a given problem changes in the problem
definition might or might not lead to changes in the lower bound two possible
changes to the standard towers of hanoi problem are
 not all disks need to start on the first pole
 multiple disks can be moved at one time
the first variation does not change the lower bound at least not asymptotically
the second one does

152

lower bounds on searching lists

in section 79 we presented an important lower bounds proof to show that the
problem of sorting is n log n in the worst case in chapter 9 we discussed a
number of algorithms to search in sorted and unsorted lists but we did not provide
any lower bounds proofs to this important problem we will extend our pool of
techniques for lower bounds proofs in this section by studying lower bounds for
searching unsorted and sorted lists
1521

searching in unsorted lists

given an unsorted list l of n elements and a search key k we seek to identify one
element in l which has key value k if any exists for the rest of this discussion
we will assume that the key values for the elements in l are unique that the set of
all possible keys is totally ordered that is the operations   and  are defined
for all pairs of key values and that comparison is our only way to find the relative
ordering of two keys our goal is to solve the problem using the minimum number
of comparisons
given this definition for searching we can easily come up with the standard
sequential search algorithm and we can also see that the lower bound for this problem is obviously n comparisons keep in mind that the key k might not actually
appear in the list however lower bounds proofs are a bit slippery and it is instructive to see how they can go wrong
theorem 151 the lower bound for the problem of searching in an unsorted list
is n comparisons
3

recalling the advice to be suspicious of any lower bounds proof that argues a given behavior
must happen this proof should be raising red flags however in this particular case the problem is
so constrained that there really is no better alternative to this particular sequence of events


$$@@$$PAGE: 518
sec 152 lower bounds on searching lists

499

here is our first attempt at proving the theorem
proof 1 we will try a proof by contradiction assume an algorithm a exists that
requires only n  1 or less comparisons of k with elements of l because there
are n elements of l a must have avoided comparing k with li for some value
i we can feed the algorithm an input with k in position i such an input is legal in
our model so the algorithm is incorrect
2
is this proof correct unfortunately no first of all any given algorithm need
not necessarily consistently skip any given position i in its n  1 searches for
example it is not necessary that all algorithms search the list from left to right it
is not even necessary that all algorithms search the same n  1 positions first each
time through the list
we can try to dress up the proof as follows proof 2 on any given run of the
algorithm if n  1 elements are compared against k then some element position
call it position i gets skipped it is possible that k is in position i at that time and
will not be found therefore n comparisons are required
2
unfortunately there is another error that needs to be fixed it is not true that
all algorithms for solving the problem must work by comparing elements of l
against k an algorithm might make useful progress by comparing elements of l
against each other for example if we compare two elements of l then compare
the greater against k and find that this element is less than k we know that the
other element is also less than k it seems intuitively obvious that such comparisons wont actually lead to a faster algorithm but how do we know for sure we
somehow need to generalize the proof to account for this approach
we will now present a useful abstraction for expressing the state of knowledge
for the value relationships among a set of objects a total order defines relationships within a collection of objects such that for every pair of objects one is greater
than the other a partially ordered set or poset is a set on which only a partial
order is defined that is there can be pairs of elements for which we cannot decide which is greater for our purpose here the partial order is the state of our
current knowledge about the objects such that zero or more of the order relations
between pairs of elements are known we can represent this knowledge by drawing
directed acyclic graphs dags showing the known relationships as illustrated by
figure 151
proof 3 initially we know nothing about the relative order of the elements in l
or their relationship to k so initially we can view the n elements in l as being in
n separate partial orders any comparison between two elements in l can affect
the structure of the partial orders this is somewhat similar to the unionfind
algorithm implemented using parent pointer trees described in section 62
now every comparison between elements in l can at best combine two of the
partial orders together any comparison between k and an element say a in l can
at best eliminate the partial order that contains a thus if we spend m comparisons


$$@@$$PAGE: 519
500

chap 15 lower bounds

a

b

c
d

g
e
f

figure 151 illustration of using a poset to model our current knowledge of the
relationships among a collection of objects a directed acyclic graph dag is
used to draw the poset assume all edges are directed downward in this example
our knowledge is such that we dont know how a or b relate to any of the other
objects however we know that both c and g are greater than e and f further
we know that c is greater than d and that e is greater than f

comparing elements in l we have at least n  m partial orders every such partial
order needs at least one comparison against k to make sure that k is not somewhere
in that partial order thus any algorithm must make at least n comparisons in the
worst case
2
1522

searching in sorted lists

we will now assume that list l is sorted in this case is linear search still optimal
clearly no but why not because we have additional information to work with that
we do not have when the list is unsorted we know that the standard binary search
algorithm has a worst case cost of olog n can we do better than this we can
prove that this is the best possible in the worst case with a proof similar to that used
to show the lower bound on sorting
again we use the decision tree to model our algorithm unlike when searching
an unsorted list comparisons between elements of l tell us nothing new about their
relative order so we consider only comparisons between k and an element in l at
the root of the decision tree our knowledge rules out no positions in l so all are
potential candidates as we take branches in the decision tree based on the result
of comparing k to an element in l we gradually rule out potential candidates
eventually we reach a leaf node in the tree representing the single position in l
that can contain k there must be at least n  1 nodes in the tree because we have
n  1 distinct positions that k can be in any position in l plus not in l at all
some path in the tree must be at least log n levels deep and the deepest node in the
tree represents the worst case for that algorithm thus any algorithm on a sorted
array requires at least log n comparisons in the worst case
we can modify this proof to find the average cost lower bound again we
model algorithms using decision trees except now we are interested not in the
depth of the deepest node the worst case and therefore the tree with the leastdeepest node instead we are interested in knowing what the minimum possible is


$$@@$$PAGE: 520
sec 153 finding the maximum value

501

for the average depth of the leaf nodes define the total path length as the sum
of the levels for each node the cost of an outcome is the level of the corresponding
node plus 1 the average cost of the algorithm is the average cost of the outcomes
total path lengthn what is the tree with the least average depth this is equivalent to the tree that corresponds to binary search thus binary search is optimal in
the average case
while binary search is indeed an optimal algorithm for a sorted list in the worst
and average cases when searching a sorted array there are a number of circumstances that might lead us to select another algorithm instead one possibility is
that we know something about the distribution of the data in the array we saw in
section 91 that if each position in l is equally likely to hold x equivalently the
data are well distributed along the full key range then an interpolation search is
log log n in the average case if the data are not sorted then using binary search
requires us to pay the cost of sorting the list in advance which is only worthwhile if
many at least olog n searches will be performed on the list binary search also
requires that the list even if sorted be implemented using an array or some other
structure that supports random access to all elements with equal cost finally if we
know all search requests in advance we might prefer to sort the list by frequency
and do linear search in extreme search distributions as discussed in section 92

153

finding the maximum value

how can we find the ith largest value in a sorted list obviously we just go to the
ith position but what if we have an unsorted list can we do better than to sort
it if we are looking for the minimum or maximum value certainly we can do
better than sorting the list is this true for the second biggest value for the median
value in later sections we will examine those questions for this section we
will continue our examination of lower bounds proofs by reconsidering the simple
problem of finding the maximum value in an unsorted list
here is a simple algorithm for finding the largest value
 return position of largest value in a of size n
int largestint a int n 
int currlarge  0  holds largest element position
for int i1 in i
 for each array element
if acurrlarge  ai  if ai is larger
currlarge  i

remember its position
return currlarge
 return largest position


obviously this algorithm requires n comparisons is this optimal it should be
intuitively obvious that it is but let us try to prove it before reading further you
might try writing down your own proof


$$@@$$PAGE: 521
502

chap 15 lower bounds

proof 1 the winner must compare against all other elements so there must be
n  1 comparisons
2
this proof is clearly wrong because the winner does not need to explicitly compare against all other elements to be recognized for example a standard singleelimination playoff sports tournament requires only n  1 comparisons and the
winner does not play every opponent so lets try again
proof 2 only the winner does not lose there are n  1 losers a single comparison generates at most one new loser therefore there must be n  1 comparisons
2
this proof is sound however it will be useful later to abstract this by introducing the concept of posets as we did in section 1521 we can view the maximumfinding problem as starting with a poset where there are no known relationships so
every member of the collection is in its own separate dag of one element
proof 2a to find the largest value we start with a poset of n dags each with
a single element and we must build a poset having all elements in one dag such
that there is one maximum value and by implication n  1 losers we wish to
connect the elements of the poset into a single dag with the minimum number of
links this requires at least n  1 links a comparison provides at most one new
link thus a minimum of n  1 comparisons must be made
2
what is the average cost of largest because it always does the same number of comparisons clearly it must cost n  1 comparisons we can also consider
the number of assignments that largest must do function largest might do
an assignment on any iteration of the for loop
because this event does happen or does not happen if we are given no information about distribution we could guess that an assignment is made after each comparison with a probability of one half but this is clearly wrong in fact largest
does an assignment on the ith iteration if and only if ai is the biggest of the the
first i elements assuming all permutations are equally likely the probability of
this being true is 1i thus the average number of assignments done is
1

n
x
1
i2

i



n
x
1
i1

i

which is the harmonic series hn  hn  log n more exactly hn is close to
loge n
how reliable is this average that is how much will a given run of the
program deviate from the mean cost according to cebysevs inequality an observation will fall within two standard deviations of the mean at least 75 of the time
for largest the variance is
hn 

2
2
 loge n  
6
6


$$@@$$PAGE: 522
sec 154 adversarial lower bounds proofs

503

p
the standard deviation
p is thus about logepn so 75 of the observations are
between loge n  2 loge n and loge n  2 loge n is this a narrow spread or a
wide spread compared to the mean value this spread is pretty wide meaning that
the number of assignments varies widely from run to run of the program

154

adversarial lower bounds proofs

our next problem will be finding the second largest in a collection of objects consider what happens in a standard singleelimination tournament even if we assume
that the best team wins in every game is the second best the one that loses in the
finals not necessarily we might expect that the second best must lose to the best
but they might meet at any time
let us go through our standard algorithm for finding algorithms by first
proposing an algorithm then a lower bound and seeing if they match unlike
our analysis for most problems this time we are going to count the exact number
of comparisons involved and attempt to minimize this count a simple algorithm
for finding the second largest is to first find the maximum in n  1 comparisons
discard it and then find the maximum of the remaining elements in n  2 comparisons for a total cost of 2n  3 comparisons is this optimal that seems doubtful
but let us now proceed to the step of attempting to prove a lower bound
theorem 152 the lower bound for finding the second largest value is 2n  3
proof any element that loses to anything other than the maximum cannot be
second so the only candidates for second place are those that lost to the maximum
function largest might compare the maximum element to n  1 others thus
we might need n  2 additional comparisons to find the second largest
2
this proof is wrong it exhibits the necessity fallacy our algorithm does
something therefore all algorithms solving the problem must do the same
this leaves us with our best lower bounds argument at the moment being that
finding the second largest must cost at least as much as finding the largest or n  1
let us take another try at finding a better algorithm by adopting a strategy of divide
and conquer what if we break the list into halves and run largest on each
half we can then compare the two winners we have now used a total of n  1
comparisons and remove the winner from its half another call to largest on
the winners half yields its second best a final comparison against the winner of
the other half gives us the true second place winner the total cost is d3n2e  2 is
this optimal what if we break the list into four pieces the best would be d5n4e
what if we break the list into eight pieces then the cost would be about d9n8e
notice that as we break the list into more parts comparisons among the winners of
the parts becomes a larger concern


$$@@$$PAGE: 523
504

chap 15 lower bounds

figure 152 an example of building a binomial tree pairs of elements are
combined by choosing one of the parents to be the root of the entire tree given
two trees of size four one of the roots is chosen to be the root for the combined
tree of eight nodes

looking at this another way the only candidates for second place are losers to
the eventual winner and our goal is to have as few of these as possible so we need
to keep track of the set of elements that have lost in direct comparison to the eventual winner we also observe that we learn the most from a comparison when both
competitors are known to be larger than the same number of other values so we
would like to arrange our comparisons to be against equally strong competitors
we can do all of this with a binomial tree a binomial tree of height m has 2m
nodes either it is a single node if m  0 or else it is two height m  1 binomial
trees with one trees root becoming a child of the other figure 152 illustrates how
a binomial tree with eight nodes would be constructed
the resulting algorithm is simple in principle build the binomial tree for all n
elements and then compare the dlog ne children of the root to find second place
we could store the binomial tree as an explicit tree structure and easily build it in
time linear on the number of comparisons as each comparison requires one link be
added because the shape of a binomial tree is heavily constrained we can also
store the binomial tree implicitly in an array much as we do for a heap assume
that two trees each with 2k nodes are in the array the first tree is in positions 1
to 2k  the second tree is in positions 2k  1 to 2k1  the root of each subtree is in
the final array position for that subtree
to join two trees we simply compare the roots of the subtrees if necessary
swap the subtrees so that tree with the the larger root element becomes the second
subtree this trades space we only need space for the data values no node pointers for time in the worst case all of the data swapping might cost on log n
though this does not affect the number of comparisons required note that for
some applications this is an important observation that the arrays data swapping
requires no comparisons if a comparison is simply a check between two integers
then of course moving half the values within the array is too expensive but if a
comparison requires that a competition be held between two sports teams then the
cost of a little bit or even a lot of book keeping becomes irrelevent
because the binomial trees root has log n children and building the tree requires n  1 comparisons the number of comparisons required by this algorithm is
n  dlog ne  2 this is clearly better than our previous algorithm is it optimal


$$@@$$PAGE: 524
sec 154 adversarial lower bounds proofs

505

we now go back to trying to improve the lower bounds proof to do this
we introduce the concept of an adversary the adversarys job is to make an
algorithms cost as high as possible imagine that the adversary keeps a list of all
possible inputs we view the algorithm as asking the adversary for information
about the algorithms input the adversary may never lie in that its answer must
be consistent with the previous answers but it is permitted to rearrange the input
as it sees fit in order to drive the total cost for the algorithm as high as possible in
particular when the algorithm asks a question the adversary must answer in a way
that is consistent with at least one remaining input the adversary then crosses out
all remaining inputs inconsistent with that answer keep in mind that there is not
really an entity within the computer program that is the adversary and we dont
actually modify the program the adversary operates merely as an analysis device
to help us reason about the program
as an example of the adversary concept consider the standard game of hangman player a picks a word and tells player b how many letters the word has
player b guesses various letters if b guesses a letter in the word then a will indicate which positions in the word have the letter player b is permitted to make
only so many guesses of letters not in the word before losing
in the hangman game example the adversary is imagined to hold a dictionary
of words of some selected length each time the player guesses a letter the adversary consults the dictionary and decides if more words will be eliminated by
accepting the letter and indicating which positions it holds or saying that its not
in the word the adversary can make any decision it chooses so long as at least
one word in the dictionary is consistent with all of the decisions in this way the
adversary can hope to make the player guess as many letters as possible
before explaining how the adversary plays a role in our lower bounds proof
first observe that at least n  1 values must lose at least once this requires at least
n  1 compares in addition at least k  1 values must lose to the second largest
value that is k direct losers to the winner must be compared there must be at
least n  k  2 comparisons the question is how low can we make k
call the strength of element ai the number of elements that ai is known
to be bigger than if ai has strength a and aj has strength b then the winner
has strength a  b  1 the algorithm gets to know the current strengths for each
element and it gets to pick which two elements are compared next the adversary
gets to decide who wins any given comparison what strategy by the adversary
would cause the algorithm to learn the least from any given comparison it should
minimize the rate at which any element improves it strength it can do this by
making the element with the greater strength win at every comparison this is a
fair use of an adversary in that it represents the results of providing a worstcase
input for that given algorithm


$$@@$$PAGE: 525
506

chap 15 lower bounds

to minimize the effects of worstcase behavior the algorithms best strategy is
to maximize the minimum improvement in strength by balancing the strengths of
any two competitors from the algorithms point of view the best outcome is that
an element doubles in strength this happens whenever a  b where a and b are
the strengths of the two elements being compared all strengths begin at zero so
the winner must make at least k comparisons when 2k1  n  2k  thus there
must be at least n  dlog ne  2 comparisons so our algorithm is optimal

155

state space lower bounds proofs

we now consider the problem of finding both the minimum and the maximum from
an unsorted list of values this might be useful if we want to know the range of
a collection of values to be plotted for the purpose of drawing the plots scales
of course we could find them independently in 2n  2 comparisons a slight
modification is to find the maximum in n  1 comparisons remove it from the
list and then find the minimum in n  2 further comparisons for a total of 2n  3
comparisons can we do better than this
before continuing think a moment about how this problem of finding the minimum and the maximum compares to the problem of the last section that of finding
the second biggest value and by implication the maximum which of these two
problems do you think is harder it is probably not at all obvious to you that one
problem is harder or easier than the other there is intuition that argues for either case on the one hand intuition might argue that the process of finding the
maximum should tell you something about the second biggest value more than
that process should tell you about the minimum value on the other hand any
given comparison tells you something about which of two can be a candidate for
maximum value and which can be a candidate for minimum value thus making
progress in both directions
we will start by considering a simple divideandconquer approach to finding
the minimum and maximum split the list into two parts and find the minimum and
maximum elements in each part then compare the two minimums and maximums
to each other with a further two comparisons to get the final result the algorithm
is shown in figure 153
the cost of this algorithm can be modeled by the following recurrence

n1
 0
1
n2
tn 

tbn2c  tdn2e  2 n  2
this is a rather interesting recurrence and its solution ranges between 3n2  2
when n  2i or n  21  1 and 5n3  2 when n  3  2i  we can infer from
this behavior that how we divide the list affects the performance of the algorithm


$$@@$$PAGE: 526
507

sec 155 state space lower bounds proofs

 return the minimum and maximum values in a
 between positions l and r
template typename e
void minmaxe a int l int r e min e max 
if l  r 
 n1
min  ar
max  ar

else if l1  r   n2
min  minal ar
max  maxal ar

else 
 n2
int min1 min2 max1 max2
int mid  l  r2
minmaxa l mid min1 max1
minmaxa mid1 r min2 max2
min  minmin1 min2
max  maxmax1 max2


figure 153 recursive algorithm for finding the minimum and maximum values
in an array

for example what if we have six items in the list if we break the list into two
sublists of three elements the cost would be 8 if we break the list into a sublist of
size two and another of size four then the cost would only be 7
with divide and conquer the best algorithm is the one that minimizes the work
not necessarily the one that balances the input sizes one lesson to learn from this
example is that it can be important to pay attention to what happens for small sizes
of n because any division of the list will eventually produce many small lists
we can model all possible divideandconquer strategies for this problem with
the following recurrence

 0
1
tn 

min1kn1 tk  tn  k  2

n1
n2
n2

that is we want to find a way to break up the list that will minimize the total
work if we examine various ways of breaking up small lists we will eventually
recognize that breaking the list into a sublist of size 2 and a sublist of size n  2
will always produce results as good as any other division this strategy yields the
following recurrence

n1
 0
1
n2
tn 

tn  2  3 n  2


$$@@$$PAGE: 527
508

chap 15 lower bounds

this recurrence and the corresponding algorithm yields tn  d3n2e  2
comparisons is this optimal we now introduce yet another tool to our collection
of lower bounds proof techniques the state space proof
we will model our algorithm by defining a state that the algorithm must be in at
any given instant we can then define the start state the end state and the transitions
between states that any algorithm can support from this we will reason about the
minimum number of states that the algorithm must go through to get from the start
to the end to reach a state space lower bound
at any given instant we can track the following four categories of elements





untested elements that have not been tested
winners elements that have won at least once and never lost
losers elements that have lost at least once and never won
middle elements that have both won and lost at least once

we define the current state to be a vector of four values u w l m  for
untested winners losers and middles respectively for a set of n elements the
initial state of the algorithm is n 0 0 0 and the end state is 0 1 1 n  2 thus
every run for any algorithm must go from state n 0 0 0 to state 0 1 1 n  2
we also observe that once an element is identified to be a middle it can then be
ignored because it can neither be the minimum nor the maximum
given that there are four types of elements there are 10 types of comparison
comparing with a middle cannot be more efficient than other comparisons so we
should ignore those leaving six comparisons of interest we can enumerate the
effects of each comparison type as follows if we are in state i j k l and we have
a comparison then the state changes are as follows
u u
w w
ll
lu
or
w u
or
w l
or

i  2
i
i
i  1
i  1
i  1
i  1
i
i

j  1
j  1
j
j  1
j
j
j
j
j  1

k  1
k
k  1
k
k
k  1
k
k
k  1

l
l  1
l  1
l
l  1
l
l  1
l
l  2

now let us consider what an adversary will do for the various comparisons
the adversary will make sure that each comparison does the least possible amount
of work in taking the algorithm toward the goal state for example comparing a
winner to a loser is of no value because the worst case result is always to learn
nothing new the winner remains a winner and the loser remains a loser thus
only the following five transitions are of interest


$$@@$$PAGE: 528
sec 156 finding the ith best element

509


i1



ni

figure 154 the poset that represents the minimum information necessary to
determine the ith element in a list we need to know which element has i  1
values less and n  i values more but we do not need to know the relationships
among the elements with values less or greater than the ith element

u u
lu
w u
w w
ll

i  2
i  1
i  1
i
i

j  1
j  1
j
j  1
j

k  1
k
k  1
k
k  1

l
l
l
l  1
l  1

only the last two transition types increase the number of middles so there
must be n  2 of these the number of untested elements must go to 0 and the first
transition is the most efficient way to do this thus dn2e of these are required
our conclusion is that the minimum possible number of transitions comparisons
is n  dn2e  2 thus our algorithm is optimal

156

finding the ith best element

we now tackle the problem of finding the ith best element in a list as observed
earlier one solution is to sort the list and simply look in the ith position however
this process provides considerably more information than we need to solve the
problem the minimum amount of information that we actually need to know can
be visualized as shown in figure 154 that is all we need to know is the i  1
items less than our desired value and the n  i items greater we do not care about
the relative order within the upper and lower groups so can we find the required
information faster than by first sorting looking at the lower bound can we tighten
that beyond the trivial lower bound of n comparisons we will focus on the specific
question of finding the median element ie the element with rank n2 because
the resulting algorithm can easily be modified to find the ith largest value for any i
looking at the quicksort algorithm might give us some insight into solving the
median problem recall that quicksort works by selecting a pivot value partitioning the array into those elements less than the pivot and those greater than the pivot
and moving the pivot to its proper location in the array if the pivot is in position i
then we are done if not we can solve the subproblem recursively by only considering one of the sublists that is if the pivot ends up in position k  i then we


$$@@$$PAGE: 529
510

chap 15 lower bounds

figure 155 a method for finding a pivot for partitioning a list that guarantees
at least a fixed fraction of the list will be in each partition we divide the list into
groups of five elements and find the median for each group we then recursively
find the median of these n5 medians the median of five elements is guaranteed to have at least two in each partition the median of three medians from
a collection of 15 elements is guaranteed to have at least five elements in each
partition

simply solve by finding the ith best element in the left partition if the pivot is at
position k  i then we wish to find the i  kth element in the right partition
what is the worst case cost of this algorithm as with quicksort we get bad
performance if the pivot is the first or last element in the array this would lead to
possibly on2  performance however if the pivot were to always cut the array in
half then our cost would be modeled by the recurrence tn  tn2  n  2n
or on cost
finding the average cost requires us to use a recurrence with full history similar
to the one we used to model the cost of quicksort if we do this we will find that
tn is in on in the average case
is it possible to modify our algorithm to get worstcase linear time to do
this we need to pick a pivot that is guaranteed to discard a fixed fraction of the
elements we cannot just choose a pivot at random because doing so will not meet
this guarantee the ideal situation would be if we could pick the median value for
the pivot each time but that is essentially the same problem that we are trying to
solve to begin with
notice however that if we choose any constant c and then if we pick the
median from a sample of size nc then we can guarantee that we will discard
at least n2c elements actually we can do better than this by selecting small
subsets of a constant size so we can find the median of each in constant time and
then taking the median of these medians figure 155 illustrates this idea this
observation leads directly to the following algorithm
 choose the n5 medians for groups of five elements from the list choosing
the median of five items can be done in constant time
 recursively select m the median of the n5 mediansoffives
 partition the list into those elements larger and smaller than m


$$@@$$PAGE: 530
511

sec 157 optimal sorting

while selecting the median in this way is guaranteed to eliminate a fraction of
the elements leaving at most d7n  510e elements left we still need to be sure
that our recursion yields a lineartime algorithm we model the algorithm by the
following recurrence
tn  tdn5e  td7n  510e  6dn5e  n  1

the tdn5e term comes from computing the median of the mediansoffives
the 6dn5e term comes from the cost to calculate the medianoffives exactly six
comparisons for each group of five element and the td7n510e term comes
from the recursive call of the remaining up to 70 of the elements that might be
left
we will prove that this recurrence is linear by assuming that it is true for some
constant r and then show that tn  rn for all n greater than some bound
tn 




n
7n  5
n
td e  td
e  6d e  n  1
5
10
5
n
7n  5
n
r  1  r
 1  6  1  n  1
5
10
5
7r
11
3r
r
 n 
5
 
5 10
5
2
9r  22
3r  10
n

10
2

this is true for r  23 and n  380 this provides a base case that allows us to
use induction to prove that n  380 tn  23n
in reality this algorithm is not practical because its constant factor costs are so
high so much work is being done to guarantee linear time performance that it is
more efficient on average to rely on chance to select the pivot perhaps by picking
it at random or picking the middle value out of the current subarray

157

optimal sorting

we conclude this section with an effort to find the sorting algorithm with the absolute fewest possible comparisons it might well be that the result will not be
practical for a generalpurpose sorting algorithm but recall our analogy earlier to
sports tournaments in sports a comparison between two teams or individuals
means doing a competition between the two this is fairly expensive at least compared to some minor book keeping in a computer and it might be worth trading a
fair amount of book keeping to cut down on the number of games that need to be
played what if we want to figure out how to hold a tournament that will give us
the exact ordering for all teams in the fewest number of total games of course
we are assuming that the results of each game will be accurate in that we assume


$$@@$$PAGE: 531
512

chap 15 lower bounds

not only that the outcome of a playing b would always be the same at least over
the time period of the tournament but that transitivity in the results also holds in
practice these are unrealistic assumptions but such assumptions are implicitly part
of many tournament organizations like most tournament organizers we can simply accept these assumptions and come up with an algorithm for playing the games
that gives us some rank ordering based on the results we obtain
recall insertion sort where we put element i into a sorted sublist of the first i
1 elements what if we modify the standard insertion sort algorithm to use binary
search to locate where the ith element goes in the sorted sublist this algorithm
is called binary insert sort as a generalpurpose sorting algorithm this is not
practical because we then have to on average move about i2 elements to make
room for the newly inserted element in the sorted sublist but if we count only
comparisons binary insert sort is pretty good and we can use some ideas from
binary insert sort to get closer to an algorithm that uses the absolute minimum
number of comparisons needed to sort
consider what happens when we run binary insert sort on five elements how
many comparisons do we need to do we can insert the second element with one
comparison the third with two comparisons and the fourth with 2 comparisons
when we insert the fifth element into the sorted list of four elements we need to
do three comparisons in the worst case notice exactly what happens when we
attempt to do this insertion we compare the fifth element against the second if the
fifth is bigger we have to compare it against the third and if it is bigger we have
to compare it against the fourth in general when is binary search most efficient
when we have 2i  1 elements in the list it is least efficient when we have 2i
elements in the list so we can do a bit better if we arrange our insertions to avoid
inserting an element into a list of size 2i if possible
figure 156 illustrates a different organization for the comparisons that we
might do first we compare the first and second element and the third and fourth
elements the two winners are then compared yielding a binomial tree we can
view this as a sorted chain of three elements with element a hanging off from the
root if we then insert element b into the sorted chain of three elements we will
end up with one of the two posets shown on the right side of figure 156 at a cost of
2 comparisons we can then merge a into the chain for a cost of two comparisons
because we already know that it is smaller then either one or two elements we are
actually merging it into a list of two or three elements thus the total number of
comparisons needed to sort the five elements is at most seven instead of eight
if we have ten elements to sort we can first make five pairs of elements using
five compares and then sort the five winners using the algorithm just described
using seven more compares now all we need to do is to deal with the original
losers we can generalize this process for any number of elements as
 pair up all the nodes with b n2 c comparisons


$$@@$$PAGE: 532
513

sec 157 optimal sorting

a

or a
b

a

figure 156 organizing comparisons for sorting five elements first we order
two pairs of elements and then compare the two winners to form a binomial tree
of four elements the original loser to the root is labeled a and the remaining
three elements form a sorted chain we then insert element b into the sorted
chain finally we put a into the resulting chain to yield a final sorted list

 recursively sort the winners
 fold in the losers
we use binary insert to place the losers however we are free to choose the
best ordering for inserting keeping in mind the fact that binary search has the
same cost for 2i through 2i1  1 items for example binary search requires three
comparisons in the worst case for lists of size 4 5 6 or 7 so we pick the order of
inserts to optimize the binary searches which means picking an order that avoids
growing a sublist size such that it crosses the boundary on list size to require an
additional comparison this sort is called merge insert sort and also known as
the ford and johnson sort
for ten elements given the poset shown in figure 157 we fold in the last
four elements labeled 1 to 4 in the order element 3 element 4 element 1 and
finally element 2 element 3 will be inserted into a list of size three costing two
comparisons depending on where element 3 then ends up in the list element 4
will now be inserted into a list of size 2 or 3 costing two comparisons in either
case depending on where elements 3 and 4 are in the list element 1 will now be
inserted into a list of size 5 6 or 7 all of which requires three comparisons to place
in sort order finally element 2 will be inserted into a list of size 5 6 or 7
merge insert sort is pretty good but is it optimal recall from section 79 that
no sorting algorithm can be faster than n log n to be precise the information
theoretic lower bound for sorting can be proved to be dlog ne that is we can
prove a lower bound of exactly dlog ne comparisons merge insert sort gives us
a number of comparisons equal to this information theoretic lower bound for all
values up to n  12 at n  12 merge insert sort requires 30 comparisons
while the information theoretic lower bound is only 29 comparisons however for
such a small number of elements it is possible to do an exhaustive study of every
possible arrangement of comparisons it turns out that there is in fact no possible
arrangement of comparisons that makes the lower bound less than 30 comparisons
when n  12 thus the information theoretic lower bound is an underestimate in
this case because 30 really is the best that can be done


$$@@$$PAGE: 533
514

chap 15 lower bounds

1
2
3
4

figure 157 merge insert sort for ten elements first five pairs of elements are
compared the five winners are then sorted this leaves the elements labeled 14
to be sorted into the chain made by the remaining six elements

call the optimal worst cost for n elements sn we know that sn  1 
sn  dlogn  1e because we could sort n elements and use binary insert for the
last one for all n and m sn  m  sn  sm  m m n where m m n
is the best time to merge two sorted lists for n  47 it turns out that we can do
better by splitting the list into pieces of size 5 and 42 and then merging thus
merge sort is not quite optimal but it is extremely good and nearly optimal for
smallish numbers of elements

158

further reading

much of the material in this book is also covered in many other textbooks on data
structures and algorithms the biggest exception is that not many other textbooks
cover lower bounds proofs in any significant detail as is done in this chapter those
that do focus on the same example problems search and selection because it tells
such a tight and compelling story regarding related topics while showing off the
major techniques for lower bounds proofs two examples of such textbooks are
computer algorithms by baase and van gelder bg00 and compared to
what by gregory je rawlins raw92 fundamentals of algorithmics by
brassard and bratley bb96 also covers lower bounds proofs

159

exercises

151 consider the socalled algorithm for algorithms in section 151 is this
really an algorithm review the definition of an algorithm from section 14
which parts of the definition apply and which do not is the algorithm for
algorithms a heuristic for finding a good algorithm why or why not
152 singleelimination tournaments are notorious for their scheduling difficulties imagine that you are organizing a tournament for n basketball teams
you may assume that n  2i for some integer i we will further simplify


$$@@$$PAGE: 534
sec 159 exercises

515

things by assuming that each game takes less than an hour and that each team
can be scheduled for a game every hour if necessary note that everything
said here about basketball courts is also true about processors in a parallel
algorithm to solve the maximumfinding problem
a how many basketball courts do we need to insure that every team can
play whenever we want to minimize the total tournament time
b how long will the tournament be in this case
c what is the total number of courthours available how many total
hours are courts being used how many total courthours are unused
d modify the algorithm in such a way as to reduce the total number of
courts needed by perhaps not letting every team play whenever possible this will increase the total hours of the tournament but try to keep
the increase as low as possible for your new algorithm how long is the
tournament how many courts are needed how many total courthours
are available how many courthours are used and how many unused
153 explain why the cost of splitting a list of six into two lists of three to find the
minimum and maximum elements requires eight comparisons while splitting the list into a list of two and a list of four costs only seven comparisons
154 write out a table showing the number of comparisons required to find the
minimum and maximum for all divisions for all values of n  13
155 present an adversary argument as a lower bounds proof to show that n  1
comparisons are necessary to find the maximum of n values in the worst case
156 present an adversary argument as a lower bounds proof to show that n comparisons are necessary in the worst case when searching for an element with
value x if one exists from among n elements
157 section 156 claims that by picking a pivot that always discards at least a
fixed fraction c of the remaining array the resulting algorithm will be linear
explain why this is true hint the master theorem theorem 141 might
help you
158 show that any comparisonbased algorithm for finding the median must use
at least n  1 comparisons
159 show that any comparisonbased algorithm for finding the secondsmallest
of n values can be extended to find the smallest value also without requiring
any more comparisons to be performed
1510 show that any comparisonbased algorithm for sorting can be modified to
remove all duplicates without requiring any more comparisons to be performed
1511 show that any comparisonbased algorithm for removing duplicates from a
list of values must use n log n comparisons
1512 given a list of n elements an element of the list is a majority if it appears
more than n2 times


$$@@$$PAGE: 535
516

chap 15 lower bounds

a assume that the input is a list of integers design an algorithm that is
linear in the number of integerinteger comparisons in the worst case
that will find and report the majority if one exists and report that there
is no majority if no such integer exists in the list
b assume that the input is a list of elements that have no relative ordering
such as colors or fruit so all that you can do when you compare two
elements is ask if they are the same or not design an algorithm that is
linear in the number of elementelement comparisons in the worst case
that will find a majority if one exists and report that there is no majority
if no such element exists in the list
1513 given an undirected graph g the problem is to determine whether or not g
is connected use an adversary argument to prove that it is necessary to look
at all n2  n2 potential edges in the worst case
1514

a write an equation that describes the average cost for finding the median
b solve your equation from part a

1515

a write an equation that describes the average cost for finding the ithsmallest value in an array this will be a function of both n and i
tn i
b solve your equation from part a

1516 suppose that you have n objects that have identical weight except for one
that is a bit heavier than the others you have a balance scale you can place
objects on each side of the scale and see which collection is heavier your
goal is to find the heavier object with the minimum number of weighings
find and prove matching upper and lower bounds for this problem
1517 imagine that you are organizing a basketball tournament for 10 teams you
know that the merge insert sort will give you a full ranking of the 10 teams
with the minimum number of games played assume that each game can be
played in less than an hour and that any team can play as many games in
a row as necessary show a schedule for this tournament that also attempts
to minimize the number of total hours for the tournament and the number of
courts used if you have to make a tradeoff between the two then attempt to
minimize the total number of hours that basketball courts are idle
1518 write the complete algorithm for the merge insert sort sketched out in section 157
1519 here is a suggestion for what might be a truly optimal sorting algorithm pick
the best set of comparisons for input lists of size 2 then pick the best set of
comparisons for size 3 size 4 size 5 and so on combine them together into
one program with a big case statement is this an algorithm


$$@@$$PAGE: 536
sec 1510 projects

1510

517

projects

151 implement the medianfinding algorithm of section 156 then modify this
algorithm to allow finding the ith element for any value i  n


$$@@$$PAGE: 537

$$@@$$PAGE: 538
16
patterns of algorithms

this chapter presents several fundamental topics related to the theory of algorithms
included are dynamic programming section 161 randomized algorithms section 162 and the concept of a transform section 1635 each of these can be
viewed as an example of an algorithmic pattern that is commonly used for a
wide variety of applications in addition section 163 presents a number of numerical algorithms section 162 on randomized algorithms includes the skip list
section 1622 the skip list is a probabilistic data structure that can be used
to implement the dictionary adt the skip list is no more complicated than the
bst yet it often outperforms the bst because the skip lists efficiency is not tied
to the values or insertion order of the dataset being stored

161

dynamic programming

consider again the recursive function for computing the nth fibonacci number
long fibrint n   recursive fibonacci generator
 fibr46 is largest value that fits in a long
assertn  0  n  47 input out of range
if n  1  n  2 return 1  base cases
return fibrn1  fibrn2
 recursion


the cost of this algorithm in terms of function calls is the size of the nth fibonacci number itself which our analysis of section 142 showed to be exponential
approximately n162  why is this so expensive primarily because two recursive
calls are made by the function and the work that they do is largely redundant that
is each of the two calls is recomputing most of the series as is each subcall and so
on thus the smaller values of the function are being recomputed a huge number
of times if we could eliminate this redundancy the cost would be greatly reduced
the approach that we will use can also improve any algorithm that spends most of
its time recomputing common subproblems
519


$$@@$$PAGE: 539
520

chap 16 patterns of algorithms

one way to accomplish this goal is to keep a table of values and first check the
table to see if the computation can be avoided here is a straightforward example
of doing so
int fibrtint n 
 assume values has at least n slots and all
 slots are initialized to 0
if n  2 return 1
 base case
if valuesn  0
valuesn  fibrtn1  fibrtn2
return valuesn


this version of the algorithm will not compute a value more than once so its
cost should be linear of course we didnt actually need to use a table storing all of
the values since future computations do not need access to all prior subproblems
instead we could build the value by working from 0 and 1 up to n rather than
backwards from n down to 0 and 1 going up from the bottom we only need to
store the previous two values of the function as is done by our iterative version
long fibiint n   iterative fibonacci generator
 fibi46 is largest value that fits in a long
assertn  0  n  47 input out of range
long past prev curr  store temporary values
past  prev  curr  1
 initialize
for int i3 in i   compute next value
past  prev
 past holds fibii2
prev  curr
 prev holds fibii1
curr  past  prev
 curr now holds fibii

return curr


recomputing of subproblems comes up in many algorithms it is not so common that we can store only a few prior results as we did for fibi thus there are
many times where storing a complete table of subresults will be useful
this approach to designing an algorithm that works by storing a table of results
for subproblems is called dynamic programming the name is somewhat arcane
because it doesnt bear much obvious similarity to the process that is taking place
when storing subproblems in a table however it comes originally from the field of
dynamic control systems which got its start before what we think of as computer
programming the act of storing precomputed values in a table for later reuse is
referred to as programming in that field
dynamic programming is a powerful alternative to the standard principle of
divide and conquer in divide and conquer a problem is split into subproblems
the subproblems are solved independently and then recombined into a solution
for the problem being solved dynamic programming is appropriate whenever 1


$$@@$$PAGE: 540
521

sec 161 dynamic programming

subproblems are solved repeatedly and 2 we can find a suitable way of doing the
necessary bookkeeping dynamic programming algorithms are usually not implemented by simply using a table to store subproblems for recursive calls ie going
backwards as is done by fibrt instead such algorithms are typically implemented by building the table of subproblems from the bottom up thus fibi better represents the most common form of dynamic programming than does fibrt
even though it doesnt use the complete table
1611

the knapsack problem

we will next consider a problem that appears with many variations in a variety
of commercial settings many businesses need to package items with the greatest
efficiency one way to describe this basic idea is in terms of packing items into
a knapsack and so we will refer to this as the knapsack problem we will first
define a particular formulation of the knapsack problem and then we will discuss
an algorithm to solve it based on dynamic programming we will see other versions
of the knapsack problem in the exercises and in chapter 17
assume that we have a knapsack with a certain amount of space that we will
define using integer value k we also have n items each with a certain size such
that that item i has integer size ki  the problem is to find a subset of the n items
whose sizes exactly sum to k if one exists for example if our knapsack has
capacity k  5 and the two items are of size k1  2 and k2  4 then no such
subset exists but if we add a third item of size k3  1 then we can fill the knapsack
exactly with the second and third items we can define the problem more formally
as find s  1 2  n such that
x

ki  k

is

example 161 assume that we are given a knapsack of size k  163
and 10 items of sizes 4 9 15 19 27 44 54 68 73 101 can we find a
subset of the items that exactly fills the knapsack you should take a few
minutes and try to do this before reading on and looking at the answer
one solution to the problem is 19 27 44 73

example 162 having solved the previous example for knapsack of size
163 how hard is it now to solve for a knapsack of size 164
unfortunately knowing the answer for 163 is of almost no use at all
when solving for 164 one solution is 9 54 101


$$@@$$PAGE: 541
522

chap 16 patterns of algorithms

if you tried solving these examples you probably found yourself doing a lot of
trialanderror and a lot of backtracking to come up with an algorithm we want
an organized way to go through the possible subsets is there a way to make the
problem smaller so that we can apply divide and conquer we essentially have two
parts to the input the knapsack size k and the n items it probably will not do us
much good to try and break the knapsack into pieces and solve the subpieces since
we already saw that knowing the answer for a knapsack of size 163 did nothing to
help us solve the problem for a knapsack of size 164
so what can we say about solving the problem with or without the nth item
this seems to lead to a way to break down the problem if the nth item is not
needed for a solution that is if we can solve the problem with the first n  1 items
then we can also solve the problem when the nth item is available we just ignore
it on the other hand if we do include the nth item as a member of the solution
subset then we now would need to solve the problem with the first n  1 items
and a knapsack of size k  kn since the nth item is taking up kn space in the
knapsack
to organize this process we can define the problem in terms of two parameters
the knapsack size k and the number of items n denote a given instance of the
problem as p n k now we can say that p n k has a solution if and only if
there exists a solution for either p n  1 k or p n  1 k  kn  that is we can
solve p n k only if we can solve one of the sub problems where we use or do
not use the nth item of course the ordering of the items is arbitrary we just need
to give them some order to keep things straight
continuing this idea to solve any subproblem of size n  1 we need only to
solve two subproblems of size n  2 and so on until we are down to only one
item that either fills the knapsack or not this naturally leads to a cost expressed
by the recurrence relation t n  2t n  1  c  2n  that can be pretty
expensive
but we should quickly realize that there are only nk  1 subproblems
to solve clearly there is the possibility that many subproblems are being solved
repeatedly this is a natural opportunity to apply dynamic programming we simply build an array of size n  k  1 to contain the solutions for all subproblems
p i k 1  i  n 0  k  k
there are two approaches to actually solving the problem one is to start with
our problem of size p n k and make recursive calls to solve the subproblems
each time checking the array to see if a subproblem has been solved and filling
in the corresponding cell in the array whenever we get a new subproblem solution
the other is to start filling the array for row 1 which indicates a successful solution
only for a knapsack of size k1  we then fill in the succeeding rows from i  2 to
n left to right as follows
if p n  1 k has a solution


$$@@$$PAGE: 542
sec 161 dynamic programming

523

then p n k has a solution
else if p n  1 k  kn  has a solution
then p n k has a solution
else p n k has no solution
in other words a new slot in the array gets its solution by looking at two slots in
the preceding row since filling each slot in the array takes constant time the total
cost of the algorithm is nk
example 163 solve the knapsack problem for k  10 and five items
with sizes 9 2 7 4 1 we do this by building the following array
0 1 2 3 4 5 6
7
8
9
10
k1  9 o        
i

k2  2 o  i       o

i
 io 
k3  7 o  o    
k4  4 o  o  i  i
o  o

k5  1 o i o i o i o io i
o
i
key
 no solution for p i k
o solutions for p i k with i omitted
i solutions for p i k with i included
io solutions for p i k with i included and omitted
for example p 3 9 stores value io it contains o because p 2 9
has a solution it contains i because p 2 2  p 2 9  7 has a solution
since p 5 10 is marked with an i it has a solution we can determine
what that solution actually is by recognizing that it includes the 5th item
of size 1 which then leads us to look at the solution for p 4 9 this
in turn has a solution that omits the 4th item leading us to p 3 9 at
this point we can either use the third item or not we can find a solution
by taking one branch we can find all solutions by following all branches
when there is a choice

1612

allpairs shortest paths

we next consider the problem of finding the shortest distance between all pairs of
vertices in the graph called the allpairs shortestpaths problem to be precise
for every u v  v calculate du v
one solution is to run dijkstras algorithm for finding the singlesource shortest
path see section 1141 v times each time computing the shortest path from a
different start vertex if g is sparse that is e  v then this is a good


$$@@$$PAGE: 543
524

chap 16 patterns of algorithms

1
7

1
4
5
0



3





2

11

2

3

12


figure 161 an example of kpaths in floyds algorithm path 1 3 is a 0path
by definition path 3 0 2 is not a 0path but it is a 1path as well as a 2path a
3path and a 4path because the largest intermediate vertex is 0 path 1 3 2 is
a 4path but not a 3path because the intermediate vertex is 3 all paths in this
graph are 4paths

solution because the total cost will be v2  ve log v  v2 log v
for the version of dijkstras algorithm based on priority queues for a dense graph
the priority queue version of dijkstras algorithm yields a cost of v3 log v
but the version using minvertex yields a cost of v3 
another solution that limits processing time to v3  regardless of the number of edges is known as floyds algorithm it is an example of dynamic programming the chief problem with solving this problem is organizing the search process
so that we do not repeatedly solve the same subproblems we will do this organization through the use of the kpath define a kpath from vertex v to vertex u to
be any path whose intermediate vertices aside from v and u all have indices less
than k a 0path is defined to be a direct edge from v to u figure 161 illustrates
the concept of kpaths
define dk v u to be the length of the shortest kpath from vertex v to vertex u
assume that we already know the shortest kpath from v to u the shortest k  1path either goes through vertex k or it does not if it does go through k then
the best path is the best kpath from v to k followed by the best kpath from k
to u otherwise we should keep the best kpath seen before floyds algorithm
simply checks all of the possibilities in a triple loop here is the implementation
for floyds algorithm at the end of the algorithm array d stores the allpairs
shortest distances


$$@@$$PAGE: 544
sec 162 randomized algorithms

525

 floyds allpairs shortest paths algorithm
 store the pairwise distances in d
void floydgraph g 
for int i0 ign i  initialize d with weights
for int j0 jgn j
if gweighti j  0 dij  gweighti j
for int k0 kgn k  compute all k paths
for int i0 ign i
for int j0 jgn j
if dij  dik  dkj
dij  dik  dkj


clearly this algorithm requires v3  running time and it is the best choice
for dense graphs because it is relatively fast and easy to implement

162

randomized algorithms

in this section we will consider how introducing randomness into our algorithms
might speed things up although perhaps at the expense of accuracy but often we
can reduce the possibility for error to be as low as we like while still speeding up
the algorithm
1621

randomized algorithms for finding large values

in section 151 we determined that the lower bound cost of finding the maximum
value in an unsorted list is n this is the least time needed to be certain that we
have found the maximum value but what if we are willing to relax our requirement
for certainty the first question is what do we mean by this there are many
aspects to certainty and we might relax the requirement in various ways
there are several possible guarantees that we might require from an algorithm
that produces x as the maximum value when the true maximum is y  so far
we have assumed that we require x to equal y  this is known as an exact or
deterministic algorithm to solve the problem we could relax this and require only
that xs rank is close to y s rank perhaps within a fixed distance or percentage
this is known as an approximation algorithm we could require that x is usually
y  this is known as a probabilistic algorithm finally we could require only that
xs rank is usually close to y s rank this is known as a heuristic algorithm
there are also different ways that we might choose to sacrifice reliability for
speed these types of algorithms also have names
1 las vegas algorithms we always find the maximum value and usually
we find it fast such algorithms have a guaranteed result but do not guarantee
fast running time


$$@@$$PAGE: 545
526

chap 16 patterns of algorithms

2 monte carlo algorithms we find the maximum value fast or we dont get
an answer at all but fast while such algorithms have good running time
their result is not guaranteed
here is an example of an algorithm for finding a large value that gives up its
guarantee of getting the best value in exchange for an improved running time this
is an example of a probabilistic algorithm since it includes steps that are affected
by random events choose m elements at random and pick the best one of those
as the answer for large n if m  log n the answer is pretty good the cost is
m  1 compares since we must find the maximum of m values but we dont
know for sure what we will get however we can estimate that the rank will be
mn
about m1
 for example if n  1 000 000 and m  log n  20 then we expect
that the largest of the 20 randomly selected values be among the top 5 of the n
values
next consider a slightly different problem where the goal is to pick a number
in the upper half of n values we would pick the maximum from among the first
n1
2 values for a cost of n2 comparisons can we do better than this not if we
want to guarantee getting the correct answer but if we are willing to accept near
certainty instead of absolute certainty we can gain a lot in terms of speed
as an alternative consider this probabilistic algorithm pick 2 numbers and
choose the greater this will be in the upper half with probability 34 since it is
not in the upper half only when both numbers we choose happen to be in the lower
half is a probability of 34 not good enough then we simply pick more numbers
for k numbers the greatest is in upper half with probability 1  21k  regardless of
the number n that we pick from so long as n is much larger than k otherwise
the chances might become even better if we pick ten numbers then the chance
of failure is only one in 210  1024 what if we really want to be sure because
lives depend on drawing a number from the upper half if we pick 30 numbers
we can fail only one time in a billion if we pick enough numbers then the chance
of picking a small number is less than the chance of the power failing during the
computation picking 100 numbers means that we can fail only one time in 10100
which is less chance than any disaster that you can imagine disrupting the process
1622

skip lists

this section presents a probabilistic search structure called the skip list like
bsts skip lists are designed to overcome a basic limitation of arraybased and
linked lists either search or update operations require linear time the skip list
is an example of a probabilistic data structure because it makes some of its
decisions at random
skip lists provide an alternative to the bst and related tree structures the primary problem with the bst is that it may easily become unbalanced the 23 tree
of chapter 10 is guaranteed to remain balanced regardless of the order in which data


$$@@$$PAGE: 546
sec 162 randomized algorithms

527

values are inserted but it is rather complicated to implement chapter 13 presents
the avl tree and the splay tree which are also guaranteed to provide good performance but at the cost of added complexity as compared to the bst the skip
list is easier to implement than known balanced tree structures the skip list is
not guaranteed to provide good performance where good performance is defined
as log n search insertion and deletion time but it will provide good performance with extremely high probability unlike the bst which has a good chance
of performing poorly as such it represents a good compromise between difficulty
of implementation and performance
figure 162 illustrates the concept behind the skip list figure 162a shows a
simple linked list whose nodes are ordered by key value to search a sorted linked
list requires that we move down the list one node at a time visiting n nodes
in the average case what if we add a pointer to every other node that lets us skip
alternating nodes as shown in figure 162b define nodes with a single pointer
as level 0 skip list nodes and nodes with two pointers as level 1 skip list nodes
to search follow the level 1 pointers until a value greater than the search key
has been found go back to the previous level 1 node then revert to a level 0 pointer
to travel one more node if necessary this effectively cuts the work in half we
can continue adding pointers to selected nodes in this way  give a third pointer
to every fourth node give a fourth pointer to every eighth node and so on  until
we reach the ultimate of log n pointers in the first and middle nodes for a list of
n nodes as illustrated in figure 162c to search start with the bottom row of
pointers going as far as possible and skipping many nodes at a time then shift
up to shorter and shorter steps as required with this arrangement the worstcase
number of accesses is log n
we will store with each skip list node an array named forward that stores
the pointers as shown in figure 162c position forward0 stores a level 0
pointer forward1 stores a level 1 pointer and so on the skip list object
includes data member level that stores the highest level for any node currently
in the skip list the skip list stores a header node named head with level
pointers the find function is shown in figure 163
searching for a node with value 62 in the skip list of figure 162c begins at
the header node follow the header nodes pointer at level which in this example
is level 2 this points to the node with value 31 because 31 is less than 62 we
next try the pointer from forward2 of 31s node to reach 69 because 69 is
greater than 62 we cannot go forward but must instead decrement the current level
counter to 1 we next try to follow forward1 of 31 to reach the node with
value 58 because 58 is smaller than 62 we follow 58s forward1 pointer
to 69 because 69 is too big follow 58s level 0 pointer to 62 because 62 is not
less than 62 we fall out of the while loop and move one step forward to the node
with value 62


$$@@$$PAGE: 547
528

chap 16 patterns of algorithms

head
5

25

30

31

42

58

62

69

42

58

62

69

42

58

62

69

0
a

head
5

25

30

31

0
1
b

head
5

25

30

31

0
1
2
c

figure 162 illustration of the skip list concept a a simple linked list
b augmenting the linked list with additional pointers at every other node to
find the node with key value 62 we visit the nodes with values 25 31 58 and 69
then we move from the node with key value 58 to the one with value 62 c the
ideal skip list guaranteeing olog n search time to find the node with key
value 62 we visit nodes in the order 31 69 58 then 69 again and finally 62

e findconst key k const 
skipnodekeye x  head
 dummy header node
for int ilevel i0 i
while xforwardi  null 
k  xforwardik
x  xforwardi
x  xforward0  move to actual record if it exists
if x  null  k  xk return xit
return null

figure 163 implementation for the skip list find function


$$@@$$PAGE: 548
sec 162 randomized algorithms

529

void insertconst key k const e it 
int i
skipnodekeye x  head
 start at header node
int newlevel  randomlevel  select level for new node
if newlevel  level 
 new node is deepest in list
adjustheadnewlevel
 add null pointers to header
level  newlevel

skipnodekeye updatelevel1  track level ends
forilevel i0 i 
 search for insert position
whilexforwardi  null  xforwardik  k
x  xforwardi
updatei  x
 keep track of end at level i

x  new skipnodekeyek it newlevel
 new node
for i0 inewlevel i 
 splice into list
xforwardi  updateiforwardi  where x points
updateiforwardi  x
 what points to x

reccount

figure 164 implementation for the skip list insert function

the ideal skip list of figure 162c has been organized so that if the first and
last nodes are not counted half of the nodes have only one pointer one quarter
have two one eighth have three and so on the distances are equally spaced in
effect this is a perfectly balanced skip list maintaining such balance would be
expensive during the normal process of insertions and deletions the key to skip
lists is that we do not worry about any of this whenever inserting a node we
assign it a level ie some number of pointers the assignment is random using
a geometric distribution yielding a 50 probability that the node will have one
pointer a 25 probability that it will have two and so on the following function
determines the level based on such a distribution
 pick a level using an exponential distribution
int randomlevelvoid 
int level  0
while random2  0 level
return level


once the proper level for the node has been determined the next step is to find
where the node should be inserted and link it in as appropriate at all of its levels
figure 164 shows an implementation for inserting a new value into the skip list
figure 165 illustrates the skip list insertion process in this example we
begin by inserting a node with value 10 into an empty skip list assume that
randomlevel returns a value of 1 ie the node is at level 1 with 2 pointers
because the empty skip list has no nodes the level of the list and thus the level


$$@@$$PAGE: 549
530

chap 16 patterns of algorithms

head

head

10

10

a

20

b

head

head

5

10

20

2

c

5

10

20

d

head

2

5

10

20

30

e

figure 165 illustration of skip list insertion a the skip list after inserting
initial value 10 at level 1 b the skip list after inserting value 20 at level 0
c the skip list after inserting value 5 at level 0 d the skip list after inserting
value 2 at level 3 e the final skip list after inserting value 30 at level 2


$$@@$$PAGE: 550
sec 162 randomized algorithms

531

of the header node must be set to 1 the new node is inserted yielding the skip
list of figure 165a
next insert the value 20 assume this time that randomlevel returns 0 the
search process goes to the node with value 10 and the new node is inserted after
as shown in figure 165b the third node inserted has value 5 and again assume
that randomlevel returns 0 this yields the skip list of figure 165c
the fourth node inserted has value 2 and assume that randomlevel returns 3 this means that the level of the skip list must rise causing the header
node to gain an additional two null pointers at this point the new node is
added to the front of the list as shown in figure 165d
finally insert a node with value 30 at level 2 this time let us take a close
look at what array update is used for it stores the farthest node reached at each
level during the search for the proper location of the new node the search process begins in the header node at level 3 and proceeds to the node storing value 2
because forward3 for this node is null we cannot go further at this level
thus update3 stores a pointer to the node with value 2 likewise we cannot
proceed at level 2 so update2 also stores a pointer to the node with value 2
at level 1 we proceed to the node storing value 10 this is as far as we can go
at level 1 so update1 stores a pointer to the node with value 10 finally at
level 0 we end up at the node with value 20 at this point we can add in the new
node with value 30 for each value i the new nodes forwardi pointer is
set to be updateiforwardi and the nodes stored in updatei for
indices 0 through 2 have their forwardi pointers changed to point to the new
node this splices the new node into the skip list at all levels
the remove function is left as an exercise it is similar to insertion in that the
update array is built as part of searching for the record to be deleted then those
nodes specified by the update array have their forward pointers adjusted to point
around the node being deleted
a newly inserted node could have a high level generated by randomlevel
or a low level it is possible that many nodes in the skip list could have many
pointers leading to unnecessary insert cost and yielding poor ie n performance during search because not many nodes will be skipped conversely too
many nodes could have a low level in the worst case all nodes could be at level 0
equivalent to a regular linked list if so search will again require n time however the probability that performance will be poor is quite low there is only one
chance in 1024 that ten nodes in a row will be at level 0 the motto of probabilistic
data structures such as the skip list is dont worry be happy we simply accept
the results of randomlevel and expect that probability will eventually work in
our favor the advantage of this approach is that the algorithms are simple while
requiring only log n time for all operations in the average case


$$@@$$PAGE: 551
532

chap 16 patterns of algorithms

in practice the skip list will probably have better performance than a bst the
bst can have bad performance caused by the order in which data are inserted for
example if n nodes are inserted into a bst in ascending order of their key value
then the bst will look like a linked list with the deepest node at depth n  1 the
skip lists performance does not depend on the order in which values are inserted
into the list as the number of nodes in the skip list increases the probability of
encountering the worst case decreases geometrically thus the skip list illustrates
a tension between the theoretical worst case in this case n for a skip list
operation and a rapidly increasing probability of averagecase performance of
log n that characterizes probabilistic data structures

163

numerical algorithms

this section presents a variety of algorithms related to mathematical computations
on numbers examples are activities like multiplying two numbers or raising a
number to a given power in particular we are concerned with situations where
builtin integer or floatingpoint operations cannot be used because the values being
operated on are too large similar concerns arise for operations on polynomials or
matrices
since we cannot rely on the hardware to process the inputs in a single constanttime operation we are concerned with how to most effectively implement the operation to minimize the time cost this begs a question as to how we should apply
our normal measures of asymptotic cost in terms of growth rates on input size
first what is an instance of addition or multiplication each value of the operands
yields a different problem instance and what is the input size when multiplying
two numbers if we view the input size as two since two numbers are input then
any nonconstanttime algorithm has a growth rate that is infinitely high compared
to the growth of the input this makes no sense especially in light of the fact that
we know from grade school arithmetic that adding or multiplying numbers does
seem to get more difficult as the value of the numbers involved increases in fact
we know from standard grade school algorithms that the cost of standard addition
is linear on the number of digits being added and multiplication has cost n  m
when multiplying an mdigit number by an ndigit number
the number of digits for the operands does appear to be a key consideration
when we are performing a numeric algorithm that is sensitive to input size the
number of digits is simply the log of the value for a suitable base of the log thus
for the purpose of calculating asymptotic growth rates of algorithms we will consider the size of an input value to be the log of that value given this view there
are a number of features that seem to relate such operations
 arithmetic operations on large values are not cheap
 there is only one instance of value n


$$@@$$PAGE: 552
533

sec 163 numerical algorithms

 there are 2k instances of length k or less
 the size length of value n is log n
 the cost of a particular algorithm can decrease when n increases in value
say when going from a value of 2k  1 to 2k to 2k  1 but generally
increases when n increases in length
1631

exponentiation

we will start our examination of standard numerical algorithms by considering how
to perform exponentiation that is how do we compute mn  we could multiply
by m a total of n  1 times can we do better yes there is a simple divide
and conquer approach that we can use we can recognize that when n is even
mn  mn2 mn2  if n is odd then mn  mbn2c mbn2c m this leads to the
following recursive algorithm
int powerbase exp 
if exp  0 return 1
int half  powerbase exp2  integer division of exp
half  half  half
if oddexp then half  half  base
return half


function power has recurrence relation

0
n1
f n 
f bn2c  1  n mod 2 n  1
whose solution is
f n  blog nc  n  1
where  is the number of 1s in the binary representation of n
how does this cost compare with the problem size the original problem size
is log m  log n and the number of multiplications required is log n this is far
better in fact exponentially better than performing n  1 multiplications
1632

largest common factor

we will next present euclids algorithm for finding the largest common factor
lcf for two integers the lcf is the largest integer that divides both inputs
evenly
first we make this observation if k divides n and m then k divides n  m we
know this is true because if k divides n then n  ak for some integer a and if k
divides m then m  bk for some integer b so lcf n m  lcf n  m n 
lcf m n  m  lcf m n


$$@@$$PAGE: 553
534

chap 16 patterns of algorithms

now for any value n there exists k and l such that
n  km  l where m  l  0
from the definition of the mod function we can derive the fact that
n  bnmcm  n mod m
since the lcf is a factor of both n and m and since n  km  l the lcf must
therefore be a factor of both km and l and also the largest common factor of each
of these terms as a consequence lcf n m  lcf m l  lcf m n mod
m
this observation leads to a simple algorithm we will assume that n  m at
each iteration we replace n with m and m with n mod m until we have driven m
to zero
int lcfint n int m 
if m  0 return n
return lcfm n  m


to determine how expensive this algorithm is we need to know how much
progress we are making at each step note that after two iterations we have replaced n with n mod m so the key question becomes how big is n mod m
relative to n
n  m  nm  1
 2bnmc  nm
 mbnmc  n2
 n  n2  n  mbnmc  n mod m
 n2  n mod m
thus function lcf will halve its first parameter in no more than 2 iterations
the total cost is then olog n
1633

matrix multiplication

the standard algorithm for multiplying two n  n matrices requires n3  time
it is possible to do better than this by rearranging and grouping the multiplications
in various ways one example of this is known as strassens matrix multiplication
algorithm
for simplicity we will assume that n is a power of two in the following a
and b are n  n arrays while aij and bij refer to arrays of size n2  n2 using


$$@@$$PAGE: 554
sec 163 numerical algorithms

535

this notation we can think of matrix multiplication using divide and conquer in the
following way


 

a11 a12
b11 b12
a11 b11  a12 b21
a11 b12  a12 b22


a21 a22
b21 b22
a21 b11  a22 b21
a21 b12  a22 b22
of course each of the multiplications and additions on the right side of this
equation are recursive calls on arrays of half size and additions of arrays of half
size respectively the recurrence relation for this algorithm is
t n  8t n2  4n22  n3 
this closed form solution can easily be obtained by applying the master theorem 141
strassens algorithm carefully rearranges the way that the various terms are
multiplied and added together it does so in a particular order as expressed by the
following equation


 

a11 a12
b11 b12
s1  s2  s4  s6
s4  s5


a21 a22
b21 b22
s6  s7
s2  s3  s5  s7
in other words the result of the multiplication for an n  n array is obtained by
a different series of matrix multiplications and additions for n2  n2 arrays
multiplications between subarrays also use strassens algorithm and the addition
of two subarrays requires n2  time the subfactors are defined as follows
s1  a12  a22   b21  b22 
s2  a11  a22   b11  b22 
s3  a11  a21   b11  b12 
s4  a11  a12   b22
s5  a11  b12  b22 
s6  a22  b21  b11 
s7  a21  a22   b11
with a little effort you should be able to verify that this peculiar combination of
operations does in fact produce the correct answer
now looking at the list of operations to compute the s factors and then counting the additionssubtractions needed to put them together to get the final answers
we see that we need a total of seven array multiplications and 18 array additionssubtractions to do the job this leads to the recurrence
t n  7t n2  18n22
t n  nlog2 7   n281 


$$@@$$PAGE: 555
536

chap 16 patterns of algorithms

we obtained this closed form solution again by applying the master theorem
unfortunately while strassens algorithm does in fact reduce the asymptotic
complexity over the standard algorithm the cost of the large number of addition
and subtraction operations raises the constant factor involved considerably this
means that an extremely large array size is required to make strassens algorithm
practical in real applications
1634

random numbers

the success of randomized algorithms such as were presented in section 162 depend on having access to a good random number generator while modern compilers are likely to include a random number generator that is good enough for most
purposes it is helpful to understand how they work and to even be able to construct
your own in case you dont trust the one provided this is easy to do
first let us consider what a random sequence from the following list which
appears to be a sequence of random numbers
 1 1 1 1 1 1 1 1 1 
 1 2 3 4 5 6 7 8 9 
 2 7 1 8 2 8 1 8 2 
in fact all three happen to be the beginning of a some sequence in which one
could continue the pattern to generate more values in case you do not recognize
it the third one is the initial digits of the irrational constant e viewed as a series
of digits ideally every possible sequence has equal probability of being generated
even the three sequences above in fact definitions of randomness generally have
features such as
 one cannot predict the next item the series is unpredictable
 the series cannot be described more briefly than simply listing it out this is
the equidistribution property
there is no such thing as a random number sequence only random enough
sequences a sequence is pseudorandom if no future term can be predicted in
polynomial time given all past terms
most computer systems use a deterministic algorithm to select pseudorandom
numbers1 the most commonly used approach historically is known as the linear
congruential method lcm the lcm method is quite simple we begin by
picking a seed that we will call r1 then we can compute successive terms as
follows
ri  ri  1  b mod t
where b and t are constants
1

another approach is based on using a computer chip that generates random numbers resulting
from thermal noise in the system time will tell if this approach replaces deterministic approaches


$$@@$$PAGE: 556
537

sec 163 numerical algorithms

by definition of the mod function all generated numbers must be in the range
0 to t  1 now consider what happens when ri  rj for values i and j of
course then ri  1  rj  1 which means that we have a repeating cycle
since the values coming out of the random number generator are between 0 and
t  1 the longest cycle that we can hope for has length t in fact since r0  0 it
cannot even be quite this long it turns out that to get a good result it is crucial to
pick good values for both b and t to see why consider the following example
example 164 given a t value of 13 we can get very different results
depending on the b value that we pick in ways that are hard to predict
ri  6ri  1 mod 13 
 1 6 10 8 9 2 12 7 3 5 4 11 1 
ri  7ri  1 mod 13 
 1 7 10 5 9 11 12 6 3 8 4 2 1 
ri  5ri  1 mod 13 
 1 5 12 8 1 
 2 10 11 3 2 
 4 7 9 6 4 
 0 0 
clearly a b value of 5 is far inferior to b values of 6 or 7 in this example
if you would like to write a simple lcm random number generator of your
own an effective one can be made with the following formula
ri  16807ri  1 mod 231  1
1635

the fast fourier transform

as noted at the beginning of this section multiplication is considerably more difficult than addition the cost to multiply two nbit numbers directly is on2  while
addition of two nbit numbers is on
recall from section 23 that one property of logarithms is
log nm  log n  log m
thus if taking logarithms and antilogarithms were cheap then we could reduce
multiplication to addition by taking the log of the two operands adding and then
taking the antilog of the sum
under normal circumstances taking logarithms and antilogarithms is expensive and so this reduction would not be considered practical however this reduction is precisely the basis for the slide rule the slide rule uses a logarithmic
scale to measure the lengths of two numbers in effect doing the conversion to logarithms automatically these two lengths are then added together and the inverse


$$@@$$PAGE: 557
538

chap 16 patterns of algorithms

logarithm of the sum is read off another logarithmic scale the part normally considered expensive taking logarithms and antilogarithms is cheap because it is a
physical part of the slide rule thus the entire multiplication process can be done
cheaply via a reduction to addition in the days before electronic calculators slide
rules were routinely used by scientists and engineers to do basic calculations of this
nature
now consider the problem of multiplying polynomials a vector a of n values
can uniquely represent a polynomial of degree n  1 expressed as
pa x 

n1
x

a i xi 

i0

alternatively a polynomial can be uniquely represented by a list of its values at
n distinct points finding the value for a polynomial at a given point is called
evaluation finding the coefficients for the polynomial given the values at n points
is called interpolation
to multiply two n  1degree polynomials a and b normally takes n2  coefficient multiplications however if we evaluate both polynomials at the same
points we can simply multiply the corresponding pairs of values to get the corresponding values for polynomial ab
example 165 polynomial a x2  1
polynomial b 2x2  x  1
polynomial ab 2x4  x3  3x2  x  1
when we multiply the evaluations of a and b at points 0 1 and 1 we
get the following results
ab1  24  8
ab0  11  1
ab1  22  4
these results are the same as when we evaluate polynomial ab at these
points
note that evaluating any polynomial at 0 is easy if we evaluate at 1 and 1 we can share a lot of the work between the two evaluations but we would
need five points to nail down polynomial ab since it is a degree4 polynomial
fortunately we can speed processing for any pair of values c and c this seems
to indicate some promising ways to speed up the process of evaluating polynomials
but evaluating two points in roughly the same time as evaluating one point only
speeds the process by a constant factor is there some way to generalized these


$$@@$$PAGE: 558
539

sec 163 numerical algorithms

observations to speed things up further and even if we do find a way to evaluate
many points quickly we will also need to interpolate the five values to get the
coefficients of ab back
so we see that we could multiply two polynomials in less than n2  operations
if a fast way could be found to do evaluationinterpolation of 2n  1 points before
considering further how this might be done first observe again the relationship
between evaluating a polynomial at values c and c in general we can write
pa x  ea x  oa x where ea is the even powers and oa is the odd powers
so
n21
n21
x
x
2i
pa x 
a2i x 
a2i1 x2i1
i0

i0

the significance is that when evaluating the pair of values c and c we get
ea c  oa c  ea c  oa c
oa c  oa c
thus we only need to compute the es and os once instead of twice to get both
evaluations
the key to fast polynomial multiplication is finding the right points to use for
evaluationinterpolation to make the process efficient in particular we want to take
advantage of symmetries such as the one we see for evaluating x and x but we
need to find even more symmetries between points if we want to do more than cut
the work in half we have to find symmetries not just between pairs of values but
also further symmetries between pairs of pairs and then pairs of pairs of pairs and
so on
recall that a complex number z has a real component and an imaginary component we can consider the position of z on a number line if we use the y dimension
for the imaginary component now we will define a primitive nth root of unity if
1 z n  1 and
2 z k 
6 1 for 0  k  n
z 0  z 1   z n1 are called the nth roots of unity for example when n  4 then
z  i or z  i in general we have the identities ei  1 and z j  e2ijn 
12jn  the significance is that we can find as many points on a unit circle as we
would need see figure 166 but these points are special in that they will allow
us to do just the right computation necessary to get the needed symmetries to speed
up the overall process of evaluating many points at once
the next step is to define how the computation is done define an n  n matrix
az with row i and column j as
az  z ij 


$$@@$$PAGE: 559
540

chap 16 patterns of algorithms

i

i

1

1

1

i

1

i

figure 166 examples of the 4th and 8th roots of unity

the idea is that there is a row for each root row i for z i  while the columns correspond to the power of the exponent of the x value in the polynomial for example
when n  4 we have z  i thus the az array appears as follows
1
1
1
1
1
i 1 i
az 
1 1
1 1
1 i 1
i
let a  a0  a1   an1 t be a vector that stores the coefficients for the polynomial being evaluated we can then do the calculations to evaluate the polynomial
at the nth roots of unity by multiplying the az matrix by the coefficient vector the
resulting vector fz is called the discrete fourier transform for the polynomial
fz  az a  b
bi 

n1
x

ak z ik 

k0

when n  8 then z 
follows



i since



8

i  1 so the corresponding matrix is as

1
1
1
1
1
1
1
1
1
i
i
i i 1  i i i i
1
i
1
i
1
i


i 1

1
i i i
i 1 i i
i  i
az 
1
1
1
1
1
1
1
1




1  i
i i i 1
i i
i i
1
i 1
1
i 1

i

i
1 i i i  i 1
i i
i
i
we still have two problems we need to be able to multiply this matrix and
the vector faster than just by performing a standard matrixvector multiplication


$$@@$$PAGE: 560
541

sec 163 numerical algorithms

otherwise the cost is still n2 multiplies to do the evaluation even if we can multiply the matrix and vector cheaply we still need to be able to reverse the process
that is after transforming the two input polynomials by evaluating them and then
pairwise multiplying the evaluated points we must interpolate those points to get
the resulting polynomial back that corresponds to multiplying the original input
polynomials
the interpolation step is nearly identical to the evaluation step
0
0
fz1  a1
z b a

we need to find a1
z  this turns out to be simple to compute and is defined as
follows
1
a1
a 
z 
n 1z
in other words interpolation the inverse transformation requires the same computation as evaluation except that we substitute 1z for z and multiply by 1n at
the end so if we can do one fast we can do the other fast
if you examine the example az matrix for n  8 you should see that there
are symmetries within the matrix for example the top half is identical to the
bottom half with suitable sign changes on some rows and columns likewise for the
left and right halves an efficient divide and conquer algorithm exists to perform
both the evaluation and the interpolation in n log n time this is called the
discrete fourier transform dft it is a recursive function that decomposes
the matrix multiplications taking advantage of the symmetries made available by
doing evaluation at the nth roots of unity the algorithm is as follows
fourier transformdouble polynomial int n 
 compute the fourier transform of polynomial
 with degree n polynomial is a list of
 coefficients indexed from 0 to n1 n is
 assumed to be a power of 2
double evenn2 oddn2 list1n2 list2n2
if n1 return polynomial0
for j0 jn21 j 
evenj  polynomial2j
oddj  polynomial2j1

list1  fourier transformeven n2
list2  fourier transformodd n2
for j0 jn1 j 
imaginary z  powe 2ipijn
k  j  n2
polynomialj  list1k  zlist2k

return polynomial



$$@@$$PAGE: 561
542

chap 16 patterns of algorithms

thus the full process for multiplying polynomials a and b using the fourier
transform is as follows
1 represent an n  1degree polynomial as 2n  1 coefficients
a0  a1   an1  0  0
2 perform fourier transform on the representations for a and b
3 pairwise multiply the results to get 2n  1 values
4 perform the inverse fourier transform to get the 2n  1 degree polynomial ab

164

further reading

for further information on skip lists see skip lists a probabilistic alternative
to balanced trees by william pugh pug90

165

exercises

161 solve towers of hanoi using a dynamic programming algorithm
162 there are six possible permutations of the lines
for int k0 kgn k
for int i0 ign i
for int j0 jgn j

in floyds algorithm which ones give a correct algorithm
163 show the result of running floyds allpairs shortestpaths algorithm on the
graph of figure 1126
164 the implementation for floyds algorithm given in section 1612 is inefficient for adjacency lists because the edges are visited in a bad order when
initializing array d what is the cost of of this initialization step for the adjacency list how can this initialization step be revised so that it costs v2 
in the worst case
165 state the greatest possible lower bound that you can prove for the allpairs
shortestpaths problem and justify your answer
166 show the skip list that results from inserting the following values draw
the skip list after each insert with each value assume the depth of its
corresponding node is as given in the list


$$@@$$PAGE: 562
543

sec 166 projects

value
5
20
30
2
25
26
31

depth
2
0
0
0
1
3
0

167 if we had a linked list that would never be modified we can use a simpler
approach than the skip list to speed access the concept would remain the
same in that we add additional pointers to list nodes for efficient access to the
ith element how can we add a second pointer to each element of a singly
linked list to allow access to an arbitrary element in olog n time
168 what is the expected average number of pointers for a skip list node
169 write a function to remove a node with given value from a skip list
1610 write a function to find the ith node on a skip list

166

projects

161 complete the implementation of the skip listbased dictionary begun in section 1622
162 implement both a standard n3  matrix multiplication algorithm and strassens matrix multiplication algorithm see exercise 141633 using empirical testing try to estimate the constant factors for the runtime equations of
the two algorithms how big must n be before strassens algorithm becomes
more efficient than the standard algorithm


$$@@$$PAGE: 563

$$@@$$PAGE: 564
17
limits to computation

this book describes data structures that can be used in a wide variety of problems
and many examples of efficient algorithms in general our search algorithms strive
to be at worst in olog n to find a record and our sorting algorithms strive to be
in on log n a few algorithms have higher asymptotic complexity both floyds
allpairs shortestpaths algorithm and standard matrix multiply have running times
of n3  though for both the amount of data being processed is n2 
we can solve many problems efficiently because we have available and choose
to use efficient algorithms given any problem for which you know some algorithm it is always possible to write an inefficient algorithm to solve the problem
for example consider a sorting algorithm that tests every possible permutation of
its input until it finds the correct permutation that provides a sorted list the running
time for this algorithm would be unacceptably high because it is proportional to
the number of permutations which is n for n inputs when solving the minimumcost spanning tree problem if we were to test every possible subset of edges to
see which forms the shortest minimum spanning tree the amount of work would
be proportional to 2e for a graph with e edges fortunately for both of these
problems we have more clever algorithms that allow us to find answers relatively
quickly without explicitly testing every possible solution
unfortunately there are many computing problems for which the best possible
algorithm takes a long time to run a simple example is the towers of hanoi
problem which requires 2n moves to solve a tower with n disks it is not possible
for any computer program that solves the towers of hanoi problem to run in less
than 2n  time because that many moves must be printed out
besides those problems whose solutions must take a long time to run there are
also many problems for which we simply do not know if there are efficient algorithms or not the best algorithms that we know for such problems are very slow
but perhaps there are better ones waiting to be discovered of course while having
a problem with high running time is bad it is even worse to have a problem that
cannot be solved at all such problems do exist and are discussed in section 173
545


$$@@$$PAGE: 565
546

chap 17 limits to computation

this chapter presents a brief introduction to the theory of expensive and impossible problems section 171 presents the concept of a reduction which is the
central tool used for analyzing the difficulty of a problem as opposed to analyzing
the cost of an algorithm reductions allow us to relate the difficulty of various
problems which is often much easier than doing the analysis for a problem from
first principles section 172 discusses hard problems by which we mean problems that require or at least appear to require time exponential on the input size
finally section 173 considers various problems that while often simple to define
and comprehend are in fact impossible to solve using a computer program the
classic example of such a problem is deciding whether an arbitrary computer program will go into an infinite loop when processing a specified input this is known
as the halting problem

171

reductions

we begin with an important concept for understanding the relationships between
problems called reduction reduction allows us to solve one problem in terms
of another equally importantly when we wish to understand the difficulty of a
problem reduction allows us to make relative statements about upper and lower
bounds on the cost of a problem as opposed to an algorithm or program
because the concept of a problem is discussed extensively in this chapter we
want notation to simplify problem descriptions throughout this chapter a problem
will be defined in terms of a mapping between inputs and outputs and the name of
the problem will be given in all capital letters thus a complete definition of the
sorting problem could appear as follows
sorting
input a sequence of integers x0  x1  x2   xn1 
output a permutation y0  y1  y2   yn1 of the sequence such that yi  yj
whenever i  j
when you buy or write a program to solve one problem such as sorting you
might be able to use it to help solve a different problem this is known in software
engineering as software reuse to illustrate this let us consider another problem
pairing
input two sequences of integers x  x0  x1   xn1  and y 
y0  y1   yn1 
output a pairing of the elements in the two sequences such that the least
value in x is paired with the least value in y the next least value in x is paired
with the next least value in y and so on


$$@@$$PAGE: 566
547

sec 171 reductions

23

48

42

59

17

11

93

89

88

12

12

91

57

64

90

34

figure 171 an illustration of pairing the two lists of numbers are paired up
so that the least values from each list make a pair the next smallest values from
each list make a pair and so on

figure 171 illustrates pairing one way to solve pairing is to use an existing sorting program to sort each of the two sequences and then pair off items based
on their position in sorted order technically we say that in this solution pairing
is reduced to sorting because sorting is used to solve pairing
notice that reduction is a threestep process the first step is to convert an
instance of pairing into two instances of sorting the conversion step in this
example is not very interesting it simply takes each sequence and assigns it to an
array to be passed to sorting the second step is to sort the two arrays ie apply
sorting to each array the third step is to convert the output of sorting to
the output for pairing this is done by pairing the first elements in the sorted
arrays the second elements and so on
a reduction of pairing to sorting helps to establish an upper bound on the
cost of pairing in terms of asymptotic notation assuming that we can find one
method to convert the inputs to pairing into inputs to sorting fast enough
and a second method to convert the result of sorting back to the correct result
for pairing fast enough then the asymptotic cost of pairing cannot be more
than the cost of sorting in this case there is little work to be done to convert
from pairing to sorting or to convert the answer from sorting back to the
answer for pairing so the dominant cost of this solution is performing the sort
operation thus an upper bound for pairing is in on log n
it is important to note that the pairing problem does not require that elements
of the two sequences be sorted this is merely one possible way to solve the problem pairing only requires that the elements of the sequences be paired correctly
perhaps there is another way to do it certainly if we use sorting to solve pairing the algorithms will require n log n time but another approach might
conceivably be faster


$$@@$$PAGE: 567
548

chap 17 limits to computation

there is another use of reductions aside from applying an old algorithm to solve
a new problem and thereby establishing an upper bound for the new problem
that is to prove a lower bound on the cost of a new problem by showing that it
could be used as a solution for an old problem with a known lower bound
assume we can go the other way and convert sorting to pairing fast
enough what does this say about the minimum cost of pairing we know
from section 79 that the cost of sorting in the worst and average cases is in
n log n in other words the best possible algorithm for sorting requires at least
n log n time
assume that pairing could be done in on time then one way to create a
sorting algorithm would be to convert sorting into pairing run the algorithm
for pairing and finally convert the answer back to the answer for sorting
provided that we can convert sorting tofrom pairing fast enough this process would yield an on algorithm for sorting because this contradicts what we
know about the lower bound for sorting and the only flaw in the reasoning is
the initial assumption that pairing can be done in on time we can conclude
that there is no on time algorithm for pairing this reduction process tells us
that pairing must be at least as expensive as sorting and so must itself have a
lower bound in n log n
to complete this proof regarding the lower bound for pairing we need now
to find a way to reduce sorting to pairing this is easily done take an instance of sorting ie an array a of n elements a second array b is generated
that simply stores i in position i for 0  i  n pass the two arrays to pairing
take the resulting set of pairs and use the value from the b half of the pair to tell
which position in the sorted array the a half should take that is we can now reorder
the records in the a array using the corresponding value in the b array as the sort
key and running a simple n binsort the conversion of sorting to pairing
can be done in on time and likewise the conversion of the output of pairing
can be converted to the correct output for sorting in on time thus the cost
of this sorting algorithm is dominated by the cost for pairing
consider any two problems for which a suitable reduction from one to the other
can be found the first problem takes an arbitrary instance of its input which we
will call i and transforms i to a solution which we will call sln the second problem takes an arbitrary instance of its input which we will call i 0  and transforms i 0
to a solution which we will call sln 0  we can define reduction more formally as a
threestep process
1 transform an arbitrary instance of the first problem to an instance of the
second problem in other words there must be a transformation from any
instance i of the first problem to an instance i 0 of the second problem
2 apply an algorithm for the second problem to the instance i 0  yielding a
solution sln 0 


$$@@$$PAGE: 568
549

sec 171 reductions

problem a

i

transform 1
i

problem b
sln

transform 2

sln

figure 172 the general process for reduction shown as a blackbox diagram

3 transform sln 0 to the solution of i known as sln note that sln must in
fact be the correct solution for i for the reduction to be acceptable
figure 172 shows a graphical representation of the general reduction process
showing the role of the two problems and the two transformations figure 173
shows a similar diagram for the reduction of sorting to pairing
it is important to note that the reduction process does not give us an algorithm
for solving either problem by itself it merely gives us a method for solving the first
problem given that we already have a solution to the second more importantly for
the topics to be discussed in the remainder of this chapter reduction gives us a way
to understand the bounds of one problem in terms of another specifically given
efficient transformations the upper bound of the first problem is at most the upper
bound of the second conversely the lower bound of the second problem is at least
the lower bound of the first
as a second example of reduction consider the simple problem of multiplying
two ndigit numbers the standard longhand method for multiplication is to multiply the last digit of the first number by the second number taking n time
multiply the second digit of the first number by the second number again taking
n time and so on for each of the n digits of the first number finally the intermediate results are added together note that adding two numbers of length m
and n can easily be done in m n  time because each digit of the first number


$$@@$$PAGE: 569
550

chap 17 limits to computation

sorting

integer array a

transform 1
integers
0 to n1

integer
array a

pairing
array of pairs

transform 2

sorted
integer array
figure 173 a reduction of sorting to pairing shown as a blackbox
diagram

is multiplied against each digit of the second this algorithm requires n2  time
asymptotically faster but more complicated algorithms are known but none is so
fast as to be in on
next we ask the question is squaring an ndigit number as difficult as multiplying two ndigit numbers we might hope that something about this special case
will allow for a faster algorithm than is required by the more general multiplication
problem however a simple reduction proof serves to show that squaring is as
hard as multiplying
the key to the reduction is the following formula
x y 

x  y 2  x  y 2

4

the significance of this formula is that it allows us to convert an arbitrary instance
of multiplication to a series of operations involving three additionsubtractions
each of which can be done in linear time two squarings and a division by 4
note that the division by 4 can be done in linear time simply convert to binary
shift right by two digits and convert back
this reduction shows that if a linear time algorithm for squaring can be found
it can be used to construct a linear time algorithm for multiplication


$$@@$$PAGE: 570
sec 172 hard problems

551

our next example of reduction concerns the multiplication of two n  n matrices for this problem we will assume that the values stored in the matrices are simple integers and that multiplying two simple integers takes constant time because
multiplication of two int variables takes a fixed number of machine instructions
the standard algorithm for multiplying two matrices is to multiply each element
of the first matrixs first row by the corresponding element of the second matrixs
first column then adding the numbers this takes n time each of the n2 elements of the solution are computed in similar fashion requiring a total of n3 
time faster algorithms are known see the discussion of strassens algorithm in
section 1633 but none are so fast as to be in on2 
now consider the case of multiplying two symmetric matrices a symmetric
matrix is one in which entry ij is equal to entry ji that is the upperright triangle
of the matrix is a mirror image of the lowerleft triangle is there something about
this restricted case that allows us to multiply two symmetric matrices faster than
in the general case the answer is no as can be seen by the following reduction
assume that we have been given two n  n matrices a and b we can construct a
2n  2n symmetric matrix from an arbitrary matrix a as follows


0 a

at 0
here 0 stands for an n  n matrix composed of zero values a is the original matrix
and at stands for the transpose of matrix a1 note that the resulting matrix is now
symmetric we can convert matrix b to a symmetric matrix in a similar manner
if symmetric matrices could be multiplied quickly in particular if they could
be multiplied together in n2  time then we could find the result of multiplying
two arbitrary n  n matrices in n2  time by taking advantage of the following
observation
 



0 a
ab
0
0 bt


b 0
at 0
0 at bt
in the above formula ab is the result of multiplying matrices a and b together

172

hard problems

there are several ways that a problem could be considered hard for example we
might have trouble understanding the definition of the problem itself at the beginning of a large data collection and analysis project developers and their clients
might have only a hazy notion of what their goals actually are and need to work
that out over time for other types of problems we might have trouble finding or
understanding an algorithm to solve the problem understanding spoken english
1

the transpose operation takes position ij of the original matrix and places it in position ji of the
transpose matrix this can easily be done in n2 time for an n  n matrix


$$@@$$PAGE: 571
552

chap 17 limits to computation

and translating it to written text is an example of a problem whose goals are easy
to define but whose solution is not easy to discover but even though a natural
language processing algorithm might be difficult to write the programs running
time might be fairly fast there are many practical systems today that solve aspects
of this problem in reasonable time
none of these is what is commonly meant when a computer theoretician uses
the word hard throughout this section hard means that the bestknown algorithm for the problem is expensive in its running time one example of a hard
problem is towers of hanoi it is easy to understand this problem and its solution
it is also easy to write a program to solve this problem but it takes an extremely
long time to run for any reasonably large value of n try running a program to
solve towers of hanoi for only 30 disks
the towers of hanoi problem takes exponential time that is its running time
is 2n  this is radically different from an algorithm that takes n log n time
or n2  time it is even radically different from a problem that takes n4  time
these are all examples of polynomial running time because the exponents for all
terms of these equations are constants recall from chapter 3 that if we buy a new
computer that runs twice as fast the size of problem with complexity n4  that
we can solve in a certain amount of time is increased by the fourth root of two
in other words there is a multiplicative factor increase even if it is a rather small
one this is true for any algorithm whose running time can be represented by a
polynomial
consider what happens if you buy a computer that is twice as fast and try to
solve a bigger towers of hanoi problem in a given amount of time because its
complexity is 2n  we can solve a problem only one disk bigger there is no
multiplicative factor and this is true for any exponential algorithm a constant
factor increase in processing power results in only a fixed addition in problemsolving power
there are a number of other fundamental differences between polynomial running times and exponential running times that argues for treating them as qualitatively different polynomials are closed under composition and addition thus
running polynomialtime programs in sequence or having one program with polynomial running time call another a polynomial number of times yields polynomial
time also all computers known are polynomially related that is any program
that runs in polynomial time on any computer today when transferred to any other
computer will still run in polynomial time
there is a practical reason for recognizing a distinction in practice most polynomial time algorithms are feasible in that they can run reasonably large inputs
in reasonable time in contrast most algorithms requiring exponential time are
not practical to run even for fairly modest sizes of input one could argue that
a program with high polynomial degree such as n100  is not practical while an


$$@@$$PAGE: 572
sec 172 hard problems

553

exponentialtime program with cost 1001n is practical but the reality is that we
know of almost no problems where the best polynomialtime algorithm has high
degree they nearly all have degree four or less while almost no exponentialtime
algorithms whose cost is ocn  have their constant c close to one so there is not
much gray area between polynomial and exponential time algorithms in practice
for the rest of this chapter we define a hard algorithm to be one that runs in
exponential time that is in cn  for some constant c  1 a definition for a hard
problem will be presented in the next section
1721

the theory of n p completeness

imagine a magical computer that works by guessing the correct solution from
among all of the possible solutions to a problem another way to look at this is
to imagine a super parallel computer that could test all possible solutions simultaneously certainly this magical or highly parallel computer can do anything a
normal computer can do it might also solve some problems more quickly than a
normal computer can consider some problem where given a guess for a solution
checking the solution to see if it is correct can be done in polynomial time even
if the number of possible solutions is exponential any given guess can be checked
in polynomial time equivalently all possible solutions are checked simultaneously
in polynomial time and thus the problem can be solved in polynomial time by our
hypothetical magical computer another view of this concept is this if you cannot
get the answer to a problem in polynomial time by guessing the right answer and
then checking it then you cannot do it in polynomial time in any other way
the idea of guessing the right answer to a problem  or checking all possible
solutions in parallel to determine which is correct  is called nondeterminism
an algorithm that works in this manner is called a nondeterministic algorithm
and any problem with an algorithm that runs on a nondeterministic machine in
polynomial time is given a special name it is said to be a problem in n p thus
problems in n p are those problems that can be solved in polynomial time on a
nondeterministic machine
not all problems requiring exponential time on a regular computer are in n p
for example towers of hanoi is not in n p because it must print out o2n  moves
for n disks a nondeterministic machine cannot guess and print the correct
answer in less time
on the other hand consider the traveling salesman problem
traveling salesman 1
input a complete directed graph g with positive distances assigned to
each edge in the graph
output the shortest simple cycle that includes every vertex


$$@@$$PAGE: 573
554

chap 17 limits to computation

2

a
3

6

3

2

1

8

e

b

c

4
1

d

1

figure 174 an illustration of the traveling salesman problem five
vertices are shown with edges between each pair of cities the problem is to visit
all of the cities exactly once returning to the start city with the least total cost

figure 174 illustrates this problem five vertices are shown with edges and
associated costs between each pair of edges for simplicity figure 174 shows an
undirected graph assuming that the cost is the same in both directions though this
need not be the case if the salesman visits the cities in the order abcdea he
will travel a total distance of 13 a better route would be abdcea with cost 11
the best route for this particular graph would be abedca with cost 9
we cannot solve this problem in polynomial time with a guessandtest nondeterministic computer the problem is that given a candidate cycle while we can
quickly check that the answer is indeed a cycle of the appropriate form and while
we can quickly calculate the length of the cycle we have no easy way of knowing
if it is in fact the shortest such cycle however we can solve a variant of this
problem cast in the form of a decision problem a decision problem is simply one
whose answer is either yes or no the decision problem form of traveling
salesman is as follows
traveling salesman 2
input a complete directed graph g with positive distances assigned to
each edge in the graph and an integer k
output yes if there is a simple cycle with total distance  k containing
every vertex in g and no otherwise
we can solve this version of the problem in polynomial time with a nondeterministic computer the nondeterministic algorithm simply checks all of the possible subsets of edges in the graph in parallel if any subset of the edges is an
appropriate cycle of total length less than or equal to k the answer is yes otherwise the answer is no note that it is only necessary that some subset meet the
requirement it does not matter how many subsets fail checking a particular subset is done in polynomial time by adding the distances of the edges and verifying
that the edges form a cycle that visits each vertex exactly once thus the checking
algorithm runs in polynomial time unfortunately there are 2e subsets to check


$$@@$$PAGE: 574
sec 172 hard problems

555

so this algorithm cannot be converted to a polynomial time algorithm on a regular computer nor does anybody in the world know of any other polynomial time
algorithm to solve traveling salesman on a regular computer despite the
fact that the problem has been studied extensively by many computer scientists for
many years
it turns out that there is a large collection of problems with this property we
know efficient nondeterministic algorithms but we do not know if there are efficient deterministic algorithms at the same time we have not been able to prove
that any of these problems do not have efficient deterministic algorithms this class
of problems is called n pcomplete what is truly strange and fascinating about
n pcomplete problems is that if anybody ever finds the solution to any one of
them that runs in polynomial time on a regular computer then by a series of reductions every other problem that is in n p can also be solved in polynomial time on
a regular computer
define a problem to be n phard if any problem in n p can be reduced to x
in polynomial time thus x is as hard as any problem in n p a problem x is
defined to be n pcomplete if
1 x is in n p and
2 x is n phard
the requirement that a problem be n phard might seem to be impossible but
in fact there are hundreds of such problems including traveling salesman
another such problem is called kclique
kclique
input an arbitrary undirected graph g and an integer k
output yes if there is a complete subgraph of at least k vertices and no
otherwise
nobody knows whether there is a polynomial time solution for kclique but if
such an algorithm is found for kclique or for traveling salesman then
that solution can be modified to solve the other or any other problem in n p in
polynomial time
the primary theoretical advantage of knowing that a problem p1 is n pcomplete is that it can be used to show that another problem p2 is n pcomplete this is
done by finding a polynomial time reduction of p1 to p2 because we already know
that all problems in n p can be reduced to p1 in polynomial time by the definition
of n pcomplete we now know that all problems can be reduced to p2 as well by
the simple algorithm of reducing to p1 and then from there reducing to p2
there is a practical advantage to knowing that a problem is n pcomplete it
relates to knowing that if a polynomial time solution can be found for any prob


$$@@$$PAGE: 575
556

chap 17 limits to computation

exponential time problems
toh
np problems
npcomplete problems
traveling salesman
p problems
sorting

figure 175 our knowledge regarding the world of problems requiring exponential time or less some of these problems are solvable in polynomial time by a
nondeterministic computer of these some are known to be n pcomplete and
some are known to be solvable in polynomial time on a regular computer

lem that is n pcomplete then a polynomial solution can be found for all such
problems the implication is that
1 because no one has yet found such a solution it must be difficult or impossible to do and
2 effort to find a polynomial time solution for one n pcomplete problem can
be considered to have been expended for all n pcomplete problems
how is n pcompleteness of practical significance for typical programmers
well if your boss demands that you provide a fast algorithm to solve a problem
she will not be happy if you come back saying that the best you could do was
an exponential time algorithm but if you can prove that the problem is n pcomplete while she still wont be happy at least she should not be mad at you by
showing that her problem is n pcomplete you are in effect saying that the most
brilliant computer scientists for the last 50 years have been trying and failing to find
a polynomial time algorithm for her problem
problems that are solvable in polynomial time on a regular computer are said
to be in class p clearly all problems in p are solvable in polynomial time on
a nondeterministic computer simply by neglecting to use the nondeterministic
capability some problems in n p are n pcomplete we can consider all problems
solvable in exponential time or better as an even bigger class of problems because
all problems solvable in polynomial time are solvable in exponential time thus we
can view the world of exponentialtimeorbetter problems in terms of figure 175
the most important unanswered question in theoretical computer science is
whether p  n p if they are equal then there is a polynomial time algorithm


$$@@$$PAGE: 576
sec 172 hard problems

557

for traveling salesman and all related problems because traveling
salesman is known to be n pcomplete if a polynomial time algorithm were to
be found for this problem then all problems in n p would also be solvable in polynomial time conversely if we were able to prove that traveling salesman
has an exponential time lower bound then we would know that p 
6 n p
1722

n p completeness proofs

to start the process of being able to prove problems are n pcomplete we need to
prove just one problem h is n pcomplete after that to show that any problem
x is n phard we just need to reduce h to x when doing n pcompleteness
proofs it is very important not to get this reduction backwards if we reduce candidate problem x to known hard problem h this means that we use h as a step to
solving x all that means is that we have found a known hard way to solve x
however when we reduce known hard problem h to candidate problem x that
means we are using x as a step to solve h and if we know that h is hard that
means x must also be hard because if x were not hard then neither would h be
hard
so a crucial first step to getting this whole theory off the ground is finding one
problem that is n phard the first proof that a problem is n phard and because
it is in n p therefore n pcomplete was done by stephen cook for this feat
cook won the first turing award which is the closest computer science equivalent
to the nobel prize the granddaddy n pcomplete problem that cook used is
call satisfiability or sat for short
a boolean expression includes boolean variables combined using the operators and  or  and not to negate boolean variable x we write x a
literal is a boolean variable or its negation a clause is one or more literals ored
together let e be a boolean expression over variables x1  x2   xn  then we
define conjunctive normal form cnf to be a boolean expression written as a
series of clauses that are anded together for example
e  x5  x7  x8  x10   x2  x3   x1  x3  x6 
is in cnf and has three clauses now we can define the problem sat
satisfiability sat
input a boolean expression e over variables x1  x2   in conjunctive normal form
output yes if there is an assignment to the variables that makes e true
no otherwise
cook proved that sat is n phard explaining cooks proof is beyond the
scope of this book but we can briefly summarize it as follows any decision


$$@@$$PAGE: 577
558

chap 17 limits to computation

problem f can be recast as some language acceptance problem l
f i  yes  li 0   accept
that is if a decision problem f yields yes on input i then there is a language l
containing string i 0 where i 0 is some suitable transformation of input i conversely
if f would give answer no for input i then is transformed version i 0 is not in the
language l
turing machines are a simple model of computation for writing programs that
are language acceptors there is a universal turing machine that can take as input a description for a turing machine and an input string and return the execution
of that machine on that string this turing machine in turn can be cast as a boolean
expression such that the expression is satisfiable if and only if the turing machine
yields accept for that string cook used turing machines in his proof because
they are simple enough that he could develop this transformation of turing machines to boolean expressions but rich enough to be able to compute any function
that a regular computer can compute the significance of this transformation is that
any decision problem that is performable by the turing machine is transformable
to sat thus sat is n phard
as explained above to show that a decision problem x is n pcomplete we
prove that x is in n p normally easy and normally done by giving a suitable
polynomialtime nondeterministic algorithm and then prove that x is n phard
to prove that x is n phard we choose a known n pcomplete problem say a
we describe a polynomialtime transformation that takes an arbitrary instance i of
a to an instance i0 of x we then describe a polynomialtime transformation from
sln0 to sln such that sln is the solution for i the following example provides a
model for how an n pcompleteness proof is done
3satisfiability 3 sat
input a boolean expression e in cnf such that each clause contains exactly 3 literals
output yes if the expression can be satisfied no otherwise
example 171 3 sat is a special case of sat is 3 sat easier than sat
not if we can prove it to be n pcomplete
theorem 171 3 sat is n pcomplete
proof prove that 3 sat is in n p guess nondeterministically truth
values for the variables the correctness of the guess can be verified in
polynomial time
prove that 3 sat is n phard we need a polynomialtime reduction
from sat to 3 sat let e  c1  c2    ck be any instance of sat our


$$@@$$PAGE: 578
559

sec 172 hard problems

strategy is to replace any clause ci that does not have exactly three literals
with a set of clauses each having exactly three literals recall that a literal
can be a variable such as x or the negation of a variable such as x let
ci  x1  x2    xj where x1   xj are literals
1 j  1 so ci  x1  replace ci with ci0 
x1  y  z  x1  y  z  x1  y  z  x1  y  z
where y and z are variables not appearing in e clearly ci0 is satisfiable if and only if x1  is satisfiable meaning that x1 is true
2 j  2 so ci  x1  x2  replace ci with
x1  x2  z  x1  x2  z
where z is a new variable not appearing in e this new pair of clauses
is satisfiable if and only if x1  x2  is satisfiable that is either x1 or
x2 must be true
3 j  3 replace ci  x1  x2      xj  with
x1  x2  z1   x3  z1  z2   x4  z2  z3   
xj2  zj4  zj3   xj1  xj  zj3 
where z1   zj3 are new variables
after appropriate replacements have been made for each ci  a boolean
expression results that is an instance of 3 sat each replacement is satisfiable if and only if the original clause is satisfiable the reduction is clearly
polynomial time
for the first two cases it is fairly easy to see that the original clause
is satisfiable if and only if the resulting clauses are satisfiable for the
case were we replaced a clause with more than three literals consider the
following
1 if e is satisfiable then e 0 is satisfiable assume xm is assigned
true then assign zt  t  m  2 as true and zk  t  m  1 as
false then all clauses in case 3 are satisfied
2 if x1  x2   xj are all false then z1  z2   zj3 are all true but
then xj1  xj2  zj3  is false
2
next we define the problem vertex cover for use in further examples
vertex cover
input a graph g and an integer k
output yes if there is a subset s of the vertices in g of size k or less such
that every edge of g has at least one of its endpoints in s and no otherwise


$$@@$$PAGE: 579
560

chap 17 limits to computation

example 172 in this example we make use of a simple conversion between two graph problems
theorem 172 vertex cover is n pcomplete
proof prove that vertex cover is in n p simply guess a subset
of the graph and determine in polynomial time whether that subset is in fact
a vertex cover of size k or less
prove that vertex cover is n phard we will assume that kclique is already known to be n pcomplete we will see this proof in
the next example for now just accept that it is true
given that kclique is n pcomplete we need to find a polynomialtime transformation from the input to kclique to the input to vertex
cover and another polynomialtime transformation from the output for
vertex cover to the output for kclique this turns out to be a
simple matter given the following observation consider a graph g and
a vertex cover s on g denote by s0 the set of vertices in g but not in s
there can be no edge connecting any two vertices in s0 because if there
were then s would not be a vertex cover denote by g0 the inverse graph
for g that is the graph formed from the edges not in g if s is of size
k then s0 forms a clique of size n  k in graph g0  thus we can reduce
kclique to vertex cover simply by converting graph g to g0  and
asking if g0 has a vertex cover of size n  k or smaller if yes then
there is a clique in g of size k if no then there is not
2

example 173 so far our n pcompleteness proofs have involved transformations between inputs of the same type such as from a boolean expression to a boolean expression or from a graph to a graph sometimes an
n pcompleteness proof involves a transformation between types of inputs
as shown next
theorem 173 kclique is n pcomplete
proof kclique is in n p because we can just guess a collection of k
vertices and test in polynomial time if it is a clique now we show that kclique is n phard by using a reduction from sat an instance of sat
is a boolean expression
b  c1  c2    cm
whose clauses we will describe by the notation
ci  yi 1  yi 2    yi ki 


$$@@$$PAGE: 580
561

sec 172 hard problems

x1
x1

x2

x2

x1

x3
x3

c1

c2

c3

figure 176 the graph generated from boolean expression b  x1 x2 x1 
x2  x3   x1  x3  literals from the first clause are labeled c1 and literals from
the second clause are labeled c2 there is an edge between every pair of vertices
except when both vertices represent instances of literals from the same clause or
a negation of the same variable thus the vertex labeled c1  y1 does not connect
to the vertex labeled c1  y2 because they are literals in the same clause or the
vertex labeled c2  y1 because they are opposite values for the same variable

where ki is the number of literals in clause ci  we will transform this to an
instance of kclique as follows we build a graph
g  vi j1  i  m 1  j  ki 
that is there is a vertex in g corresponding to every literal in boolean
expression b we will draw an edge between each pair of vertices vi1  j1 
and vi2  j2  unless 1 they are two literals within the same clause i1  i2 
or 2 they are opposite values for the same variable ie one is negated
and the other is not set k  m figure 176 shows an example of this
transformation
b is satisfiable if and only if g has a clique of size k or greater b being
satisfiable implies that there is a truth assignment such that at least one
literal yi ji  is true for each i if so then these m literals must correspond
to m vertices in a clique of size k  m conversely if g has a clique of
size k or greater then the clique must have size exactly k because no two
vertices corresponding to literals in the same clause can be in the clique
and there is one vertex vi ji  in the clique for each i there is a truth
assignment making each yi ji  true that truth assignment satisfies b
we conclude that kclique is n phard therefore n pcomplete 2


$$@@$$PAGE: 581
562
1723

chap 17 limits to computation

coping with n p complete problems

finding that your problem is n pcomplete might not mean that you can just forget
about it traveling salesmen need to find reasonable sales routes regardless of the
complexity of the problem what do you do when faced with an n pcomplete
problem that you must solve
there are several techniques to try one approach is to run only small instances
of the problem for some problems this is not acceptable for example traveling salesman grows so quickly that it cannot be run on modern computers for
problem sizes much over 30 cities which is not an unreasonable problem size for
reallife situations however some other problems in n p while requiring exponential time still grow slowly enough that they allow solutions for problems of a
useful size
consider the knapsack problem from section 1611 we have a dynamic programming algorithm whose cost is nk for n objects being fit into a knapsack of
size k but it turns out that knapsack is n pcomplete isnt this a contradiction
not when we consider the relationship between n and k how big is k input size
is typically on lg k because the item sizes are smaller than k thus nk is
exponential on input size
this dynamic programming algorithm is tractable if the numbers are reasonable that is we can successfully find solutions to the problem when nk is in
the thousands such an algorithm is called a pseudopolynomial time algorithm
this is different from traveling salesman which cannot possibly be solved
when n  100 given current algorithms
a second approach to handling n pcomplete problems is to solve a special
instance of the problem that is not so hard for example many problems on graphs
are n pcomplete but the same problem on certain restricted types of graphs is
not as difficult for example while the vertex cover and kclique problems are n pcomplete in general there are polynomial time solutions for bipartite graphs ie graphs whose vertices can be separated into two subsets such
that no pair of vertices within one of the subsets has an edge between them 2satisfiability where every clause in a boolean expression has at most two
literals has a polynomial time solution several geometric problems require only
polynomial time in two dimensions but are n pcomplete in three dimensions or
more knapsack is considered to run in polynomial time if the numbers and
k are small small here means that they are polynomial on n the number of
items
in general if we want to guarantee that we get the correct answer for an n pcomplete problem we potentially need to examine all of the exponential number
of possible solutions however with some organization we might be able to either
examine them quickly or avoid examining a great many of the possible answers
in some cases for example dynamic programming section 161 attempts to


$$@@$$PAGE: 582
sec 172 hard problems

563

organize the processing of all the subproblems to a problem so that the work is
done efficiently
if we need to do a bruteforce search of the entire solution space we can use
backtracking to visit all of the possible solutions organized in a solution tree for
example satisfiability has 2n possible ways to assign truth values to the n
variables contained in the boolean expression being satisfied we can view this as
a tree of solutions by considering that we have a choice of making the first variable
true or false thus we can put all solutions where the first variable is true on
one side of the tree and the remaining solutions on the other we then examine the
solutions by moving down one branch of the tree until we reach a point where we
know the solution cannot be correct such as if the current partial collection of assignments yields an unsatisfiable expression at this point we backtrack and move
back up a node in the tree and then follow down the alternate branch if this fails
we know to back up further in the tree as necessary and follow alternate branches
until finally we either find a solution that satisfies the expression or exhaust the
tree in some cases we avoid processing many potential solutions or find a solution
quickly in others we end up visiting a large portion of the 2n possible solutions
banchandbounds is an extension of backtracking that applies to optimization problems such as traveling salesman where we are trying to find the
shortest tour through the cities we traverse the solution tree as with backtracking however we remember the best value found so far proceeding down a given
branch is equivalent to deciding which order to visit cities so any node in the solution tree represents some collection of cities visited so far if the sum of these
distances exceeds the best tour found so far then we know to stop pursuing this
branch of the tree at this point we can immediately back up and take another
branch if we have a quick method for finding a good but not necessarily best
solution we can use this as an initial bound value to effectively prune portions of
the tree
another coping strategy is to find an approximate solution to the problem
there are many approaches to finding approximate solutions one way is to use
a heuristic to solve the problem that is an algorithm based on a rule of thumb
that does not always give the best answer for example the traveling salesman problem can be solved approximately by using the heuristic that we start at
an arbitrary city and then always proceed to the next unvisited city that is closest
this rarely gives the shortest path but the solution might be good enough there
are many other heuristics for traveling salesman that do a better job
some approximation algorithms have guaranteed performance such that the
answer will be within a certain percentage of the best possible answer for example consider this simple heuristic for the vertex cover problem let m be
a maximal not necessarily maximum matching in g a matching pairs vertices
with connecting edges so that no vertex is paired with more than one partner


$$@@$$PAGE: 583
564

chap 17 limits to computation

maximal means to pick as many pairs as possible selecting them in some order until there are no more available pairs to select maximum means the matching that
gives the most pairs possible for a given graph if opt is the size of a minimum
vertex cover then m   2  opt because at least one endpoint of every matched
edge must be in any vertex cover
a better example of a guaranteed bound on a solution comes from simple
heuristics to solve the bin packing problem
bin packing
input numbers x1  x2   xn between 0 and 1 and an unlimited supply of
bins of size 1 no bin can hold numbers whose sum exceeds 1
output an assignment of numbers to bins that requires the fewest possible
bins
bin packing in its decision form ie asking if the items can be packed in
less than k bins is known to be n pcomplete one simple heuristic for solving
this problem is to use a first fit approach we put the first number in the first
bin we then put the second number in the first bin if it fits otherwise we put it in
the second bin for each subsequent number we simply go through the bins in the
order we generated them and place the number in the first bin that fits the number
of bins used is no more than twice the sum of the numbers because every bin
except perhaps one must be at least half full however this first fit heuristic can
give us a result that is much worse than optimal consider the following collection
of numbers 6 of 17   6 of 13   and 6 of 12   where  is a small positive
number properly organized this requires 6 bins but if done wrongly we might
end up putting the numbers into 10 bins
a better heuristic is to use decreasing first fit this is the same as first fit except
that we keep the bins sorted from most full to least full then when deciding where
to put the next item we place it in the fullest bin that can hold it this is similar to
the best fit heuristic for memory management discussed in section 123 the significant thing about this heuristic is not just that it tends to give better performance
than simple first fit this decreasing first fit heuristic can be proven to require no
more than 119 the optimal number of bins thus we have a guarantee on how
much inefficiency can result when using the heuristic
the theory of n pcompleteness gives a technique for separating tractable from
probably intractable problems recalling the algorithm for generating algorithms
in section 151 we can refine it for problems that we suspect are n pcomplete
when faced with a new problem we might alternate between checking if it is
tractable that is we try to find a polynomialtime solution and checking if it is
intractable we try to prove the problem is n pcomplete while proving that
some problem is n pcomplete does not actually make our upper bound for our


$$@@$$PAGE: 584
sec 173 impossible problems

565

algorithm match the lower bound for the problem with certainty it is nearly as
good once we realize that a problem is n pcomplete then we know that our next
step must either be to redefine the problem to make it easier or else use one of the
coping strategies discussed in this section

173

impossible problems

even the best programmer sometimes writes a program that goes into an infinite
loop of course when you run a program that has not stopped you do not know
for sure if it is just a slow program or a program in an infinite loop after enough
time you shut it down wouldnt it be great if your compiler could look at your
program and tell you before you run it that it will get into an infinite loop to be
more specific given a program and a particular input it would be useful to know if
executing the program on that input will result in an infinite loop without actually
running the program
unfortunately the halting problem as this is called cannot be solved there
will never be a computer program that can positively determine for an arbitrary
program p if p will halt for all input nor will there even be a computer program
that can positively determine if arbitrary program p will halt for a specified input i
how can this be programmers look at programs regularly to determine if they will
halt surely this can be automated as a warning to those who believe any program
can be analyzed in this way carefully examine the following code fragment before
reading on
while n  1
if oddn
n  3  n  1
else
n  n  2

this is a famous piece of code the sequence of values that is assigned to n
by this code is sometimes called the collatz sequence for input value n does
this code fragment halt for all values of n nobody knows the answer every
input that has been tried halts but does it always halt note that for this code
fragment because we do not know if it halts we also do not know an upper bound
for its running time as for the lower bound we can easily show log n see
exercise 314
personally i have faith that someday some smart person will completely analyze the collatz function proving once and for all that the code fragment halts for
all values of n doing so may well give us techniques that advance our ability to
do algorithm analysis in general unfortunately proofs from computability  the
branch of computer science that studies what is impossible to do with a computer
 compel us to believe that there will always be another bit of program code that


$$@@$$PAGE: 585
566

chap 17 limits to computation

we cannot analyze this comes as a result of the fact that the halting problem is
unsolvable
1731

uncountability

before proving that the halting problem is unsolvable we first prove that not all
functions can be implemented as a computer program this must be so because the
number of programs is much smaller than the number of possible functions
a set is said to be countable or countably infinite if it is a set with an infinite
number of members if every member of the set can be uniquely assigned to a
positive integer a set is said to be uncountable or uncountably infinite if it is
not possible to assign every member of the set to its own positive integer
to understand what is meant when we say assigned to a positive integer
imagine that there is an infinite row of bins labeled 1 2 3 and so on take a set
and start placing members of the set into bins with at most one member per bin if
we can find a way to assign all of the set members to bins then the set is countable
for example consider the set of positive even integers 2 4 and so on we can
assign an integer i to bin i2 or if we dont mind skipping some bins then we can
assign even number i to bin i thus the set of even integers is countable this
should be no surprise because intuitively there are fewer positive even integers
than there are positive integers even though both are infinite sets but there are not
really any more positive integers than there are positive even integers because we
can uniquely assign every positive integer to some positive even integer by simply
assigning positive integer i to positive even integer 2i
on the other hand the set of all integers is also countable even though this set
appears to be bigger than the set of positive integers this is true because we can
assign 0 to positive integer 1 1 to positive integer 2 1 to positive integer 3 2 to
positive integer 4 2 to positive integer 5 and so on in general assign positive
integer value i to positive integer value 2i and assign negative integer value i to
positive integer value 2i  1 we will never run out of positive integers to assign
and we know exactly which positive integer every integer is assigned to because
every integer gets an assignment the set of integers is countably infinite
are the number of programs countable or uncountable a program can be
viewed as simply a string of characters including special punctuation spaces and
line breaks let us assume that the number of different characters that can appear
in a program is p  using the ascii character set p must be less than 128 but
the actual number does not matter if the number of strings is countable then
surely the number of programs is also countable we can assign strings to the
bins as follows assign the null string to the first bin now take all strings of
one character and assign them to the next p bins in alphabetic or ascii code
order next take all strings of two characters and assign them to the next p 2 bins
again in ascii code order working from left to right strings of three characters


$$@@$$PAGE: 586
sec 173 impossible problems

567

are likewise assigned to bins then strings of length four and so on in this way a
string of any given length can be assigned to some bin
by this process any string of finite length is assigned to some bin so any program which is merely a string of finite length is assigned to some bin because all
programs are assigned to some bin the set of all programs is countable naturally
most of the strings in the bins are not legal programs but this is irrelevant all that
matters is that the strings that do correspond to programs are also in the bins
now we consider the number of possible functions to keep things simple
assume that all functions take a single positive integer as input and yield a single positive integer as output we will call such functions integer functions a
function is simply a mapping from input values to output values of course not
all computer programs literally take integers as input and yield integers as output
however everything that computers read and write is essentially a series of numbers which may be interpreted as letters or something else any useful computer
programs input and output can be coded as integer values so our simple model
of computer input and output is sufficiently general to cover all possible computer
programs
we now wish to see if it is possible to assign all of the integer functions to the
infinite set of bins if so then the number of functions is countable and it might
then be possible to assign every integer function to a program if the set of integer
functions cannot be assigned to bins then there will be integer functions that must
have no corresponding program
imagine each integer function as a table with two columns and an infinite number of rows the first column lists the positive integers starting at 1 the second
column lists the output of the function when given the value in the first column
as input thus the table explicitly describes the mapping from input to output for
each function call this a function table
next we will try to assign function tables to bins to do so we must order the
functions but it does not matter what order we choose for example bin 1 could
store the function that always returns 1 regardless of the input value bin 2 could
store the function that returns its input bin 3 could store the function that doubles
its input and adds 5 bin 4 could store a function for which we can see no simple
relationship between input and output2 these four functions as assigned to the first
four bins are shown in figure 177
can we assign every function to a bin the answer is no because there is
always a way to create a new function that is not in any of the bins suppose that
somebody presents a way of assigning functions to bins that they claim includes
all of the functions we can build a new function that has not been assigned to
2

there is no requirement for a function to have any discernible relationship between input and
output a function is simply a mapping of inputs to outputs with no constraint on how the mapping
is determined


$$@@$$PAGE: 587
568

chap 17 limits to computation

1
x f1x
1 1
2 1
3 1
4 1
5 1
6 1

2
x f2x
1 1
2 2
3 3
4 4
5 5
6 6

3
x f3x
1 7
2 9
3 11
4 13
5 15
6 17

4

5

x f4x
1 15
2 1
3 7
4 13
5 2
6 7

figure 177 an illustration of assigning functions to bins

1
x f1x
1 1
2 1
3 1
4 1
5 1
6 1

2
x f2x
1 1
2 2
3 3
4 4
5 5
6 6

3
x f3x
1 7
2 9
3 11
4 13
5 15
6 17

4
x f4x
1 15
2 1
3 7
4 13
5 2
6 7

5
x fnew x
1 2
2 3
3 12
4 14
5
6

figure 178 illustration for the argument that the number of integer functions is
uncountable

any bin as follows take the output value for input 1 from the function in the first
bin call this value f1 1 add 1 to it and assign the result as the output of a new
function for input value 1 regardless of the remaining values assigned to our new
function it must be different from the first function in the table because the two
give different outputs for input 1 now take the output value for 2 from the second
function in the table known as f2 2 add 1 to this value and assign it as the
output for 2 in our new function thus our new function must be different from
the function of bin 2 because they will differ at least at the second value continue
in this manner assigning fnew i  fi i  1 for all values i thus the new
function must be different from any function fi at least at position i this procedure
for constructing a new function not already in the table is called diagonalization
because the new function is different from every other function it must not be in
the table this is true no matter how we try to assign functions to bins and so the
number of integer functions is uncountable the significance of this is that not all
functions can possibly be assigned to programs so there must be functions with no
corresponding program figure 178 illustrates this argument


$$@@$$PAGE: 588
sec 173 impossible problems

1732

569

the halting problem is unsolvable

while there might be intellectual appeal to knowing that there exists some function
that cannot be computed by a computer program does this mean that there is any
such useful function after all does it really matter if no program can compute a
nonsense function such as shown in bin 4 of figure 177 now we will prove
that the halting problem cannot be computed by any computer program the proof
is by contradiction
we begin by assuming that there is a function named halt that can solve the
halting problem obviously it is not possible to write out something that does not
exist but here is a plausible sketch of what a function to solve the halting problem
might look like if it did exist function halt takes two inputs a string representing
the source code for a program or function and another string representing the input
that we wish to determine if the input program or function halts on function halt
does some work to make a decision which is encapsulated into some fictitious
function named program halts function halt then returns true if the input
program or function does halt on the given input and false otherwise
bool haltstring prog string input 
if program haltsprog input
return true
else
return false


we now will examine two simple functions that clearly can exist because the
complete code for them is presented here
 return true if prog halts when given itself as input
bool selfhaltchar prog 
if haltprog prog
return true
else
return false

 return the reverse of what selfhalt returns on prog
void contrarychar prog 
if selfhaltprog
while true  go into an infinite loop


what happens if we make a program whose sole purpose is to execute the function contrary and run that program with itself as input one possibility is that
the call to selfhalt returns true that is selfhalt claims that contrary
will halt when run on itself in that case contrary goes into an infinite loop and
thus does not halt on the other hand if selfhalt returns false then halt is
proclaiming that contrary does not halt on itself and contrary then returns


$$@@$$PAGE: 589
570

chap 17 limits to computation

that is it halts thus contrary does the contrary of what halt says that it will
do
the action of contrary is logically inconsistent with the assumption that
halt solves the halting problem correctly there are no other assumptions we
made that might cause this inconsistency thus by contradiction we have proved
that halt cannot solve the halting problem correctly and thus there is no program
that can solve the halting problem
now that we have proved that the halting problem is unsolvable we can use
reduction arguments to prove that other problems are also unsolvable the strategy is to assume the existence of a computer program that solves the problem in
question and use that program to solve another problem that is already known to be
unsolvable
example 174 consider the following variation on the halting problem
given a computer program will it halt when its input is the empty string
that is will it halt when it is given no input to prove that this problem is
unsolvable we will employ a standard technique for computability proofs
use a computer program to modify another computer program
proof assume that there is a function ehalt that determines whether
a given program halts when given no input recall that our proof for the
halting problem involved functions that took as parameters a string representing a program and another string representing an input consider
another function combine that takes a program p and an input string i as
parameters function combine modifies p to store i as a static variable s
and further modifies all calls to input functions within p to instead get their
input from s call the resulting program p 0  it should take no stretch of the
imagination to believe that any decent compiler could be modified to take
computer programs and input strings and produce a new computer program
that has been modified in this way now take p 0 and feed it to ehalt if
ehalt says that p 0 will halt then we know that p would halt on input i
in other words we now have a procedure for solving the original halting
problem the only assumption that we made was the existence of ehalt
thus the problem of determining if a program will halt on no input must
be unsolvable
2

example 175 for arbitrary program p does there exist any input for
which p halts
proof this problem is also uncomputable assume that we had a function
ahalt that when given program p as input would determine if there is
some input for which p halts we could modify our compiler or write


$$@@$$PAGE: 590
sec 174 further reading

571

a function as part of a program to take p and some input string w and
modify it so that w is hardcoded inside p with p reading no input call this
modified program p 0  now p 0 always behaves the same way regardless of
its input because it ignores all input however because w is now hardwired
inside of p 0  the behavior we get is that of p when given w as input so p 0
will halt on any arbitrary input if and only if p would halt on input w we
now feed p 0 to function ahalt if ahalt could determine that p 0 halts
on some input then that is the same as determining that p halts on input w
but we know that that is impossible therefore ahalt cannot exist
2
there are many things that we would like to have a computer do that are unsolvable many of these have to do with program behavior for example proving
that an arbitrary program is correct that is proving that a program computes a
particular function is a proof regarding program behavior as such what can be
accomplished is severely limited some other unsolvable problems include





does a program halt on every input
does a program compute a particular function
do two programs compute the same function
does a particular line in a program get executed

this does not mean that a computer program cannot be written that works on
special cases possibly even on most programs that we would be interested in checking for example some c compilers will check if the control expression for a
while loop is a constant expression that evaluates to false if it is the compiler
will issue a warning that the while loop code will never be executed however it
is not possible to write a computer program that can check for all input programs
whether a specified line of code will be executed when the program is given some
specified input
another unsolvable problem is whether a program contains a computer virus
the property contains a computer virus is a matter of behavior thus it is not
possible to determine positively whether an arbitrary program contains a computer
virus fortunately there are many good heuristics for determining if a program
is likely to contain a virus and it is usually possible to determine if a program
contains a particular virus at least for the ones that are now known real virus
checkers do a pretty good job but it will always be possible for malicious people
to invent new viruses that no existing virus checker can recognize

174

further reading

the classic text on the theory of n pcompleteness is computers and intractability a guide to the theory of n pcompleteness by garey and johnston gj79
the traveling salesman problem edited by lawler et al llks85 discusses


$$@@$$PAGE: 591
572

chap 17 limits to computation

many approaches to finding an acceptable solution to this particular n pcomplete
problem in a reasonable amount of time
for more information about the collatz function see on the ups and downs
of hailstone numbers by b hayes hay84 and the 3x  1 problem and its
generalizations by jc lagarias lag85
for an introduction to the field of computability and impossible problems see
discrete structures logic and computability by james l hein hei09

175

exercises

171 consider this algorithm for finding the maximum element in an array first
sort the array and then select the last maximum element what if anything
does this reduction tell us about the upper and lower bounds to the problem
of finding the maximum element in a sequence why can we not reduce
sorting to finding the maximum element
172 use a reduction to prove that squaring an n  n matrix is just as expensive
asymptotically as multiplying two n  n matrices
173 use a reduction to prove that multiplying two upper triangular n  n matrices is just as expensive asymptotically as multiplying two arbitrary n  n
matrices
174 a explain why computing the factorial of n by multiplying all values
from 1 to n together is an exponential time algorithm
b explain why computing an approximation to the factorial of n by making use of stirlings formula see section 22 is a polynomial time
algorithm
175 consider this algorithm for solving the kclique problem first generate
all subsets of the vertices containing exactly k vertices there are onk  such
subsets altogether then check whether any subgraphs induced by these
subsets is complete if this algorithm ran in polynomial time what would
be its significance why is this not a polynomialtime algorithm for the kclique problem
176 write the 3 sat expression obtained from the reduction of sat to 3 sat
described in section 1721 for the expression
a  b  c  d  d  b  c  a  b  a  c  b
is this expression satisfiable
177 draw the graph obtained by the reduction of sat to the kclique problem
given in section 1721 for the expression
a  b  c  a  b  c  a  b  c  a  b  c
is this expression satisfiable


$$@@$$PAGE: 592
sec 175 exercises

573

178 a hamiltonian cycle in graph g is a cycle that visits every vertex in the
graph exactly once before returning to the start vertex the problem hamiltonian cycle asks whether graph g does in fact contain a hamiltonian
cycle assuming that hamiltonian cycle is n pcomplete prove that
the decisionproblem form of traveling salesman is n pcomplete
179 use the assumption that vertex cover is n pcomplete to prove that kclique is also n pcomplete by finding a polynomial time reduction from
vertex cover to kclique
1710 we define the problem independent set as follows
independent set
input a graph g and an integer k
output yes if there is a subset s of the vertices in g of size k or
greater such that no edge connects any two vertices in s and no otherwise
assuming that kclique is n pcomplete prove that independent
set is n pcomplete
1711 define the problem partition as follows
partition
input a collection of integers
output yes if the collection can be split into two such that the sum
of the integers in each partition sums to the same amount no otherwise
a assuming that partition is n pcomplete prove that the decision
form of bin packing is n pcomplete
b assuming that partition is n pcomplete prove that knapsack
is n pcomplete
1712 imagine that you have a problem p that you know is n pcomplete for
this problem you have two algorithms to solve it for each algorithm some
problem instances of p run in polynomial time and others run in exponential time there are lots of heuristicbased algorithms for real n pcomplete
problems with this behavior you cant tell beforehand for any given problem instance whether it will run in polynomial or exponential time on either
algorithm however you do know that for every problem instance at least
one of the two algorithms will solve it in polynomial time
a what should you do
b what is the running time of your solution


$$@@$$PAGE: 593
574

1713

1714

1715
1716

1717

1718

1719

1720

1721

1722

chap 17 limits to computation

c what does it say about the question of p  n p if the conditions
described in this problem existed
here is another version of the knapsack problem which we will call exact
knapsack given a set of items each with given integer size and a knapsack of size integer k is there a subset of the items which fits exactly within
the knapsack
assuming that exact knapsack is n pcomplete use a reduction argument to prove that knapsack is n pcomplete
the last paragraph of section 1723 discusses a strategy for developing a
solution to a new problem by alternating between finding a polynomial time
solution and proving the problem n pcomplete refine the algorithm for
designing algorithms from section 151 to incorporate identifying and dealing with n pcomplete problems
prove that the set of real numbers is uncountable use a proof similar to the
proof in section 1731 that the set of integer functions is uncountable
prove using a reduction argument such as given in section 1732 that the
problem of determining if an arbitrary program will print any output is unsolvable
prove using a reduction argument such as given in section 1732 that the
problem of determining if an arbitrary program executes a particular statement within that program is unsolvable
prove using a reduction argument such as given in section 1732 that the
problem of determining if two arbitrary programs halt on exactly the same
inputs is unsolvable
prove using a reduction argument such as given in section 1732 that the
problem of determining whether there is some input on which two arbitrary
programs will both halt is unsolvable
prove using a reduction argument such as given in section 1732 that the
problem of determining whether an arbitrary program halts on all inputs is
unsolvable
prove using a reduction argument such as given in section 1732 that the
problem of determining whether an arbitrary program computes a specified
function is unsolvable
consider a program named comp that takes two strings as input it returns
true if the strings are the same it returns false if the strings are different
why doesnt the argument that we used to prove that a program to solve the
halting problem does not exist work to prove that comp does not exist

176

projects

171 implement vertex cover that is given graph g and integer k answer
the question of whether or not there is a vertex cover of size k or less begin


$$@@$$PAGE: 594
sec 176 projects

575

by using a bruteforce algorithm that checks all possible sets of vertices of
size k to find an acceptable vertex cover and measure the running time on a
number of input graphs then try to reduce the running time through the use
of any heuristics you can think of next try to find approximate solutions to
the problem in the sense of finding the smallest set of vertices that forms a
vertex cover
172 implement knapsack see section 161 measure its running time on a
number of inputs what is the largest practical input size for this problem
173 implement an approximation of traveling salesman that is given a
graph g with costs for all edges find the cheapest cycle that visits all vertices
in g try various heuristics to find the best approximations for a wide variety
of input graphs
174 write a program that given a positive integer n as input prints out the collatz
sequence for that number what can you say about the types of integers that
have long collatz sequences what can you say about the length of the
collatz sequence for various types of integers


$$@@$$PAGE: 595

$$@@$$PAGE: 596
part vi
appendix

577


$$@@$$PAGE: 597

$$@@$$PAGE: 598
a
utility functions

here are various utility functions used by the c example programs in this text
 return true iff x is even
inline bool evenint x  return x  2  0 
 return true iff x is odd
inline bool oddint x  return x  2  0 
 assert if val is false print a message and terminate
 the program
void assertbool val string s 
if val   assertion failed  close the program
cout  assertion failed   s  endl
exit1


 swap two elements in a generic array
templatetypename e
inline void swape a int i int j 
e temp  ai
ai  aj
aj  temp

 random number generator functions
inline void randomize  seed the generator
 srand1 
 return a random value in range 0 to n1
inline int randomint n
 return rand  n 

579


$$@@$$PAGE: 599

$$@@$$PAGE: 600
bibliography

ag06
aha00

ahu74

ahu83
bb96
ben75

ben82
ben84
ben85
ben86
ben88
ben00
bg00

ken arnold and james gosling the java programming language
addisonwesley reading ma usa fourth edition 2006
dan aharoni cogito ergo sum cognitive processes of students dealing with data structures in proceedings of sigcse00 pages 2630
acm press march 2000
alfred v aho john e hopcroft and jeffrey d ullman the design
and analysis of computer algorithms addisonwesley reading ma
1974
alfred v aho john e hopcroft and jeffrey d ullman data structures and algorithms addisonwesley reading ma 1983
g brassard and p bratley fundamentals of algorithmics prentice
hall upper saddle river nj 1996
john louis bentley multidimensional binary search trees used for
associative searching communications of the acm 189509517
september 1975 issn 00010782
john louis bentley writing efficient programs prentice hall upper
saddle river nj 1982
john louis bentley programming pearls the back of the envelope
communications of the acm 273180184 march 1984
john louis bentley programming pearls thanks heaps communications of the acm 283245250 march 1985
john louis bentley programming pearls the envelope is back communications of the acm 293176182 march 1986
john bentley more programming pearls confessions of a coder
addisonwesley reading ma 1988
john bentley programming pearls addisonwesley reading ma
second edition 2000
sara baase and allen van gelder computer algorithms introduction
to design  analysis addisonwesley reading ma usa third
edition 2000
581


$$@@$$PAGE: 601
582
bm85

bro95

bstw86

clrs09

com79
dd08
ecw92
ed88
epp10
es90
ess81

fby92
ff89
ffbs95

fhcd92

fl95
fz98

bibliography

john louis bentley and catherine c mcgeoch amortized analysis
of selforganizing sequential search heuristics communications of the
acm 284404411 april 1985
frederick p brooks the mythical manmonth essays on software
engineering 25th anniversary edition addisonwesley reading
ma 1995
john louis bentley daniel d sleator robert e tarjan and victor k
wei a locally adaptive data compression scheme communications
of the acm 294320330 april 1986
thomas h cormen charles e leiserson ronald l rivest and clifford stein introduction to algorithms the mit press cambridge
ma third edition 2009
douglas comer
the ubiquitous btree
computing surveys
112121137 june 1979
hm deitel and pj deitel c  how to program prentice hall
upper saddle river nj sixth edition 2008
vladimir estivillcastro and derick wood a survey of adaptive sorting algorithms computing surveys 244441476 december 1992
rj enbody and hc du dynamic hashing schemes computing
surveys 20285113 june 1988
susanna s epp discrete mathematics with applications brookscole
publishing company pacific grove ca fourth edition 2010
margaret a ellis and bjarne stroustrup the annotated c  reference manual addisonwesley reading ma 1990
s c eisenstat m h schultz and a h sherman algorithms and
data structures for sparse symmetric gaussian elimination siam journal on scientific computing 22225237 june 1981
wb frakes and r baezayates editors information retrieval data
structures  algorithms prentice hall upper saddle river nj 1992
daniel p friedman and matthias felleisen the little lisper macmillan publishing company new york ny 1989
daniel p friedman matthias felleisen duane bibby and gerald j
sussman the little schemer the mit press cambridge ma fourth
edition 1995
edward a fox lenwood s heath q f chen and amjad m daoud
practical minimal perfect hash functions for large databases communications of the acm 351105121 january 1992
h scott folger and steven e leblanc strategies for creative problem solving prentice hall upper saddle river nj 1995
mj folk and b zoellick file structures an objectoriented approach with c   addisonwesley reading ma third edition 1998


$$@@$$PAGE: 602
bibliography

583

ghjv95 erich gamma richard helm ralph johnson and john vlissides
design patterns elements of reusable objectoriented software
addisonwesley reading ma 1995
gi91
zvi galil and giuseppe f italiano data structures and algorithms
for disjoint set union problems computing surveys 233319344
september 1991
gj79
michael r garey and david s johnson computers and intractability
a guide to the theory of npcompleteness wh freeman new york
ny 1979
gkp94 ronald l graham donald e knuth and oren patashnik concrete
mathematics a foundation for computer science addisonwesley
reading ma second edition 1994
gle92
james gleick genius the life and science of richard feynman
vintage new york ny 1992
gms91 john r gilbert cleve moler and robert schreiber sparse matrices
in matlab design and implementation siam journal on matrix
analysis and applications 131333356 1991
gut84
antonin guttman rtrees a dynamic index structure for spatial
searching in b yormark editor annual meeting acm sigmod
pages 4757 boston ma june 1984
hay84
b hayes computer recreations on the ups and downs of hailstone
numbers scientific american 25011016 january 1984
hei09
james l hein discrete structures logic and computability jones
and bartlett sudbury ma third edition 2009
jay90
julian jaynes the origin of consciousness in the breakdown of the
bicameral mind houghton mifflin boston ma 1990
kaf98
dennis kafura objectoriented software design and construction
with c   prentice hall upper saddle river nj 1998
knu94
donald e knuth the stanford graphbase addisonwesley reading ma 1994
knu97
donald e knuth the art of computer programming fundamental
algorithms volume 1 addisonwesley reading ma third edition
1997
knu98
donald e knuth the art of computer programming sorting and
searching volume 3 addisonwesley reading ma second edition
1998
koz05
charles m kozierok the pc guide wwwpcguidecom 2005
kp99
brian w kernighan and rob pike the practice of programming
addisonwesley reading ma 1999
lag85
j c lagarias the 3x1 problem and its generalizations the american mathematical monthly 921323 january 1985


$$@@$$PAGE: 603
584
lev94

bibliography

marvin levine effective problem solving prentice hall upper saddle river nj second edition 1994
llks85 el lawler jk lenstra ahg rinnooy kan and db shmoys
editors the traveling salesman problem a guided tour of combinatorial optimization john wiley  sons new york ny 1985
man89 udi manber introduction to algorithms a creative approach
addisionwesley reading ma 1989
mm04
nimrod megiddo and dharmendra s modha outperforming lru with
an adaptive replacement cache algorithm ieee computer 37458
65 april 2004
mm08
zbigniew michaelewicz and matthew michalewicz puzzlebased
learning an introduction to critical thinking mathematics and problem solving hybrid publishers melbourne australia 2008
pol57
george polya how to solve it princeton university press princeton
nj second edition 1957
pug90
w pugh skip lists a probabilistic alternative to balanced trees communications of the acm 336668676 june 1990
raw92 gregory je rawlins compared to what an introduction to the
analysis of algorithms computer science press new york ny 1992
rie96
arthur j riel objectoriented design heuristics addisonwesley
reading ma 1996
rob84
fred s roberts applied combinatorics prentice hall upper saddle
river nj 1984
rob86
eric s roberts thinking recursively john wiley  sons new york
ny 1986
rw94
chris ruemmler and john wilkes an introduction to disk drive modeling ieee computer 2731728 march 1994
sal88
betty salzberg file structures an analytic approach prentice hall
upper saddle river nj 1988
sam06
hanan samet foundations of multidimensional and metric data
structures morgan kaufmann san francisco ca 2006
sb93
clifford a shaffer and patrick r brown a paging scheme for
pointerbased quadtrees in d abel and bc ooi editors advances in
spatial databases pages 89104 springer verlag berlin june 1993
sed80
robert sedgewick quicksort garland publishing inc new york
ny 1980
sed11
robert sedgewick algorithms addisonwesley reading ma 4th
edition 2011
sel95
kevin self technically speaking ieee spectrum 32259 february
1995


$$@@$$PAGE: 604
bibliography

sh92

sjh93

ski10
sm83
sol09

st85
sta11a
sta11b
ste90
sto88
str00
su92

sw94

swh93

tan06
tar75

585

clifford a shaffer and gregory m herb a realtime robot arm collision avoidance system ieee transactions on robotics 82149160
1992
clifford a shaffer ramana juvvadi and lenwood s heath a generalized comparison of quadtree and bintree storage requirements image
and vision computing 117402412 september 1993
steven s skiena the algorithm design manual springer verlag
new york ny second edition 2010
gerard salton and michael j mcgill introduction to modern information retrieval mcgrawhill new york ny 1983
daniel solow how to read and do proofs an introduction to mathematical thought processes john wiley  sons new york ny fifth
edition 2009
dd sleator and robert e tarjan selfadjusting binary search trees
journal of the acm 32652686 1985
william stallings operating systems internals and design principles prentice hall upper saddle river nj seventh edition 2011
richard m stallman gnu emacs manual free software foundation
cambridge ma sixteenth edition 2011
guy l steele common lisp the language digital press bedford
ma second edition 1990
james a storer data compression methods and theory computer
science press rockville md 1988
bjarne stroustrup the c  programming language special edition
addisonwesley reading ma 2000
clifford a shaffer and mahesh t ursekar large scale editing and
vector to raster conversion via quadtree spatial indexing in proceedings of the 5th international symposium on spatial data handling
pages 505513 august 1992
murali sitaraman and bruce w weide special feature componentbased software using resolve software engineering notes 19421
67 october 1994
murali sitaraman lonnie r welch and douglas e harms on
specification of reusable software components international journal
of software engineering and knowledge engineering 32207229
june 1993
andrew s tanenbaum structured computer organization prentice
hall upper saddle river nj fifth edition 2006
robert e tarjan on the efficiency of a good but not linear set merging
algorithm journal of the acm 222215225 april 1975


$$@@$$PAGE: 605
586
wel88

bibliography

dominic welsh codes and cryptography oxford university press
oxford 1988
win94
patrick henry winston on to c   addisonwesley reading ma
1994
wl99
arthur whimbey and jack lochhead problem solving  comprehension lawrence erlbaum associates mahwah nj sixth edition
1999
wmb99 ih witten a moffat and tc bell managing gigabytes morgan
kaufmann second edition 1999
zei07
paul zeitz the art and craft of problem solving john wiley  sons
new york ny second edition 2007


$$@@$$PAGE: 606
index

implementation 8 9 21
artificial intelligence 381
abstract data type adt xiv 812 20 assert xvi
21 49 9599 134143 145
asymptotic analysis see algorithm
146 155 168 169 204206
analysis asymptotic
213 215 223 224 285290
atm machine 6
381 386 388 421 436 464
averagecase analysis 6162
abstraction 10
avl tree 196 359 437 442446 464
accounting 120 129
ackermanns function 223
back of the envelope napkin see
estimating
activation record see compiler
activation record
backtracking 561
bag 26 49
aggregate type 8
bank 67
algorithm analysis xiii 4 5591 231
basic operation 5 6 20 21 57 58 63
amortized see amortized analysis
best fit see memory management best
asymptotic 4 55 56 6570 95
fit
469
bestcase analysis 6162
empirical comparison 5556 85
232
bigoh notation see o notation
for program statements 7175
bin packing 562
multiple parameters 7980
binary search see search binary
binary search tree see bst
running time measures 57
binary tree 151201 203
space requirements 56 8082
bst see bst
algorithm definition of 1718
complete 152 153 168 179 251
allpairs shortest paths 521523 540
543
full 152155 166 167 186 196
221
amortized analysis 73 114 321 469
484485 487 489 490
implementation 151 153 196
approximation 561
node 151 155 160166
array
null pointers 155
overhead 166
dynamic 113 489
8020 rule 319 343

587


$$@@$$PAGE: 607
588
parent pointer 160
space requirements 154 160
166167
terminology 151153
threaded 200
traversal see traversal binary tree
binsort 81 82 252259 262 331 546
bintree 460 464
birthday problem 325 347
block 292 296
boolean expression 555
clause 555
conjunctive normal form 555
literal 555
boolean variable 8 30 90 91
branch and bounds 561
breadthfirst search 381 394 396 397
410
bst xv 168178 196198 200 226
245 251 358363 368 437
442448 450 459 464 524
530
efficiency 176
insert 172174
remove 174176
search 170172
search tree property 169
traversal see traversal
btree 312 324 352 356 364375
377 461
analysis 374375
b tree 8 10 352 356 368375
377 378 438

b tree 374
bubble sort 76 235236 238239
260 266
buffer pool xv 11 273 282290
306308 319 320 358 367
429
adt 285290

index

replacement schemes 283284
320322
cache 276 282290 319
cdrom 9 274 277 325
ceiling function 30
city database 149 200 454 464
class see objectoriented programming
class
clique 553 558560 571
cluster problem 226 464
cluster file 278 281 429
code tuning 5557 8386 250251
collatz sequence 69 89 563 570 573
comparator xiv
compiler 85
activation record 125
efficiency 56
optimization 84
complexity 10
composite see design pattern
composite
composite type 8
computability 19 563 570
computer graphics 438 448
connected component see graph
connected component
contradiction proof by see proof
contradiction
cost 5
cylinder see disk drive cylinder
data item 8
data member 9
data structure 4 9
costs and benefits xiii 3 68
definition 9
philosophy 46
physical vs logical form xv 89
1112 95 179 276 286 413
selecting 56
spatial see spatial data structure


$$@@$$PAGE: 608
index

data type 8
decision problem 552 556 571
decision tree 262265 498499
decomposition
image space 438
key space 438
object space 437
delete 110 111 422
depthfirst search 381 393395 410
432 490
deque 149
dequeue see queue dequeue
design pattern xiv 1216 20
composite 1415 163 459
flyweight 13 163 200 458459
strategy 1516 143
visitor 1314 158 394 412
deutschschorrwaite algorithm 433
436
dictionary xiv 169 339 439
adt 134143 311 349 378 517
dijkstras algorithm 400402 404
410 411 521 522
diminishing increment sort see
shellsort
directed acyclic graph dag 383 396
410 414 432
discrete mathematics xiv 47
disjoint 151
disjoint set see equivalence class
disk drive 9 273 276304 306
access cost 280282 303
cylinder 277 357
organization 276279
disk processing see file processing
divide and conquer 245 248 250 314
475 480482
document retrieval 324 345
double buffering 283 295 297
dynamic array see array dynamic
dynamic memory allocation 103

589
dynamic programming 517523 540
561
efficiency xiii 35
element 25
homogeneity 96 114
implementation 114115
emacs text editor 431 433
encapsulation 9
enqueue see queue enqueue
entrysequenced file 351
enumeration see traversal
equation representation 162
equivalence 2728
class 27 203 208213 223 224
226 407 408 411 412 464
relation 27 48
estimation 25 4648 52 53 5557
65
exactmatch query see search
exactmatch query
exponential growth rate see growth
rate exponential
expression tree 162166
extent 279
external sorting see sorting external
factorial function 29 34 36 45 49
73 81 86 127 262 265 570
stirlings approximation 29 265
fibonacci sequence 34 4951 91
477478 517
fifo list 129
file access 290291
file manager 276 278 282 422 423
429
file processing 82 232 303
file structure 9 275 351 375
first fit see memory management first
fit
floor function 30
floppy disk drive 277


$$@@$$PAGE: 609
590
floyds algorithm 521523 540 543
flyweight see design pattern flyweight
fragmentation 279 282 423 427429
external 423
internal 279 423
free store 110111
free tree 383 403 409
freelist 120 124
fstream class 290291
full binary tree theorem 153155 166
196 221
function mathematical 16
general tree 203227
adt 204205 223
converting to binary tree 218 224
dynamic implementations 224
implementation 213218
leftchildrightsibling 215 224
list of children 214215 224 383
parent pointer implementation
207213 445
terminology 203204
traversal see traversal
geographic information system 78
geometric distribution 318 527 530
gigabyte 29
graph xv 22 381412 415
adjacency list 381 383 384 391
410
adjacency matrix 381 383 384
388 389 410 416
adt 381 386 388
connected component 383 412
490
edge 382
implementation 381 386388
modeling of problems 381 390
396 399 400 402
parallel edge 382
representation 383386
self loop 382

index

terminology 381383
traversal see traversal graph
undirected 382 416
vertex 382
greatest common divisor see largest
common factor
greedy algorithm 189 405 407
growth rate 55 5860 86
asymptotic 65
constant 58 66
exponential 60 64 544 549563
linear 60 63 65 82
quadratic 60 63 64 82 83
halting problem 563569
hamiltonian cycle 571
harmonic series 33 319 483
hashing 7 10 31 62 312 324345
351 352 365 417 461 489
analysis of 339344
bucket 331333 349
closed 330339 348
collision resolution 325 331339
343 345
deletion 344345 348
double 339 348
dynamic 345
hash function 325329 347
home position 331
linear probing 333336 339 342
343 347 349
load factor 341
open 330331
perfect 325 345
primary clustering 336338
probe function 334 336339
probe sequence 334339 341345
pseudorandom probing 337339
quadratic probing 338 339 347
search 334
table 324
tombstone 344


$$@@$$PAGE: 610
index

header node 124 134
heap 151 153 168 178185 196 198
201 251252 271 401 407
building 181184
for memory management 422
insert 181 182
maxheap 179
minheap 179 297
partial ordering property 179
remove 184 185
siftdown 183 184 271
heapsort 179 251253 260 271 297
heuristic 561562
hidden obligations see obligations
hidden
huffman coding tree 151 153 162
185196 198201 226 438
prefix property 194

591
kilobyte 29
knapsack problem 560 572 573
kruskals algorithm xv 252 407408
411

largest common factor 50 531532
latency 278 280 281
least frequently used lfu 284 306
320
least recently used lru 284 306
308 320 367
lifo list 120
linear growth see growth rate linear
linear index see index linear
linear search see search sequential
link see list link class
linked list see list linked
lisp 48 415 430 432 433
list 23 95151 187 352 413 415
489
independent set 571
adt 9 9599 146
index 11 267 351378
append 100 116
file 293 351
arraybased 8 95 100103
inverted list 355 376
112114 121 149
linear 8 353355 376 377
basic
operations 96
tree 352 358375
circular 147
induction 224
comparison of space requirements
induction proof by see proof induction
148
inheritance xvi 97 100 105 142 162
current position 96 97 104106
164 165 167
113
inorder traversal see traversal inorder
doubly linked 115120 147 149
input size 57 61
160
insertion sort 76 233236 238241
space
118120
250 260 262267 269 270
element 96 114115
double 270
freelist 110112 422432
integer representation 4 89 20 149
head 96 100 105
inversion 235 239
implementations compared
inverted list see index inverted list
112114
isam 352 356358 377
initialization 96 100
kd tree 450455 459 461 464
insert 96 100 102 104106 109
kary tree 218219 223 224 455
113 116 119 151
link class 103
key 134138


$$@@$$PAGE: 611
592
linked 8 95 100 103106
112114 354 413 437 524
525 529
node 103105 116 117
notation 96
ordered by frequency 317323
469
orthogonal 418
remove 96 100 106 109 113
116 118 119
search 151 311323
selforganizing xv 62 320323
343 345 346 348 445
486487
singly linked 103 115
sorted 4 96 140143
space requirements 112113 147
tail 96
terminology 96
unsorted 96
locality of reference 278 282 343
358 365
logarithm 3132 49 535536
log  213 223
logical representation see data
structure physical vs logical
form
lookup table 81
lower bound 55 6770 342
sorting 261265 546
map 381 399
master theorem see recurrence
relation master theorem
matching 561
matrix 416420
multiplication 548 549
sparse xv 9 413 417420 434
435
triangular 416 417
megabyte 29

index

member see objectoriented
programming member
member function see objectoriented
programming member
function
memory management 11 413
420435
adt 421 436
best fit 426 562
buddy method 423 427428 436
failure policy 423 429433
first fit 426 562
garbage collection 430433
memory allocation 421
memory pool 421
sequential fit 423427 435
worst fit 426
mergesort 127 241244 256 260
269 475 480 482
external 294296
multiway merging 300302
306308
metaphor 10 19
microsoft windows 278 303
millisecond 29
minimumcost spanning tree 231 252
381 402408 410 411 543
modulus function 28 30
movetofront 320323 346 486487
multilist 26 413416 434
multiway merging see mergesort
multiway merging
nested parentheses 22 148
networks 381 400
new 110111 116 422
n p see problem n p
null pointer 103
o notation 6570 87
objectoriented programming 9 1116
1920


$$@@$$PAGE: 612
index

class xvi 9 96
class hierarchy 1415 160166
458459
inheritance xvi
members and objects 8 9
obligations hidden 143 158 289
octree 459
 notation 6770 87
oneway list 103
operating system 18 178 276 278
281284 294 297 422 429
434
operator overloading xvi 110
overhead 81 112113
binary tree 197
matrix 418
stack 125
pairing 544546
palindrome 148
partial order 28 49 179
poset 28
partition 571
path compression 212213 224 491
permutation 29 49 50 81 82 252
263265 341
physical representation see data
structure physical vs logical
form
pigeonhole principle 51 132
point quadtree 460 464
pop see stack pop
postorder traversal see traversal
postorder
powerset see set powerset
pr quadtree 13 162 218 450
455459 461 463 464
preorder traversal see traversal
preorder
prerequisite problem 381
prims algorithm 404406 410 411
primary index 352

593
primary key 352
priority queue 151 168 187 201 402
405
probabilistic data structure 517
524530
problem 6 16 18 544
analysis of 55 7677 232
261265
hard 549563
impossible 563569
instance 16
n p 551
n pcomplete 551563 569
n phard 553
problem solving 19
program 3 18
running time 5657
programming style 19
proof
contradiction 3940 51 405 546
567 568
direct 39
induction 34 3944 48 51 154
182 192 196 265 409
470473 475 476 479 488
489 491 509
pseudopolynomial time algorithm 560
pseudocode xvii 17
push see stack push
quadratic growth see growth rate
quadratic
queue 95 103 129134 148 394
397 398
arraybased 129132
circular 129132 148
dequeue 129 134
empty vs full 131132
enqueue 129
implementations compared 134
linked 134 135
priority see priority queue


$$@@$$PAGE: 613
594
terminology 129
quicksort 127 235 244252 260
267 270 292 294 296 346
469
analysis 482483

index

exactmatch query 78 10 311
312 351
in a dictionary 314
interpolation 314317 346
jump 313314
methods 311
multidimensional 448
radix sort 255260 262 271
range query 78 10 311 324
ram 274 275
351 358
random 30
sequential 21 5758 6162 66
range query 352 see search range
67 7375 89 91 312313
query
322 346 484
realtime applications 61 62
sets
323324
recurrence relation 3435 51 249
successful 311
469 475483 487 488
unsuccessful 311 341
divide and conquer 480482
search trees 62 178 352 356 359
estimating 475478
365 442 445 448
expanding 478 480 489
secondary index 352
master theorem 480482
secondary key 352
solution 35
secondary storage 273282 304306
recursion xiv 34 3638 40 41 49
sector 277 279 281 292
50 73 111 126 157158
172 197 200 242244 267
seek 277 280
269 270 432
selection sort 237239 250 260 266
implemented by stack 125129
selforganizing lists see list
250
selforganizing
replaced by iteration 50 127
sequence 27 29 49 96 312 323 353
reduction 262 544549 568 570 572
544
relation 2729 48 49
sequential search see search sequential
replacement selection 179 296301
sequential tree implementations
307 308 469
219222 225 226
resource constraints 5 6 16 55 56
serialization 220
run in sorting 294
set 2529 49
run file 294 295
powerset 26 29
runningtime equation 58
search 312 323324
subset superset 26
terminology 2526
satisfiability 555561
union intersection difference 26
scheme 48
323 347
search 21 82 311349 351
binary 31 7375 8991 266 314 shellsort 235 239241 260 267
shortest paths 381 399402 410
346 353 367 480 489
simulation 85
defined 311


$$@@$$PAGE: 614
index

skip list xv 524530 540 541
slide rule 32 535
software engineering xiii 4 19 544
sorting 17 21 22 57 61 62 7677
79 82 231271 313 322
544546
adaptive 265
comparing algorithms 232233
259261 302
exchange sorting 238239
external xv 168 232 251 273
291303 306308
internal xv
lower bound 232 261265
small data sets 233 250 265 268
stable algorithms 232 266
terminology 232233
spatial data structure 437 448461
splay tree 178 196 359 437 442
445448 461 462 464 469
525
stable sorting alorithms see sorting
stable algorithms
stack 95 103 120129 148 197 200
250 266 267 270 393395
485
arraybased 121123
constructor 121
implementations compared 125
insert 120 121
linked 124 125
pop 121123 125 150
push 121123 125 150
remove 120 121
terminology 121
top 121123 125
two in one array 125 148
variablesize elements 149
strassens algorithm 532 541
strategy see design pattern strategy

595
subclass see objectoriented
programming class hierarchy
subset see set subset
suffix tree 463
summation 3234 41 42 51 72 73
90 177 184 248 249 318
319 417 469474 479 481
482 484 485 487
guess and test 487
list of solutions 33 34
notation 32
shifting method 471474 483
488
swap 30
tape drive 276 291
template 114
templates xvi 12 97
text compression 151 185195
322323 345 349
 notation 6870 89
topological sort 381 394398 410
total order 28 49 179
towers of hanoi 3638 127 543 550
tradeoff xiii 3 13 75 279 292
diskbased spacetime principle
82 343
spacetime principle 8182 97
120 185 343
transportation network 381 399
transpose 321 322 346
traveling salesman 551553 560 561
571 573
traversal
binary tree 127 151 155160
163 169 177 197 390
enumeration 155 169 220
general tree 205206 223
graph 381 390398
tree
height balanced 364 365 367 525
terminology 151


$$@@$$PAGE: 615
596
trie 162 186 196 259 437442 462
463
alphabet 438
binary 438
patricia 439442 461 462
tuple 27
turing machine 556
twocoloring 44
23 tree 178 352 360364 367 370
377 378 442 489 524
type 8
uncountability 564566
unionfind xv 207 408 412 469
491
units of measure 29 47
unix 245 278 303 432
upper bound 55 6569
variablelength record 149 150 353
377 413 415 420
sorting 233
vector 27 113
vertex cover 557 558 560 561 571
572
virtual function xvi 167
virtual memory 284286 294 306
visitor see design pattern visitor
weighted union rule 210212
223224 491
worst fit see memory management
worst fit
worstcase analysis 6162 67
zipf distribution 319 326 346
zivlempel coding 323 345

index

